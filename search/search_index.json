{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dataverse utilities \u00b6 This is a generalized set of utilities which help with managing Dataverse repositories. This has nothing to do with the Microsoft product of the same name. With these utilities you can: Upload your data sets from a tab-separated-value spreadsheet Bulk release multiple data sets Bulk delete (unpublished) assets Quickly duplicate records Replace licences and more! Get your copy today! Important note \u00b6 These are console utilities, meaning that they will run in a command prompt window, PowerShell, bash, zshell etc. If the sentence you just read is gibberish to you, then these utilities are probably not for you. While they don\u2019t require any programming knowledge to use, you will still need to be able to install Python . Source code (and this documentation) is available at the Github repository https://github.com/ubc-library-rc/dataverse_utils , and the user-friendly version of the documentation is at https://ubc-library-rc.github.io/dataverse_utils . Presumably you know this already otherwise you wouldn\u2019t be reading this. Installation \u00b6 Any installation will require the use of the command line/command prompt (see above). The easiest installation is with pipx . pipx will allow you to run these utilities as separate utilities isolated completely from the rest of your Python installation[s]. This should work for any platform which supports pipx pipx install dataverse_utils There is also a server specific version if you need to use the dv_facet_date utility. This can only be run on a server hosting a Dataverse instance, so for the vast majority of users it will be unusable. This can also be installed with pipx : pipx install 'dataverse_utils[server]' Note the extra quotes. You can install the server version even if you don\u2019t have server access, but there\u2019s no reason to. Upgrading \u00b6 Just as easy as installation: pipx upgrade dataverse_utils Other methods of installing Python packages can be found at https://packaging.python.org/tutorials/installing-packages/ . Downloading the source code \u00b6 Source code is available at https://github.com/ubc-library-rc/dataverse_utils . Working on the assumption that git is installed, you can download the whole works with: git clone https://github.com/ubc-library-rc/dataverse_utils If you have mkdocs installed, you can view the documentation in a web browser by running mkdocs from the top level directory of the downloaded source files by running mkdocs serve . The components \u00b6 Console utilities \u00b6 There are twelve (12) console utilities currently available. In alphabetical order: dv_collection_info : A utility to produce the recursive contents of a dataverse collection, including any or all aspects of study metadata dv_del : Bulk (unpublished) file deletion utility dv_ldc_uploader : A utility which scrapes Linguistic Data Consortium metadata from their website, converts it to Dataverse JSON and uploads it, with the possibility of including local files. dv_list_files : Lists all the files in a dataverse record, potentially including all versions and draft versions. dv_manifest_gen : Creates a simple tab-separated value format file which can be edited and then used to upload files as well as file-level metadata. Normally files will be edited after creation, usually in a spreadsheet like Excel. dv_pg_facet_date : A server-based tool which updates the publication date facet and performs a study reindex. dv_readme_creator : Creates Markdown or PDF descriptive documents from published or unpublished (ideally) studies. dv_record_copy : Copies an existing Dataverse study metadata record to a target collection, or replace a currently existing record. dv_release : A bulk release utility. Either releases all the unreleased studies in a Dataverse or individually if persistent identifiers are available. dv_replace_licence : Replaces the licence associated with a PID with text from a Markdown file. Also available as dv_replace_license for those using American English. dv_study_migrator : Moves the most current version of a study to completely different dataverse installation. Or the same one, if you need to copy it. dv_upload_tsv : Takes a tsv file in the format from dv_manifest_gen.py and does all the uploading and metadata entry. More information about these can be found on the console utilities page . Python package: dataverse_utils \u00b6 If you want to use the Python package directly, you should install with pip instead of pipx although, to be fair, you don\u2019t have to. It will just make your life much easier. If you have no interest in using dataverse_utils code in your own code, you can safely ignore this section. The package contains a variety of utility functions which, for the most part, allow uploads of files and associated metadata without having to touch the Dataverse GUI or to have complex JSON attached. For example, the upload_file requires no JSON attachments: dataverse_utils.upload_file('/path/to/file.ext', dv='https://targetdataverse.invalid' descr='A file description', tags=['Data', 'Example', 'Spam'], dirlabel=['path/to/spam'], mimetype='application/geo+json') Consult the API reference for full details. ldc \u00b6 The ldc component represents the Linguistic Data Consortium or LDC. The ldc module is designed to harvest LDC metadata from its catalogue, convert it to Dataverse JSON, then upload it to a Dataverse installation. Once the study has been created, the general dataverse_utils module can handle the file uploading. The ldc module requires the dryad2dataverse package. Because of this, it requires a tiny bit more effort, because LDC material doesn\u2019t have the required metadata. Here\u2019s snippet that shows how it works. import dataverse_utils.ldc as ldc ldc.ds.constants.DV_CONTACT_EMAIL='iamcontact@test.invalid' ldc.ds.constants.DV_CONTACT_NAME='Generic Support Email' KEY = 'IAM-YOUR-DVERSE-APIKEY' stud = 'LDC2021T02' #LDC study number a = ldc.Ldc(stud) a.fetch_record() #Data goes into the 'ldc' dataverse info = a.upload_metadata(url='https://dataverse.invalid', key=KEY, dv='ldc') hdl = info['data']['persistentId'] with open('/Users/you/tmp/testme.tsv') as fil: du.upload_from_tsv(fil, hdl=hdl,dv='https://dataverse.invalid', apikey=KEY) Note that one method uses key and the other apikey . This is what is known as ad hoc . More information is available at the API reference .","title":"Overview"},{"location":"#dataverse-utilities","text":"This is a generalized set of utilities which help with managing Dataverse repositories. This has nothing to do with the Microsoft product of the same name. With these utilities you can: Upload your data sets from a tab-separated-value spreadsheet Bulk release multiple data sets Bulk delete (unpublished) assets Quickly duplicate records Replace licences and more! Get your copy today!","title":"Dataverse utilities"},{"location":"#important-note","text":"These are console utilities, meaning that they will run in a command prompt window, PowerShell, bash, zshell etc. If the sentence you just read is gibberish to you, then these utilities are probably not for you. While they don\u2019t require any programming knowledge to use, you will still need to be able to install Python . Source code (and this documentation) is available at the Github repository https://github.com/ubc-library-rc/dataverse_utils , and the user-friendly version of the documentation is at https://ubc-library-rc.github.io/dataverse_utils . Presumably you know this already otherwise you wouldn\u2019t be reading this.","title":"Important note"},{"location":"#installation","text":"Any installation will require the use of the command line/command prompt (see above). The easiest installation is with pipx . pipx will allow you to run these utilities as separate utilities isolated completely from the rest of your Python installation[s]. This should work for any platform which supports pipx pipx install dataverse_utils There is also a server specific version if you need to use the dv_facet_date utility. This can only be run on a server hosting a Dataverse instance, so for the vast majority of users it will be unusable. This can also be installed with pipx : pipx install 'dataverse_utils[server]' Note the extra quotes. You can install the server version even if you don\u2019t have server access, but there\u2019s no reason to.","title":"Installation"},{"location":"#upgrading","text":"Just as easy as installation: pipx upgrade dataverse_utils Other methods of installing Python packages can be found at https://packaging.python.org/tutorials/installing-packages/ .","title":"Upgrading"},{"location":"#downloading-the-source-code","text":"Source code is available at https://github.com/ubc-library-rc/dataverse_utils . Working on the assumption that git is installed, you can download the whole works with: git clone https://github.com/ubc-library-rc/dataverse_utils If you have mkdocs installed, you can view the documentation in a web browser by running mkdocs from the top level directory of the downloaded source files by running mkdocs serve .","title":"Downloading the source code"},{"location":"#the-components","text":"","title":"The components"},{"location":"#console-utilities","text":"There are twelve (12) console utilities currently available. In alphabetical order: dv_collection_info : A utility to produce the recursive contents of a dataverse collection, including any or all aspects of study metadata dv_del : Bulk (unpublished) file deletion utility dv_ldc_uploader : A utility which scrapes Linguistic Data Consortium metadata from their website, converts it to Dataverse JSON and uploads it, with the possibility of including local files. dv_list_files : Lists all the files in a dataverse record, potentially including all versions and draft versions. dv_manifest_gen : Creates a simple tab-separated value format file which can be edited and then used to upload files as well as file-level metadata. Normally files will be edited after creation, usually in a spreadsheet like Excel. dv_pg_facet_date : A server-based tool which updates the publication date facet and performs a study reindex. dv_readme_creator : Creates Markdown or PDF descriptive documents from published or unpublished (ideally) studies. dv_record_copy : Copies an existing Dataverse study metadata record to a target collection, or replace a currently existing record. dv_release : A bulk release utility. Either releases all the unreleased studies in a Dataverse or individually if persistent identifiers are available. dv_replace_licence : Replaces the licence associated with a PID with text from a Markdown file. Also available as dv_replace_license for those using American English. dv_study_migrator : Moves the most current version of a study to completely different dataverse installation. Or the same one, if you need to copy it. dv_upload_tsv : Takes a tsv file in the format from dv_manifest_gen.py and does all the uploading and metadata entry. More information about these can be found on the console utilities page .","title":"Console utilities"},{"location":"#python-package-dataverse_utils","text":"If you want to use the Python package directly, you should install with pip instead of pipx although, to be fair, you don\u2019t have to. It will just make your life much easier. If you have no interest in using dataverse_utils code in your own code, you can safely ignore this section. The package contains a variety of utility functions which, for the most part, allow uploads of files and associated metadata without having to touch the Dataverse GUI or to have complex JSON attached. For example, the upload_file requires no JSON attachments: dataverse_utils.upload_file('/path/to/file.ext', dv='https://targetdataverse.invalid' descr='A file description', tags=['Data', 'Example', 'Spam'], dirlabel=['path/to/spam'], mimetype='application/geo+json') Consult the API reference for full details.","title":"Python package: dataverse_utils"},{"location":"#ldc","text":"The ldc component represents the Linguistic Data Consortium or LDC. The ldc module is designed to harvest LDC metadata from its catalogue, convert it to Dataverse JSON, then upload it to a Dataverse installation. Once the study has been created, the general dataverse_utils module can handle the file uploading. The ldc module requires the dryad2dataverse package. Because of this, it requires a tiny bit more effort, because LDC material doesn\u2019t have the required metadata. Here\u2019s snippet that shows how it works. import dataverse_utils.ldc as ldc ldc.ds.constants.DV_CONTACT_EMAIL='iamcontact@test.invalid' ldc.ds.constants.DV_CONTACT_NAME='Generic Support Email' KEY = 'IAM-YOUR-DVERSE-APIKEY' stud = 'LDC2021T02' #LDC study number a = ldc.Ldc(stud) a.fetch_record() #Data goes into the 'ldc' dataverse info = a.upload_metadata(url='https://dataverse.invalid', key=KEY, dv='ldc') hdl = info['data']['persistentId'] with open('/Users/you/tmp/testme.tsv') as fil: du.upload_from_tsv(fil, hdl=hdl,dv='https://dataverse.invalid', apikey=KEY) Note that one method uses key and the other apikey . This is what is known as ad hoc . More information is available at the API reference .","title":"ldc"},{"location":"api_ref/","text":"API Reference \u00b6 dataverse_utils \u00b6 Generalized dataverse utilities. Note that import dataverse_utils is the equivalent of import dataverse_utils.dataverse_utils DvGeneralUploadError \u00b6 Bases: Exception Raised on non-200 URL response Source code in src/dataverse_utils/dataverse_utils.py class DvGeneralUploadError(Exception): ''' Raised on non-200 URL response ''' Md5Error \u00b6 Bases: Exception Raised on md5 mismatch Source code in src/dataverse_utils/dataverse_utils.py class Md5Error(Exception): ''' Raised on md5 mismatch ''' check_lock(dv_url, study, apikey) \u00b6 Checks study lock status; returns True if locked. Parameters: dv_url ( str ) \u2013 URL of Dataverse installation study \u2013 Persistent ID of study apikey ( str ) \u2013 API key for user Source code in src/dataverse_utils/dataverse_utils.py def check_lock(dv_url, study, apikey) -> bool: ''' Checks study lock status; returns True if locked. Parameters ---------- dv_url : str URL of Dataverse installation study: str Persistent ID of study apikey : str API key for user ''' dv_url, headers, params = _make_info(dv_url, study, apikey) lock_status = requests.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() data = lock_status.json().get('data') if data: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) return True return False dump_tsv(start_dir, filename, in_list=None, **kwargs) \u00b6 Dumps output of make_tsv manifest to a file. Parameters: start_dir ( str ) \u2013 Path to start directory in_list ( list , default: None ) \u2013 List of files for which to create manifest entries. Will default to recursive directory crawl **kwargs ( dict , default: {} ) \u2013 Other parameters def_tag ( str ) \u2013 Default Dataverse tag (eg, Data, Documentation, etc). Separate tags with an easily splitable character: eg. (\u2018Data, 2016\u2019) inc_header ( bool ) \u2013 Include header for tsv. quotype ( int ) \u2013 integer value or csv quote type. Acceptable values: * csv.QUOTE_MINIMAL / 0 * csv.QUOTE_ALL / 1 * csv.QUOTE_NONNUMERIC / 2 * csv.QUOTE_NONE / 3 Source code in src/dataverse_utils/dataverse_utils.py def dump_tsv(start_dir, filename, in_list=None, **kwargs): ''' Dumps output of make_tsv manifest to a file. Parameters ---------- start_dir : str Path to start directory in_list : list List of files for which to create manifest entries. Will default to recursive directory crawl **kwargs : dict Other parameters Other parameters ---------------- def_tag : str, optional, default='Data' Default Dataverse tag (eg, Data, Documentation, etc). Separate tags with an easily splitable character: eg. ('Data, 2016') inc_header : bool, optional, default=True Include header for tsv. quotype : int, optional, default=csv.QUOTE_MINIMAL integer value or csv quote type. Acceptable values: * csv.QUOTE_MINIMAL / 0 * csv.QUOTE_ALL / 1 * csv.QUOTE_NONNUMERIC / 2 * csv.QUOTE_NONE / 3 ''' def_tag= kwargs.get('def_tag', 'Data') inc_header =kwargs.get('inc_header', True) mime = kwargs.get('mime', False) path = kwargs.get('path', False) quotype = kwargs.get('quotype', csv.QUOTE_MINIMAL) dumper = make_tsv(start_dir, in_list, def_tag, inc_header, mime, quotype, path=path) with open(filename, 'w', newline='', encoding='utf-8') as tsvfile: tsvfile.write(dumper) file_path(fpath, trunc='') \u00b6 Create relative file path from full path string Parameters: fpath ( str ) \u2013 File location (ie, complete path) trunc ( str , default: '' ) \u2013 Leftmost portion of path to remove Notes >>> file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp/') 'Data/2011' >>> file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp') 'Data/2011' Source code in src/dataverse_utils/dataverse_utils.py def file_path(fpath, trunc='') -> str: ''' Create relative file path from full path string Parameters ---------- fpath : str File location (ie, complete path) trunc : str Leftmost portion of path to remove Notes ----- ``` >>> file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp/') 'Data/2011' >>> file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp') 'Data/2011' ``` ''' if trunc and not trunc.endswith(os.sep): trunc += os.sep path = os.path.dirname(fpath) try: if fpath.find(trunc) == -1: dirlabel = os.path.relpath(os.path.split(path)[0]) dirlabel = os.path.relpath(path[path.find(trunc)+len(trunc):]) if dirlabel == '.': dirlabel = '' return dirlabel except ValueError: return '' force_notab_unlock(study, dv_url, fid, apikey, try_uningest=True) \u00b6 Forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Returns 0 if unlocked, file id if locked (and then unlocked). Parameters: study ( str ) \u2013 Persistent indentifer of study dv_url ( str ) \u2013 URL to base Dataverse installation fid ( str ) \u2013 File ID for file object apikey ( str ) \u2013 API key for user try_uningest ( bool , default: True ) \u2013 Try to uningest the file that was locked. Default: True Source code in src/dataverse_utils/dataverse_utils.py def force_notab_unlock(study, dv_url, fid, apikey, try_uningest=True) -> int: ''' Forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Returns 0 if unlocked, file id if locked (and then unlocked). Parameters ---------- study : str Persistent indentifer of study dv_url : str URL to base Dataverse installation fid : str File ID for file object apikey : str API key for user try_uningest : bool Try to uningest the file that was locked. Default: True ''' dv_url, headers, params = _make_info(dv_url, study, apikey) force_unlock = requests.delete(f'{dv_url}/api/datasets/:persistentId/locks', params=params, headers=headers, timeout=300) LOGGER.warning('Lock removed for %s', study) LOGGER.warning('Lock status:\\n %s', force_unlock.json()) if try_uningest: uningest_file(dv_url, fid, apikey, study) return int(fid) return 0 make_tsv(start_dir, in_list=None, def_tag='Data', inc_header=True, mime=False, quotype=csv.QUOTE_MINIMAL, **kwargs) \u00b6 Recurses the tree for files and produces tsv output with with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. The \u2018description\u2019 is the filename without an extension. Returns tsv as string. Parameters: start_dir ( str ) \u2013 Path to start directory in_list ( list , default: None ) \u2013 Input file list. Defaults to recursive walk of current directory. def_tag ( str , default: 'Data' ) \u2013 Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with a comma: eg. (\u2018Data, 2016\u2019) inc_header ( bool , default: True ) \u2013 Include header row mime ( bool , default: False ) \u2013 Include automatically determined mimetype quotype \u2013 integer value or csv quote type. Default = csv.QUOTE_MINIMAL Acceptable values: csv.QUOTE_MINIMAL / 0 csv.QUOTE_ALL / 1 csv.QUOTE_NONNUMERIC / 2 csv.QUOTE_NONE / 3 **kwargs ( dict , default: {} ) \u2013 Other parameters path ( bool ) \u2013 If true include a \u2018path\u2019 field so that you can type in a custom path instead of actually structuring your data Source code in src/dataverse_utils/dataverse_utils.py def make_tsv(start_dir, in_list=None, def_tag='Data', inc_header=True, mime=False, quotype=csv.QUOTE_MINIMAL, **kwargs) -> str: # pylint: disable=too-many-positional-arguments # pylint: disable=too-many-arguments ''' Recurses the tree for files and produces tsv output with with headers 'file', 'description', 'tags'. The 'description' is the filename without an extension. Returns tsv as string. Parameters ---------- start_dir : str Path to start directory in_list : list Input file list. Defaults to recursive walk of current directory. def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with a comma: eg. ('Data, 2016') inc_header : bool Include header row mime : bool Include automatically determined mimetype quotype: int integer value or csv quote type. Default = csv.QUOTE_MINIMAL Acceptable values: csv.QUOTE_MINIMAL / 0 csv.QUOTE_ALL / 1 csv.QUOTE_NONNUMERIC / 2 csv.QUOTE_NONE / 3 **kwargs : dict Other parameters Other parameters ---------------- path : bool If true include a 'path' field so that you can type in a custom path instead of actually structuring your data ''' if start_dir.endswith(os.sep): #start_dir += os.sep start_dir = start_dir[:-1] if not in_list: in_list = [f'{x[0]}{os.sep}{y}' for x in os.walk(start_dir) for y in x[2] if not y.startswith('.')] if isinstance(in_list, set): in_list=list(in_list) in_list.sort() def_tag = \", \".join([x.strip() for x in def_tag.split(',')]) headers = ['file', 'description', 'tags'] if mime: headers.append('mimetype') if kwargs.get('path'): headers.insert(1, 'path') outf = io.StringIO(newline='') tsv_writer = csv.DictWriter(outf, delimiter='\\t', quoting=quotype, fieldnames=headers, extrasaction='ignore') if inc_header: tsv_writer.writeheader() for row in in_list: #the columns r = {} r['file'] = row r['description'] = os.path.splitext(os.path.basename(row))[0] r['mimetype'] = mimetypes.guess_type(row)[0] r['tags'] = def_tag r['path'] = '' tsv_writer.writerow(r) outf.seek(0) outfile = outf.read() outf.close() return outfile restrict_file(**kwargs) \u00b6 Restrict file in Dataverse study. Parameters: **kwargs ( dict , default: {} ) \u2013 pid ( str ) \u2013 file persistent ID fid ( str ) \u2013 file database ID dv ( ( str , required ) ) \u2013 url to base Dataverse installation eg: \u2018https://abacus.library.ubc.ca\u2019 apikey ( ( str , required ) ) \u2013 API key for user rest ( bool ) \u2013 On True, restrict. Default True Notes One of pid or fid is required Source code in src/dataverse_utils/dataverse_utils.py def restrict_file(**kwargs): ''' Restrict file in Dataverse study. Parameters ---------- **kwargs : dict Other parameters ---------------- pid : str, optional file persistent ID fid : str, optional file database ID dv : str, required url to base Dataverse installation eg: 'https://abacus.library.ubc.ca' apikey : str, required API key for user rest : bool On True, restrict. Default True Notes -------- One of `pid` or `fid` is **required** ''' headers = {'X-Dataverse-key': kwargs['apikey']} headers.update(dataverse_utils.UAHEADER) #Requires a true/false *string* for the API. if kwargs.get('rest', True): rest = 'true' else: rest= 'false' if kwargs.get('pid'): params={'persistentId':kwargs['pid']} rest = requests.put(f'{kwargs[\"dv\"]}/api/files/:persistentId/restrict', headers=headers, params=params, data=rest, timeout=300) elif kwargs.get('fid'): rest = requests.put(f'{kwargs[\"dv\"]}/api/files/{kwargs[\"fid\"]}/restrict', headers=headers, data=rest, timeout=300) else: LOGGER.error('No file ID/PID supplied for file restriction') raise KeyError('One of persistentId (pid) or database ID' '(fid) is required for file restriction') script_ver_stmt(name) \u00b6 Returns a formatted version statement for any script Parameters: name ( str ) \u2013 Name of utility to join to create version statement. Normally %prog from argparse. Source code in src/dataverse_utils/__init__.py def script_ver_stmt(name:str)->str: ''' Returns a formatted version statement for any script Parameters ---------- name : str Name of utility to join to create version statement. Normally %prog from argparse. ''' key = pathlib.Path(name).stem if not SCRIPT_VERSIONS.get(key): return f'dataverse_utils: v{__version__}' return (f\"{key} v{'.'.join(map(str, SCRIPT_VERSIONS[key]))} / \" f'dataverse_utils v{__version__}') uningest_file(dv_url, fid, apikey, study='n/a') \u00b6 Tries to uningest a file that has been ingested. Requires superuser API key. Parameters: dv_url ( str ) \u2013 URL to base Dataverse installation fid ( int or str ) \u2013 File ID of file to uningest apikey ( str ) \u2013 API key for superuser study ( str , default: 'n/a' ) \u2013 Optional handle parameter for log messages Source code in src/dataverse_utils/dataverse_utils.py def uningest_file(dv_url, fid, apikey, study='n/a'): ''' Tries to uningest a file that has been ingested. Requires superuser API key. Parameters ---------- dv_url : str URL to base Dataverse installation fid : int or str File ID of file to uningest apikey : str API key for superuser study : str, optional Optional handle parameter for log messages ''' dv_url, headers, params = _make_info(dv_url, fid, apikey) fid = params['persistentId'] #TODONE: Awaiting answer from Harvard on how to remove progress bar #for uploaded tab files that squeak through. #Answer: you can't! try: uningest = requests.post(f'{dv_url}/api/files/{fid}/uningest', headers=headers, timeout=300) LOGGER.warning('Ingest halted for file %s for fileID %s', fid, study) uningest.raise_for_status() except requests.exceptions.HTTPError: LOGGER.error('Uningestion error: %s', uningest.reason) print(uningest.reason) upload_file(fpath, hdl, **kwargs) \u00b6 Uploads file to Dataverse study and sets file metadata and tags. Parameters: fpath ( str ) \u2013 file location (ie, complete path) hdl ( str ) \u2013 Dataverse persistent ID for study (handle or DOI) **kwargs ( dict , default: {} ) \u2013 Other parameters dv ( ( str , required ) ) \u2013 URL to base Dataverse installation eg: \u2018https://abacus.library.ubc.ca\u2019 apikey ( ( str , required ) ) \u2013 API key for user descr ( str ) \u2013 File description md5 ( str ) \u2013 md5sum for file checking tags ( list ) \u2013 list of text file tags. Eg [\u2018Data\u2019, \u2018June 2020\u2019] dirlabel ( str ) \u2013 Unix style relative pathname for Dataverse file path: eg: path/to/file/ nowait ( bool ) \u2013 Force a file unlock and uningest instead of waiting for processing to finish trunc ( str ) \u2013 Leftmost portion of path to remove rest ( bool ) \u2013 Restrict file. Defaults to false unless True supplied mimetype ( str ) \u2013 Mimetype of file. Useful if using File Previewers. Mimetype for zip files (application/zip) will be ignored to circumvent Dataverse\u2019s automatic unzipping function. label ( str ) \u2013 If included in kwargs, this value will be used for the label timeout ( int ) \u2013 Timeout in seconds override ( bool ) \u2013 Ignore NOTAB (ie, NOTAB = []) Source code in src/dataverse_utils/dataverse_utils.py def upload_file(fpath, hdl, **kwargs): ''' Uploads file to Dataverse study and sets file metadata and tags. Parameters ---------- fpath : str file location (ie, complete path) hdl : str Dataverse persistent ID for study (handle or DOI) **kwargs : dict Other parameters Other parameters ---------------- dv : str, required URL to base Dataverse installation eg: 'https://abacus.library.ubc.ca' apikey : str, required API key for user descr : str, optional File description md5 : str, optional md5sum for file checking tags : list, optional list of text file tags. Eg ['Data', 'June 2020'] dirlabel : str, optional Unix style relative pathname for Dataverse file path: eg: path/to/file/ nowait : bool, optional Force a file unlock and uningest instead of waiting for processing to finish trunc : str, optional Leftmost portion of path to remove rest : bool, optional Restrict file. Defaults to false unless True supplied mimetype : str, optional Mimetype of file. Useful if using File Previewers. Mimetype for zip files (application/zip) will be ignored to circumvent Dataverse's automatic unzipping function. label : str, optional If included in kwargs, this value will be used for the label timeout : int, optional Timeout in seconds override : bool, optional Ignore NOTAB (ie, NOTAB = []) ''' #Why are SPSS files getting processed anyway? #Does SPSS detection happen *after* upload #Does the file need to be renamed post hoc? #I don't think this can be fixed. Goddamitsomuch. dvurl = kwargs['dv'].strip('\\\\ /') if os.path.splitext(fpath)[1].lower() in NOTAB and not kwargs.get('override'): file_name_clean = os.path.basename(fpath) #file_name = os.path.basename(fpath) + '.NOPROCESS' # using .NOPROCESS doesn't seem to work? file_name = os.path.basename(fpath) + '.NOPROCESS' else: file_name = os.path.basename(fpath) file_name_clean = file_name #My workstation python on Windows produces null for isos for some reason if mimetypes.guess_type('test.iso') == (None, None): mimetypes.add_type('application/x-iso9660-image', '.iso') mime = mimetypes.guess_type(fpath)[0] if kwargs.get('mimetype'): mime = kwargs['mimetype'] if file_name.endswith('.NOPROCESS') or mime == 'application/zip': mime = 'application/octet-stream' #create file metadata in nice, simple, chunks dv4_meta = {'label' : kwargs.get('label', file_name_clean), 'description' : kwargs.get('descr', ''), 'directoryLabel': kwargs.get('dirlabel', ''), 'categories': kwargs.get('tags', []), 'mimetype' : mime} fpath = os.path.abspath(fpath) fields = {'file': (file_name, open(fpath, 'rb'), mime)}#pylint: disable=consider-using-with fields.update({'jsonData' : json.dumps(dv4_meta)}) multi = MultipartEncoder(fields=fields) # use multipart streaming for large files headers = {'X-Dataverse-key' : kwargs.get('apikey'), 'Content-type' : multi.content_type} headers.update(dataverse_utils.UAHEADER) params = {'persistentId' : hdl} LOGGER.info('Uploading %s to %s', fpath, hdl) upload = requests.post(f\"{dvurl}/api/datasets/:persistentId/add\", params=params, headers=headers, data=multi, timeout=kwargs.get('timeout',1000)) try: print(upload.json()) except json.decoder.JSONDecodeError: #This can happend when Glassfish crashes LOGGER.critical(upload.text) print(upload.text) err = ('It\\'s possible Glassfish may have crashed. ' 'Check server logs for anomalies') LOGGER.exception(err) print(err) raise #SPSS files still process despite spoof, so there's #a forcible unlock check fid = upload.json()['data']['files'][0]['dataFile']['id'] print(f'FID: {fid}') if kwargs.get('nowait') and check_lock(dvurl, hdl, kwargs['apikey']): force_notab_unlock(hdl, dvurl, fid, kwargs['apikey']) else: while check_lock(dvurl, hdl, kwargs['apikey']): time.sleep(10) if upload.status_code != 200: LOGGER.critical('Upload failure: %s', (upload.status_code, upload.reason)) raise DvGeneralUploadError(f'\\nReason: {(upload.status_code, upload.reason)}' f'\\n{upload.text}') if kwargs.get('md5'): if upload.json()['data']['files'][0]['dataFile']['md5'] != kwargs.get('md5'): LOGGER.warning('md5sum mismatch on %s', fpath) raise Md5Error('md5sum mismatch') restrict_file(fid=fid, dv=dvurl, apikey=kwargs.get('apikey'), rest=kwargs.get('rest', False)) upload_from_tsv(fil, hdl, **kwargs) \u00b6 Utility for bulk uploading. Assumes fil is formatted as tsv with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. \u2018tags\u2019 field will be split on commas. Parameters: fil \u2013 Open file object or io.IOStream() hdl ( str ) \u2013 Dataverse persistent ID for study (handle or DOI) **kwargs ( dict , default: {} ) \u2013 Other parameters trunc ( str ) \u2013 Leftmost portion of Dataverse study file path to remove. eg: trunc =\u2019/home/user/\u2019 if the tsv field is \u2018/home/user/Data/ASCII\u2019 would set the path for that line of the tsv to \u2018Data/ASCII\u2019. Defaults to None. dv ( ( str , required ) ) \u2013 url to base Dataverse installation eg: \u2018https://abacus.library.ubc.ca\u2019 apikey ( ( str , required ) ) \u2013 API key for user rest ( bool ) \u2013 On True, restrict access. Default False Source code in src/dataverse_utils/dataverse_utils.py def upload_from_tsv(fil, hdl, **kwargs): ''' Utility for bulk uploading. Assumes fil is formatted as tsv with headers 'file', 'description', 'tags'. 'tags' field will be split on commas. Parameters ---------- fil Open file object or io.IOStream() hdl : str Dataverse persistent ID for study (handle or DOI) **kwargs : dict Other parameters Other parameters ---------------- trunc : str Leftmost portion of Dataverse study file path to remove. eg: trunc ='/home/user/' if the tsv field is '/home/user/Data/ASCII' would set the path for that line of the tsv to 'Data/ASCII'. Defaults to None. dv : str, required url to base Dataverse installation eg: 'https://abacus.library.ubc.ca' apikey : str, required API key for user rest : bool, optional On True, restrict access. Default False ''' #reader = csv.reader(fil, delimiter='\\t', quotechar='\"') #new, optional mimetype column allows using GeoJSONS. #Read the headers from the file first before using DictReader headers = fil.readline().strip('\\n\\r').split('\\t')#Goddamn it Windows fil.seek(0) reader = csv.DictReader(fil, fieldnames=headers, quotechar='\"', delimiter='\\t') #See API call for \"Adding File Metadata\" for num, row in enumerate(reader): if num == 0: continue #dirlabel = file_path(row[0], './') if row.get('path'): #Explicit separate path because that way you can organize #on upload dirlabel = row.get('path') else: dirlabel = file_path(row['file'], kwargs.get('trunc', '')) tags = row['tags'].split(',') tags = [x.strip() for x in tags] descr = row['description'] mimetype = row.get('mimetype') params = {'dv' : kwargs.get('dv'), 'tags' : tags, 'descr' : descr, 'dirlabel' : dirlabel, 'apikey' : kwargs.get('apikey'), 'md5' : kwargs.get('md5', ''), 'rest': kwargs.get('rest', False)} if mimetype: params['mimetype'] = mimetype #So that you can pass everything all at once, params #is merged onto kwargs. This is for easier upgradability kwargs.update(params) upload_file(row['file'], hdl, **kwargs) dataverse_utils.dvdata \u00b6 Dataverse studies and files File \u00b6 Bases: dict Class representing a file on a Dataverse instance Source code in src/dataverse_utils/dvdata.py class File(dict): ''' Class representing a file on a Dataverse instance ''' def __init__(self, url:str, key:str, **kwargs): ''' Dataverse file object Parameters ---------- url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges **kwargs : dict Other parameters Notes ----- To initialize correctly, pass a value from Study['file_info']. Eg: `File('https://test.invalid', 'ABC123', **Study_instance['file_info'][0])` Not to be confused with the FileAnalysis object in `dataverse_utils.collections`. ''' self['url'] = url self.__key = key self['downloaded'] = False self['downloaded_file_name'] = None self['downloaded_checksum'] = None self['verified'] = None #self['dv_file_metadata'] = None # if not self['dv_file_metadata']: # self['dv_file_metadata'] = self._get_file_metadata() for keey, val in kwargs.items(): self[keey] = val self['timeout'] = kwargs.get('timeout', TIMEOUT) def download_file(self): ''' Downloads the file to a temporary location. Data will be in the ORIGINAL format, not Dataverse-processed TSVs ''' if not self['downloaded'] or not os.path.exists(self.get('downloaded_file_name', '')): headers = headers={'X-Dataverse-key':self.__key} headers.update(UAHEADER) try: #curl \"$SERVER_URL/api/access/datafile/:persistentId/?persistentId=$PERSISTENT_ID\" dwnld = requests.get(self['url']+'/api/access/datafile/'+ str(self['dataFile']['id']), headers=headers, params = {'format':'original'}, timeout=self['timeout']) with tempfile.NamedTemporaryFile(delete=False) as fil: self['downloaded_file_name'] = fil.name fil.write(dwnld.content) self['downloaded'] = True return True except requests.exceptions.HTTPError as err: LOGGER.exception(err) LOGGER.exception(traceback.format_exc()) self['downloaded'] = False return False return None def del_tempfile(self): ''' Delete tempfile if it exists ''' if os.path.exists(self['downloaded_file_name']): os.remove(self['downloaded_file_name']) self['downloaded'] = False self['downloaded_file_name'] = None self['verified'] = None def produce_digest(self, prot: str = 'md5', blocksize: int = 2**16) -> str: ''' Returns hex digest for object Parameters ---------- prot : str, optional, default='md5' Hash type. Supported hashes: 'sha1', 'sha224', 'sha256', 'sha384', 'sha512', 'blake2b', 'blake2s', 'md5'. Default: 'md5' blocksize : int, optional, default=2**16 Read block size in bytes ''' if not self['downloaded_file_name']: return None ok_hash = {'sha1' : hashlib.sha1(), 'sha224' : hashlib.sha224(), 'sha256' : hashlib.sha256(), 'sha384' : hashlib.sha384(), 'sha512' : hashlib.sha512(), 'blake2b' : hashlib.blake2b(), 'blake2s' : hashlib.blake2s(), 'md5': hashlib.md5()} with open(self['downloaded_file_name'], 'rb') as _fobj: try: _hash = ok_hash[prot] except (UnboundLocalError, KeyError) as err: message = ('Unsupported hash type. Valid values are ' f'{list(ok_hash)}.' ) LOGGER.exception(err) LOGGER.exception(message) LOGGER.exception(traceback.format_exc()) raise fblock = _fobj.read(blocksize) while fblock: _hash.update(fblock) fblock = _fobj.read(blocksize) return _hash.hexdigest() def verify(self)->None: ''' Compares actual checksum with stated checksum ''' if not self.get('downloaded_file_name') or not self.get('downloaded'): LOGGER.error('File has not been downloaded') self['verified'] = None self['downloaded_checksum'] = None return None _hash = self.produce_digest(self['dataFile']['checksum']['type'].lower()) if _hash == self['dataFile']['checksum']['value']: self['verified'] = True self['downloaded_checksum'] = hash return True LOGGER.error('Checksum mismatch in %s', self.get('label')) self['verified'] = False self['downloaded_checksum'] = _hash return False __init__(url, key, **kwargs) \u00b6 Dataverse file object Parameters: url ( str ) \u2013 Base URL to host Dataverse instance key ( str ) \u2013 Dataverse API key with downloader privileges **kwargs ( dict , default: {} ) \u2013 Other parameters Notes To initialize correctly, pass a value from Study[\u2018file_info\u2019]. Eg: File('https://test.invalid', 'ABC123', **Study_instance['file_info'][0]) Not to be confused with the FileAnalysis object in dataverse_utils.collections . Source code in src/dataverse_utils/dvdata.py def __init__(self, url:str, key:str, **kwargs): ''' Dataverse file object Parameters ---------- url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges **kwargs : dict Other parameters Notes ----- To initialize correctly, pass a value from Study['file_info']. Eg: `File('https://test.invalid', 'ABC123', **Study_instance['file_info'][0])` Not to be confused with the FileAnalysis object in `dataverse_utils.collections`. ''' self['url'] = url self.__key = key self['downloaded'] = False self['downloaded_file_name'] = None self['downloaded_checksum'] = None self['verified'] = None #self['dv_file_metadata'] = None # if not self['dv_file_metadata']: # self['dv_file_metadata'] = self._get_file_metadata() for keey, val in kwargs.items(): self[keey] = val self['timeout'] = kwargs.get('timeout', TIMEOUT) del_tempfile() \u00b6 Delete tempfile if it exists Source code in src/dataverse_utils/dvdata.py def del_tempfile(self): ''' Delete tempfile if it exists ''' if os.path.exists(self['downloaded_file_name']): os.remove(self['downloaded_file_name']) self['downloaded'] = False self['downloaded_file_name'] = None self['verified'] = None download_file() \u00b6 Downloads the file to a temporary location. Data will be in the ORIGINAL format, not Dataverse-processed TSVs Source code in src/dataverse_utils/dvdata.py def download_file(self): ''' Downloads the file to a temporary location. Data will be in the ORIGINAL format, not Dataverse-processed TSVs ''' if not self['downloaded'] or not os.path.exists(self.get('downloaded_file_name', '')): headers = headers={'X-Dataverse-key':self.__key} headers.update(UAHEADER) try: #curl \"$SERVER_URL/api/access/datafile/:persistentId/?persistentId=$PERSISTENT_ID\" dwnld = requests.get(self['url']+'/api/access/datafile/'+ str(self['dataFile']['id']), headers=headers, params = {'format':'original'}, timeout=self['timeout']) with tempfile.NamedTemporaryFile(delete=False) as fil: self['downloaded_file_name'] = fil.name fil.write(dwnld.content) self['downloaded'] = True return True except requests.exceptions.HTTPError as err: LOGGER.exception(err) LOGGER.exception(traceback.format_exc()) self['downloaded'] = False return False return None produce_digest(prot='md5', blocksize=2 ** 16) \u00b6 Returns hex digest for object Parameters: prot ( str , default: 'md5' ) \u2013 Hash type. Supported hashes: \u2018sha1\u2019, \u2018sha224\u2019, \u2018sha256\u2019, \u2018sha384\u2019, \u2018sha512\u2019, \u2018blake2b\u2019, \u2018blake2s\u2019, \u2018md5\u2019. Default: \u2018md5\u2019 blocksize ( int , default: 2**16 ) \u2013 Read block size in bytes Source code in src/dataverse_utils/dvdata.py def produce_digest(self, prot: str = 'md5', blocksize: int = 2**16) -> str: ''' Returns hex digest for object Parameters ---------- prot : str, optional, default='md5' Hash type. Supported hashes: 'sha1', 'sha224', 'sha256', 'sha384', 'sha512', 'blake2b', 'blake2s', 'md5'. Default: 'md5' blocksize : int, optional, default=2**16 Read block size in bytes ''' if not self['downloaded_file_name']: return None ok_hash = {'sha1' : hashlib.sha1(), 'sha224' : hashlib.sha224(), 'sha256' : hashlib.sha256(), 'sha384' : hashlib.sha384(), 'sha512' : hashlib.sha512(), 'blake2b' : hashlib.blake2b(), 'blake2s' : hashlib.blake2s(), 'md5': hashlib.md5()} with open(self['downloaded_file_name'], 'rb') as _fobj: try: _hash = ok_hash[prot] except (UnboundLocalError, KeyError) as err: message = ('Unsupported hash type. Valid values are ' f'{list(ok_hash)}.' ) LOGGER.exception(err) LOGGER.exception(message) LOGGER.exception(traceback.format_exc()) raise fblock = _fobj.read(blocksize) while fblock: _hash.update(fblock) fblock = _fobj.read(blocksize) return _hash.hexdigest() verify() \u00b6 Compares actual checksum with stated checksum Source code in src/dataverse_utils/dvdata.py def verify(self)->None: ''' Compares actual checksum with stated checksum ''' if not self.get('downloaded_file_name') or not self.get('downloaded'): LOGGER.error('File has not been downloaded') self['verified'] = None self['downloaded_checksum'] = None return None _hash = self.produce_digest(self['dataFile']['checksum']['type'].lower()) if _hash == self['dataFile']['checksum']['value']: self['verified'] = True self['downloaded_checksum'] = hash return True LOGGER.error('Checksum mismatch in %s', self.get('label')) self['verified'] = False self['downloaded_checksum'] = _hash return False FileInfo \u00b6 Bases: dict An object representing all of a dataverse study\u2019s files. Easily parseable as a dict. Source code in src/dataverse_utils/dvdata.py class FileInfo(dict): ''' An object representing all of a dataverse study's files. Easily parseable as a dict. ''' #Should this be incorporated into the above class? Probably. def __init__(self, **kwargs)->None: ''' Intialize a File object Parameters ---------- **kwargs : dict Keyword arguments as below Other parameters ---------------- url : str, required Base URL of dataverse installation pid : str, required Handle or DOI of study apikey : str, optional Dataverse API key; required for DRAFT or restricted material. Or if the platform policy requires an API key. timeout : int, optional Optional timeout in seconds ''' self.kwargs = kwargs self['version_list'] = [] self.dv = None self._get_json() self._get_all_files() self['headers'] = list(self[self['current_version']][0].keys()) def _get_json(self) -> None: ''' Get study file json ''' try: headers={'X-Dataverse-key' : self.kwargs.get('apikey')} headers.update(UAHEADER) params = {'persistentId': self.kwargs['pid']} self.dv = requests.get(f'{self.kwargs[\"url\"]}/api/datasets/:persistentId/versions', params=params, timeout=self.kwargs.get('timeout', 100), headers=headers) self.dv.raise_for_status() except (requests.exceptions.RequestException, requests.exceptions.ConnectionError, requests.exceptions.HTTPError, requests.exceptions.TooManyRedirects, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.Timeout, requests.exceptions.JSONDecodeError, requests.exceptions.InvalidSchema) as err: err.add_note(f'Connection error: {\"\\n\".join((str(x) for x in err.args))}') msg = '\\n'.join(getattr(err, '__notes__', [])) LOGGER.critical(msg) raise err def _get_all_files(self): ''' Iterates over self.dv_json()['data']. to produce a list of files in self['files'] ''' try: for num, version in enumerate(self.dv.json()['data']): self._get_version_files(version, current=num) except AttributeError as err: err.add_note('No JSON present') #LOGGER.exception('FileInfo AttributeError: %s', err) #LOGGER.exception(traceback.format_exc()) raise err except KeyError as err: err.add_note(f'JSON parsing error: {err}') err.add_note('Offending JSON:') err.add_note(f'{self.dv.json()}') msg = '\\n'.join(getattr(err, '__notes__', [])) LOGGER.exception('FileInfo KeyError: %s', msg) #LOGGER.exception(traceback.format_exc()) raise err def _get_version_files(self, flist: list, current=1)->None: ''' Set version number and assign file info a version key Parameters ---------- flist : list list of file metadata for a particular version current: int, optional, default=1 Value of zero represents most current version ''' if flist['versionState'] == 'DRAFT': ver_info='DRAFT' else: ver_info = f\"{flist['versionNumber']}.{flist['versionMinorNumber']}\" if current == 0: self['current_version'] = ver_info self['version_list'].append(ver_info) self[ver_info] = [] for fil in flist['files']: self[ver_info].append(self._get_file_info(fil, ver_info=ver_info, state_info=flist['versionState'])) def _get_file_info(self, file:dict, **kwargs)->dict: ''' Returns a dict of required info from a chunk of dataverse study version metadata Parameters ---------- file : dict The dict containing one file's metadata **kwargs : dict Keyword arguments version_info: str Version info string state_info : str Publication state ''' # headers = ['file', 'description', 'pidURL','downloadURL', 'version', 'state'] file_name = file['dataFile'].get('originalFileName', file['label']) filepath = pathlib.Path(file.get('directoryLabel', ''), file_name) description = file.get('description', '') try: pid_url = file['dataFile']['pidURL'] except KeyError: pid_url = f'{self.kwargs[\"url\"]}/file.xhtml?fileId={file[\"dataFile\"][\"id\"]}' fid = file['dataFile']['id'] download_url = f'{self.kwargs[\"url\"]}/api/access/datafile/{fid}?format=original' out = {'file': str(filepath).strip(), 'description': description.strip(), 'pid_url': pid_url, 'download_url':download_url, 'version': kwargs['ver_info'], 'state' : kwargs['state_info']} return out __init__(**kwargs) \u00b6 Intialize a File object Parameters: **kwargs ( dict , default: {} ) \u2013 Keyword arguments as below url ( ( str , required ) ) \u2013 Base URL of dataverse installation pid ( ( str , required ) ) \u2013 Handle or DOI of study apikey ( str ) \u2013 Dataverse API key; required for DRAFT or restricted material. Or if the platform policy requires an API key. timeout ( int ) \u2013 Optional timeout in seconds Source code in src/dataverse_utils/dvdata.py def __init__(self, **kwargs)->None: ''' Intialize a File object Parameters ---------- **kwargs : dict Keyword arguments as below Other parameters ---------------- url : str, required Base URL of dataverse installation pid : str, required Handle or DOI of study apikey : str, optional Dataverse API key; required for DRAFT or restricted material. Or if the platform policy requires an API key. timeout : int, optional Optional timeout in seconds ''' self.kwargs = kwargs self['version_list'] = [] self.dv = None self._get_json() self._get_all_files() self['headers'] = list(self[self['current_version']][0].keys()) Study \u00b6 Bases: dict Dataverse record. Dataverse study records are pure metadata so this is represented with a dictionary. Source code in src/dataverse_utils/dvdata.py class Study(dict): #pylint: disable=too-few-public-methods ''' Dataverse record. Dataverse study records are pure metadata so this is represented with a dictionary. ''' def __init__(self, pid: str, url:str, key:str, **kwargs): ''' Initialize a Study object Parameters ---------- pid : str Record persistent identifier: hdl or doi url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges **kwargs : dict Keyword arguments Other parameters ---------------- timeout : int Request timeout in seconds ''' self['pid'] = pid self['url'] = url self.__key = key self['orig_json'] = None self['timeout'] = kwargs.get('timeout',TIMEOUT) if not self['orig_json']: self['orig_json'] = self._orig_json() self['upload_json'] = self._upload_json self['file_info'] = self['orig_json']['files'] self['file_ids'] = [x['dataFile'].get('id') for x in self['orig_json']['files']] self['file_persistentIds'] = self._get_file_pids() self['source_version'] = Study.get_version(url) self['target_version'] = None if not self['target_version']: self['target_version'] = Study.get_version(url) @classmethod def get_version(cls, url:str, timeout:int=100)->float: ''' Returns a float representing a Dataverse version number. Floating point value composed of: float(f'{major_version}.{minor_verson:03d}{patch:03d}') ie, version 5.9.2 would be 5.009002 Parameters ---------- url : str URL of base Dataverse instance. eg: 'https://abacus.library.ubc.ca' timeout : int, default=100 Request timeout in seconds ''' ver = requests.get(f'{url}/api/info/version', headers=UAHEADER, #headers = {'X-Dataverse-key' : key}, timeout = timeout) try: ver.raise_for_status() except requests.exceptions.HTTPError as exc: LOGGER.error(r'Error getting version for {url}') LOGGER.exception(exc) LOGGER.exception(traceback.format_exc()) raise requests.exceptions.HTTPError #Scholars Portal version is formatted as v5.13.9-SP, so. . . verf = ver.json()['data']['version'].strip('v ').split('.') verf = [x.split('-')[0] for x in verf] verf =[int(b)/10**(3*a) for a,b in enumerate(verf)] #it's 3*a in case for some reason we hit, say v5.99.99 and there's more before v6. verf = sum(verf) return verf def set_version(self, url:str, timeout:int=100)->None: ''' Sets self['target_version'] to appropriate integer value *AND* formats self['upload_json'] to correct JSON format Parameters ---------- url : str URL of *target* Dataverse instance timeout : int, optional, default=100 request timeout in seconds ''' self['target_version'] = Study.get_version(url, timeout) # Now fix the metadata to work with various versions if self['target_version'] >= 5.010: self.fix_licence() if self['target_version'] >= 5.013: self.production_location() def _orig_json(self) -> dict: ''' Latest study version record JSON. Retrieved from Dataverse installation so an internet connection is required. ''' #curl -H \"X-Dataverse-key:$API_TOKEN\" / #$SERVER_URL/api/datasets/:persistentId/?persistentId=$PERSISTENT_IDENTIFIER headers = {'X-Dataverse-key' : self.__key} headers.update(UAHEADER) getjson = requests.get(self['url']+'/api/datasets/:persistentId', headers=headers, params = {'persistentId': self['pid']}, timeout = self['timeout']) getjson.raise_for_status() return getjson.json()['data']['latestVersion'] def __add_email(self, upjson): ''' Adds contact information if it's not there. Fills with dummy data Parameters ---------- upjson : dict Metadata ''' #pylint: disable=possibly-used-before-assignment for n, v in enumerate((upjson['datasetVersion'] ['metadataBlocks']['citation']['fields'])): if v['typeName'] == 'datasetContact': contact_no = n for _x in (upjson['datasetVersion']['metadataBlocks'] ['citation']['fields'][contact_no]['value']): if not _x.get('datasetContactEmail'): _x['datasetContactEmail'] = {'typeName':'datasetContactEmail', 'multiple': False, 'typeClass':'primitive', 'value': 'suppressed_value@test.invalid'} return upjson @property def _upload_json(self)->dict: ''' A Dataverse JSON record with with PIDs and other information stripped suitable for upload as a new Dataverse study record. ''' upj = {'datasetVersion': {'license': self['orig_json']['license'], 'termsOfUse': self['orig_json'].get('termsOfUse',''), 'metadataBlocks': self['orig_json']['metadataBlocks'] } } return self.__add_email(upj) @property def _oldupload_json(self)->dict: ''' A Dataverse JSON record with with PIDs and other information stripped suitable for upload as a new Dataverse study record. ''' return {'datasetVersion': {'license': self['orig_json']['license'], 'termsOfUse': self['orig_json'].get('termsOfUse',''), 'metadataBlocks': self['orig_json']['metadataBlocks'] } } def _get_file_pids(self)->list: ''' Returns a list of file ids representing the file objects in dataverse record ''' pids = [x['dataFile'].get('persistentId') for x in self['orig_json']['files']] if not all(pids): return None return pids ###### #JSON metdata fixes for different versions ###### def fix_licence(self)->None: ''' Replaces non-standard licence with None Notes ----- With Dataverse v5.10+, a licence type of 'NONE' is now forbidden. Now, as per <https://guides.dataverse.org/en/5.14/api/sword.html\\ ?highlight=invalid%20license>, non-standard licences may be replaced with None. This function edits the same Study object *in place*, so returns nothing. ''' if self['upload_json']['datasetVersion']['license'] == 'NONE': self['upload_json']['datasetVersion']['license'] = None if not self['upload_json']['datasetVersion']['termsOfUse']: #This shouldn't happen, but UBC has datasets from the early 1970s self['upload_json']['datasetVersion']['termsOfUse'] = 'Not available' def production_location(self)->None: ''' Changes \"multiple\" to True where typeName == 'productionPlace' in Study['upload_json'] Changes are done *in place*. Notes ----- Multiple production places came into effect with Dataverse v5.13 ''' #{'typeName': 'productionPlace', 'multiple': True, 'typeClass': 'primitive', #'value': ['Vancouver, BC', 'Ottawa, ON']} # get index indy = None for ind, val in enumerate(self['upload_json']['datasetVersion']\\ ['metadataBlocks']['citation']['fields']): if val['typeName'] == 'productionPlace': indy = ind break if indy and not self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['multiple']: self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['multiple'] = True self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['value'] = [self['upload_json']['datasetVersion']\\ ['metadataBlocks']['citation']\\ ['fields'][indy]['value']] __add_email(upjson) \u00b6 Adds contact information if it\u2019s not there. Fills with dummy data Parameters: upjson ( dict ) \u2013 Metadata Source code in src/dataverse_utils/dvdata.py def __add_email(self, upjson): ''' Adds contact information if it's not there. Fills with dummy data Parameters ---------- upjson : dict Metadata ''' #pylint: disable=possibly-used-before-assignment for n, v in enumerate((upjson['datasetVersion'] ['metadataBlocks']['citation']['fields'])): if v['typeName'] == 'datasetContact': contact_no = n for _x in (upjson['datasetVersion']['metadataBlocks'] ['citation']['fields'][contact_no]['value']): if not _x.get('datasetContactEmail'): _x['datasetContactEmail'] = {'typeName':'datasetContactEmail', 'multiple': False, 'typeClass':'primitive', 'value': 'suppressed_value@test.invalid'} return upjson __init__(pid, url, key, **kwargs) \u00b6 Initialize a Study object Parameters: pid ( str ) \u2013 Record persistent identifier: hdl or doi url ( str ) \u2013 Base URL to host Dataverse instance key ( str ) \u2013 Dataverse API key with downloader privileges **kwargs ( dict , default: {} ) \u2013 Keyword arguments timeout ( int ) \u2013 Request timeout in seconds Source code in src/dataverse_utils/dvdata.py def __init__(self, pid: str, url:str, key:str, **kwargs): ''' Initialize a Study object Parameters ---------- pid : str Record persistent identifier: hdl or doi url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges **kwargs : dict Keyword arguments Other parameters ---------------- timeout : int Request timeout in seconds ''' self['pid'] = pid self['url'] = url self.__key = key self['orig_json'] = None self['timeout'] = kwargs.get('timeout',TIMEOUT) if not self['orig_json']: self['orig_json'] = self._orig_json() self['upload_json'] = self._upload_json self['file_info'] = self['orig_json']['files'] self['file_ids'] = [x['dataFile'].get('id') for x in self['orig_json']['files']] self['file_persistentIds'] = self._get_file_pids() self['source_version'] = Study.get_version(url) self['target_version'] = None if not self['target_version']: self['target_version'] = Study.get_version(url) fix_licence() \u00b6 Replaces non-standard licence with None Notes With Dataverse v5.10+, a licence type of \u2018NONE\u2019 is now forbidden. Now, as per https://guides.dataverse.org/en/5.14/api/sword.html ?highlight=invalid%20license , non-standard licences may be replaced with None. This function edits the same Study object in place , so returns nothing. Source code in src/dataverse_utils/dvdata.py def fix_licence(self)->None: ''' Replaces non-standard licence with None Notes ----- With Dataverse v5.10+, a licence type of 'NONE' is now forbidden. Now, as per <https://guides.dataverse.org/en/5.14/api/sword.html\\ ?highlight=invalid%20license>, non-standard licences may be replaced with None. This function edits the same Study object *in place*, so returns nothing. ''' if self['upload_json']['datasetVersion']['license'] == 'NONE': self['upload_json']['datasetVersion']['license'] = None if not self['upload_json']['datasetVersion']['termsOfUse']: #This shouldn't happen, but UBC has datasets from the early 1970s self['upload_json']['datasetVersion']['termsOfUse'] = 'Not available' get_version(url, timeout=100) classmethod \u00b6 Returns a float representing a Dataverse version number. Floating point value composed of: float(f\u2019{major_version}.{minor_verson:03d}{patch:03d}\u2019) ie, version 5.9.2 would be 5.009002 Parameters: url ( str ) \u2013 URL of base Dataverse instance. eg: \u2018https://abacus.library.ubc.ca\u2019 timeout ( int , default: 100 ) \u2013 Request timeout in seconds Source code in src/dataverse_utils/dvdata.py @classmethod def get_version(cls, url:str, timeout:int=100)->float: ''' Returns a float representing a Dataverse version number. Floating point value composed of: float(f'{major_version}.{minor_verson:03d}{patch:03d}') ie, version 5.9.2 would be 5.009002 Parameters ---------- url : str URL of base Dataverse instance. eg: 'https://abacus.library.ubc.ca' timeout : int, default=100 Request timeout in seconds ''' ver = requests.get(f'{url}/api/info/version', headers=UAHEADER, #headers = {'X-Dataverse-key' : key}, timeout = timeout) try: ver.raise_for_status() except requests.exceptions.HTTPError as exc: LOGGER.error(r'Error getting version for {url}') LOGGER.exception(exc) LOGGER.exception(traceback.format_exc()) raise requests.exceptions.HTTPError #Scholars Portal version is formatted as v5.13.9-SP, so. . . verf = ver.json()['data']['version'].strip('v ').split('.') verf = [x.split('-')[0] for x in verf] verf =[int(b)/10**(3*a) for a,b in enumerate(verf)] #it's 3*a in case for some reason we hit, say v5.99.99 and there's more before v6. verf = sum(verf) return verf production_location() \u00b6 Changes \u201cmultiple\u201d to True where typeName == \u2018productionPlace\u2019 in Study[\u2018upload_json\u2019] Changes are done in place . Notes Multiple production places came into effect with Dataverse v5.13 Source code in src/dataverse_utils/dvdata.py def production_location(self)->None: ''' Changes \"multiple\" to True where typeName == 'productionPlace' in Study['upload_json'] Changes are done *in place*. Notes ----- Multiple production places came into effect with Dataverse v5.13 ''' #{'typeName': 'productionPlace', 'multiple': True, 'typeClass': 'primitive', #'value': ['Vancouver, BC', 'Ottawa, ON']} # get index indy = None for ind, val in enumerate(self['upload_json']['datasetVersion']\\ ['metadataBlocks']['citation']['fields']): if val['typeName'] == 'productionPlace': indy = ind break if indy and not self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['multiple']: self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['multiple'] = True self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['value'] = [self['upload_json']['datasetVersion']\\ ['metadataBlocks']['citation']\\ ['fields'][indy]['value']] set_version(url, timeout=100) \u00b6 Sets self[\u2018target_version\u2019] to appropriate integer value AND formats self[\u2018upload_json\u2019] to correct JSON format Parameters: url ( str ) \u2013 URL of target Dataverse instance timeout ( int , default: 100 ) \u2013 request timeout in seconds Source code in src/dataverse_utils/dvdata.py def set_version(self, url:str, timeout:int=100)->None: ''' Sets self['target_version'] to appropriate integer value *AND* formats self['upload_json'] to correct JSON format Parameters ---------- url : str URL of *target* Dataverse instance timeout : int, optional, default=100 request timeout in seconds ''' self['target_version'] = Study.get_version(url, timeout) # Now fix the metadata to work with various versions if self['target_version'] >= 5.010: self.fix_licence() if self['target_version'] >= 5.013: self.production_location() dataverse_utils.ldc \u00b6 Creates dataverse JSON from Linguistic Data Consortium website page. Ldc \u00b6 Bases: Serializer An LDC item (eg, LDC2021T01) Source code in src/dataverse_utils/ldc.py class Ldc(ds.Serializer):#pylint: disable=too-many-instance-attributes ''' An LDC item (eg, LDC2021T01) ''' #pylint: disable=super-init-not-called, arguments-differ def __init__(self, ldc, cert=None): ''' Returns a dict with keys created from an LDC catalogue web page. Parameters ---------- ldc : str Linguistic Consortium Catalogue Number (eg. 'LDC2015T05'. This is what forms the last part of the LDC catalogue URL. cert : str, optional, default=None Path to certificate chain; LDC has had a problem with intermediate certificates, so you can download the chain with a browser and supply a path to the .pem with this parameter ''' self.ldc = ldc.strip().upper() self.ldcHtml = None self._ldcJson = None self._dryadJson = None self._dvJson = None self.cert = cert self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=ds.constants.RETRY_STRATEGY)) if self.cert: self.cert = os.path.expanduser(self.cert) self.__fixdesc = None @property def ldcJson(self): ''' Returns a JSON based on the LDC web page scraping ''' if not self._ldcJson: self._ldcJson = self.make_ldc_json() return self._ldcJson @property def dryadJson(self): ''' LDC metadata in Dryad JSON format ''' if not self._dryadJson: self._dryadJson = self.make_dryad_json() return self._dryadJson @property def dvJson(self): ''' LDC metadata in Dataverse JSON format ''' #return False if not self._dvJson: self._dvJson = self.make_dv_json() return self._dvJson @property def embargo(self)->bool: ''' Boolean indicating embargo status ''' return False @property def fileJson(self): ''' Returns False: No attached files possible at LDC ''' return False @property def files(self): ''' Returns None. No files possible ''' return None @property def oversize(self, maxsize=None): ''' Make sure file is not too big for the Dataverse instance Parameters ---------- maxsize : int, optional, default=None Maximum size in bytes ''' #pylint: disable=property-with-parameters if not maxsize: maxsize = ds.constants.MAX_UPLOAD @property def id(self): ''' Returns LDC ID ''' return self.ldc def fetch_record(self, timeout=45): ''' Downloads record from LDC website Parameters ---------- timeout : int, optional, default=45 Request timeout in seconds ''' interim = self.session.get(f'https://catalog.ldc.upenn.edu/{self.ldc}', verify=self.cert, timeout=timeout) interim.raise_for_status() self.ldcHtml = interim.text def make_ldc_json(self): ''' Returns a dict with keys created from an LDC catalogue web page. ''' if not self.ldcHtml: self.fetch_record() soup = bs(self.ldcHtml, 'html.parser') #Should data just look in the *first* table? Specifically tbody? #Is it always the first? I assume yes. tbody = soup.find('tbody')#new data = [x.text.strip() for x in tbody.find_all('td')]#new #data = [x.text.strip() for x in soup.find_all('td')]#original LDC_dict = {data[x][:data[x].find('\\n')].strip(): data[x+1].strip() for x in range(0, len(data), 2)} #Related Works appears to have an extra 'Hide' at the end if LDC_dict.get('Related Works:'): LDC_dict['Related Works'] = (x.strip() for x in LDC_dict['Related Works:'].split('\\n')) del LDC_dict['Related Works:'] #remove the renamed key LDC_dict['Linguistic Data Consortium'] = LDC_dict['LDC Catalog No.'] del LDC_dict['LDC Catalog No.']#This key must be renamed for consistency LDC_dict['Author(s)'] = [x.strip() for x in LDC_dict['Author(s)'].split(',')] #Other metadata probably has HTML in it, so we keep as much as possible other_meta = soup.find_all('div') alldesc = [x for x in other_meta if x.attrs.get('itemprop') == 'description'] #sometimes they format pages oddly and we can use this for a #quick and dirty fix self.__fixdesc = copy.deepcopy(alldesc) #sections use h3, so split on these #24 Jan 23 Apparently, this is all done manually so some of them sometime use h4. #Because reasons. #was: #alldesc = str(alldesc).split('<h3>') #is now alldesc = str(alldesc).replace('h4>', 'h3>').split('<h3>') for i in range(1, len(alldesc)): alldesc[i] = '<h3>' + alldesc[i] #first one is not actually useful, so discard it alldesc.pop(0) #So far, so good. At this point the relative links need fixing #and tables need to be converted to pre. for desc in alldesc: #It's already strings; replace relative links first desc = desc.replace('../../../', 'https://catalog.ldc.upenn.edu/') subsoup = bs(desc, 'html.parser') key = subsoup.h3.text.strip() #don't need the h3 tags anymore subsoup.find('h3').extract() # Convert tables to <pre> for tab in subsoup.find_all('table'): content = str(tab) #convert to markdown content = markdownify.markdownify(content) tab.name = 'pre' tab.string = content #There is not much documentation on the #difference between tab.string and tab.content #That was relatively easy LDC_dict[key] = str(subsoup) LDC_dict['Introduction'] = LDC_dict.get('Introduction', self.__no_intro()) #LDC puts http in front of their DOI identifier if LDC_dict.get('DOI'): LDC_dict['DOI'] = LDC_dict['DOI'].strip('https://doi.org/') return LDC_dict def __no_intro(self)->str: ''' Makes an introduction even if they forgot to include the word \"Introduction\" ''' #self.__fixdesc is set in make_ldc_json intro = [x for x in self.__fixdesc if self.__fixdesc[0]['itemprop']=='description'][0] while intro.find('div'): #nested?, not cleaning properly intro.find('div').unwrap() # remove the div tag intro = str(intro) #Normally, there's an <h3>Introduction</h3> but sometimes there's not #Assumes that the first section up to \"<h\" is an intro. #You know what they say about assuming intro = intro[:intro.find('<h')] start = intro.find('<div') if start != -1: end = intro.find('>',start)+1 intro = intro.replace(intro[start:end], '').strip() return intro @staticmethod def name_parser(name): ''' Returns lastName/firstName JSON snippet from a name Parameters ---------- name : str A name Notes ----- Can't be 100% accurate, because names can be split in many ways. However, as they say, 80% is good enough. ''' names = name.split(' ') return {'lastName': names[-1], 'firstName': ' '.join(names[:-1]), 'affiliation':''} def make_dryad_json(self, ldc=None): ''' Creates a Dryad-style dict from an LDC dictionary Parameters ---------- ldc : dict, optional, default=self.ldcJson Dictionary containing LDC data. Defaults to self.ldcJson ''' if not ldc: ldc = self.ldcJson print(ldc) dryad = {} dryad['title'] = ldc['Item Name'] dryad['authors'] = [Ldc.name_parser(x) for x in ldc['Author(s)']] abstract = ('<p><b>Introduction</b></p>' f\"<p>{ldc['Introduction']}</p>\" '<p><b>Data</b></p>' f\"<p>{ldc['Data']}</p>\") if ldc.get('Acknowledgement'): abstract += ('<p><b>Acknowledgement</b></p>' f\"<p>{ldc['Acknowledgement']}</p>\") dryad['abstract'] = abstract dryad['keywords'] = ['Linguistics'] #Dataverse accepts only ISO formatted date try: releaseDate = time.strptime(ldc['Release Date'], '%B %d, %Y') releaseDate = time.strftime('%Y-%m-%d', releaseDate) except KeyError: #Older surveys don't have a release date field #so it must be created from the record number if self.ldc[3] == '9': releaseDate = '19' + self.ldc[3:5] dryad['lastModificationDate'] = releaseDate dryad['publicationDate'] = releaseDate return dryad def _make_note(self, ldc=None)->str: ''' Creates a generalizes HTML notes field from a bunch of LDC fields that don't fit into dataverse Parameters ---------- ldc : dict, optional, default=self.ldcJson Dictionary containing LDC data ''' if not ldc: ldc = self.ldcJson note_fields = ['DCMI Type(s)', 'Sample Type', 'Sample Rate', 'Application(s)', 'Language(s)', 'Language ID(s)'] outhtml = [] for note in note_fields: if ldc.get(note): data = ldc[note].split(',') data = [x.strip() for x in data] data = ', '.join(data) if note != 'Language ID(s)': data = data[0].capitalize() + data[1:] #data = [x.capitalize() for x in data] outhtml.append(f'{note}: {data}') outhtml.append(f'Metadata automatically created from ' f'<a href=\"https://catalog.ldc.upenn.edu/{self.ldc}\">' f'https://catalog.ldc.upenn.edu/{self.ldc}</a> ' f'[{time.strftime(\"%d %b %Y\", time.localtime())}]') return '<br />'.join(outhtml) @staticmethod def find_block_index(dvjson, key): ''' Finds the index number of an item in Dataverse's idiotic JSON list Parameters ---------- dvjson : dict Dataverse JSON key : str key for which to find list index ''' for num, item in enumerate(dvjson['datasetVersion'] ['metadataBlocks']['citation']['fields']): if item['typeName'] == key: return num return None def make_dv_json(self, ldc=None):#pylint: disable=too-many-locals, too-many-statements ''' Returns complete Dataverse JSON Parameters ---------- ldc : dict, optional, default=self.ldcJson LDC dictionary. ''' if not ldc: ldc = self.ldcJson dvjson = super().dvJson.copy() #ID Numbers otherid = super()._typeclass('otherId', True, 'compound') ids = [] for item in ['Linguistic Data Consortium', 'ISBN', 'ISLRN', 'DOI']: if ldc.get(item): out = {} agency = super()._convert_generic(inJson={item:item}, dryField=item, dvField='otherIdAgency') value = super()._convert_generic(inJson={item:ldc[item]}, dryField=item, dvField='otherIdValue') out.update(agency) out.update(value) ids.append(out) otherid['value'] = ids dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(otherid) #Producer and publisher prod = super()._typeclass('producer', True, 'compound') p_name = super()._convert_generic(inJson={'producerName': 'Linguistic Data Consortium'}, dryField='producerName', dvField='producerName') p_affil = super()._convert_generic(inJson={'producerAffiliation': 'University of Pennsylvania'}, dryField='producerName', dvField='producerName') p_url = super()._convert_generic(inJson={'producerURL': 'https://www.ldc.upenn.edu/'}, dryField='producerURL', dvField='producerURL') p_name.update(p_affil) p_name.update(p_url) prod['value'] = [p_name] dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(prod) #Kind of data kind = super()._typeclass('kindOfData', True, 'primitive') kind['value'] = 'Linguistic data' #Series series = super()._typeclass('series', False, 'compound') s_name = super()._convert_generic(inJson={'seriesName': 'LDC'}, dryField='seriesName', dvField='seriesName') s_info = super()._convert_generic(inJson={'seriesInformation': 'Linguistic Data Consortium'}, dryField='seriesInformation', dvField='seriesInformation') s_name.update(s_info) series['value'] = s_name #not a list dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(series) #Data sources series = super()._typeclass('dataSources', True, 'primitive') data_sources = ldc['Data Source(s)'].split(',') data_sources = [x.strip().capitalize() for x in data_sources] series['value'] = data_sources dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(series) #Fix keyword labels that are hardcoded for Dryad #There should be only one keyword block keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion']['metadataBlocks'] ['citation']['fields']) if y.get('typeName') == 'keyword'][0] key_pos = [x for x, y in enumerate(keyword_field[1]['value']) if y['keywordVocabulary']['value'] == 'Dryad'][0] dvjson['datasetVersion']['metadataBlocks']['citation']\\ ['fields'][keyword_field[0]]['value'][key_pos]\\ ['keywordVocabulary']['value'] = 'Linguistic Data Consortium' #The first keyword field is hardcoded in by dryad2dataverse.serializer #So I think it needs to be deleted keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion']['metadataBlocks']['citation']['fields']) if y.get('typeName') == 'otherId'][0] #ibid del dvjson['datasetVersion']['metadataBlocks']['citation']['fields'][keyword_field[0]] #Notes note_index = Ldc.find_block_index(dvjson, 'notesText') if note_index: dvjson['datasetVersion']['metadataBlocks']['citation']\\ ['fields'][note_index]['value'] = self._make_note() else: notes = super()._typeclass('notesText', False, 'primitive') notes['value'] = self._make_note() dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(notes) #Deletes unused \"publication\" fields: rewrite to make it a function call. keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion'] ['metadataBlocks']['citation']['fields']) if y.get('typeName') == 'publication'][0] #ibid del dvjson['datasetVersion']['metadataBlocks']['citation']['fields'][keyword_field[0]] #And now the licence: dvjson['datasetVersion']['license'] = LIC_NAME dvjson['datasetVersion']['termsOfUse'] = LICENCE return dvjson def upload_metadata(self, **kwargs) -> dict: ''' Uploads metadata to dataverse. Returns json from connection attempt. Parameters ---------- **kwargs : dict Parameters Other parameters ---------------- url : str base url to Dataverse installation key : str api key dv : str Dataverse to which it is being uploaded ''' url = kwargs['url'].strip('/') key = kwargs['key'] dv = kwargs['dv'] json = kwargs.get('json', self.dvJson) headers = {'X-Dataverse-key':key} headers.update(UAHEADER) try: upload = self.session.post(f'{url}/api/dataverses/{dv}/datasets', headers=headers, json=json) upload.raise_for_status() return upload.json() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError): print(upload.text) raise dryadJson property \u00b6 LDC metadata in Dryad JSON format dvJson property \u00b6 LDC metadata in Dataverse JSON format embargo property \u00b6 Boolean indicating embargo status fileJson property \u00b6 Returns False: No attached files possible at LDC files property \u00b6 Returns None. No files possible id property \u00b6 Returns LDC ID ldcJson property \u00b6 Returns a JSON based on the LDC web page scraping oversize property \u00b6 Make sure file is not too big for the Dataverse instance Parameters: maxsize ( int , default: None ) \u2013 Maximum size in bytes __init__(ldc, cert=None) \u00b6 Returns a dict with keys created from an LDC catalogue web page. Parameters: ldc ( str ) \u2013 Linguistic Consortium Catalogue Number (eg. \u2018LDC2015T05\u2019. This is what forms the last part of the LDC catalogue URL. cert ( str , default: None ) \u2013 Path to certificate chain; LDC has had a problem with intermediate certificates, so you can download the chain with a browser and supply a path to the .pem with this parameter Source code in src/dataverse_utils/ldc.py def __init__(self, ldc, cert=None): ''' Returns a dict with keys created from an LDC catalogue web page. Parameters ---------- ldc : str Linguistic Consortium Catalogue Number (eg. 'LDC2015T05'. This is what forms the last part of the LDC catalogue URL. cert : str, optional, default=None Path to certificate chain; LDC has had a problem with intermediate certificates, so you can download the chain with a browser and supply a path to the .pem with this parameter ''' self.ldc = ldc.strip().upper() self.ldcHtml = None self._ldcJson = None self._dryadJson = None self._dvJson = None self.cert = cert self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=ds.constants.RETRY_STRATEGY)) if self.cert: self.cert = os.path.expanduser(self.cert) self.__fixdesc = None __no_intro() \u00b6 Makes an introduction even if they forgot to include the word \u201cIntroduction\u201d Source code in src/dataverse_utils/ldc.py def __no_intro(self)->str: ''' Makes an introduction even if they forgot to include the word \"Introduction\" ''' #self.__fixdesc is set in make_ldc_json intro = [x for x in self.__fixdesc if self.__fixdesc[0]['itemprop']=='description'][0] while intro.find('div'): #nested?, not cleaning properly intro.find('div').unwrap() # remove the div tag intro = str(intro) #Normally, there's an <h3>Introduction</h3> but sometimes there's not #Assumes that the first section up to \"<h\" is an intro. #You know what they say about assuming intro = intro[:intro.find('<h')] start = intro.find('<div') if start != -1: end = intro.find('>',start)+1 intro = intro.replace(intro[start:end], '').strip() return intro fetch_record(timeout=45) \u00b6 Downloads record from LDC website Parameters: timeout ( int , default: 45 ) \u2013 Request timeout in seconds Source code in src/dataverse_utils/ldc.py def fetch_record(self, timeout=45): ''' Downloads record from LDC website Parameters ---------- timeout : int, optional, default=45 Request timeout in seconds ''' interim = self.session.get(f'https://catalog.ldc.upenn.edu/{self.ldc}', verify=self.cert, timeout=timeout) interim.raise_for_status() self.ldcHtml = interim.text find_block_index(dvjson, key) staticmethod \u00b6 Finds the index number of an item in Dataverse\u2019s idiotic JSON list Parameters: dvjson ( dict ) \u2013 Dataverse JSON key ( str ) \u2013 key for which to find list index Source code in src/dataverse_utils/ldc.py @staticmethod def find_block_index(dvjson, key): ''' Finds the index number of an item in Dataverse's idiotic JSON list Parameters ---------- dvjson : dict Dataverse JSON key : str key for which to find list index ''' for num, item in enumerate(dvjson['datasetVersion'] ['metadataBlocks']['citation']['fields']): if item['typeName'] == key: return num return None make_dryad_json(ldc=None) \u00b6 Creates a Dryad-style dict from an LDC dictionary Parameters: ldc ( dict , default: self.ldcJson ) \u2013 Dictionary containing LDC data. Defaults to self.ldcJson Source code in src/dataverse_utils/ldc.py def make_dryad_json(self, ldc=None): ''' Creates a Dryad-style dict from an LDC dictionary Parameters ---------- ldc : dict, optional, default=self.ldcJson Dictionary containing LDC data. Defaults to self.ldcJson ''' if not ldc: ldc = self.ldcJson print(ldc) dryad = {} dryad['title'] = ldc['Item Name'] dryad['authors'] = [Ldc.name_parser(x) for x in ldc['Author(s)']] abstract = ('<p><b>Introduction</b></p>' f\"<p>{ldc['Introduction']}</p>\" '<p><b>Data</b></p>' f\"<p>{ldc['Data']}</p>\") if ldc.get('Acknowledgement'): abstract += ('<p><b>Acknowledgement</b></p>' f\"<p>{ldc['Acknowledgement']}</p>\") dryad['abstract'] = abstract dryad['keywords'] = ['Linguistics'] #Dataverse accepts only ISO formatted date try: releaseDate = time.strptime(ldc['Release Date'], '%B %d, %Y') releaseDate = time.strftime('%Y-%m-%d', releaseDate) except KeyError: #Older surveys don't have a release date field #so it must be created from the record number if self.ldc[3] == '9': releaseDate = '19' + self.ldc[3:5] dryad['lastModificationDate'] = releaseDate dryad['publicationDate'] = releaseDate return dryad make_dv_json(ldc=None) \u00b6 Returns complete Dataverse JSON Parameters: ldc ( dict , default: self.ldcJson ) \u2013 LDC dictionary. Source code in src/dataverse_utils/ldc.py def make_dv_json(self, ldc=None):#pylint: disable=too-many-locals, too-many-statements ''' Returns complete Dataverse JSON Parameters ---------- ldc : dict, optional, default=self.ldcJson LDC dictionary. ''' if not ldc: ldc = self.ldcJson dvjson = super().dvJson.copy() #ID Numbers otherid = super()._typeclass('otherId', True, 'compound') ids = [] for item in ['Linguistic Data Consortium', 'ISBN', 'ISLRN', 'DOI']: if ldc.get(item): out = {} agency = super()._convert_generic(inJson={item:item}, dryField=item, dvField='otherIdAgency') value = super()._convert_generic(inJson={item:ldc[item]}, dryField=item, dvField='otherIdValue') out.update(agency) out.update(value) ids.append(out) otherid['value'] = ids dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(otherid) #Producer and publisher prod = super()._typeclass('producer', True, 'compound') p_name = super()._convert_generic(inJson={'producerName': 'Linguistic Data Consortium'}, dryField='producerName', dvField='producerName') p_affil = super()._convert_generic(inJson={'producerAffiliation': 'University of Pennsylvania'}, dryField='producerName', dvField='producerName') p_url = super()._convert_generic(inJson={'producerURL': 'https://www.ldc.upenn.edu/'}, dryField='producerURL', dvField='producerURL') p_name.update(p_affil) p_name.update(p_url) prod['value'] = [p_name] dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(prod) #Kind of data kind = super()._typeclass('kindOfData', True, 'primitive') kind['value'] = 'Linguistic data' #Series series = super()._typeclass('series', False, 'compound') s_name = super()._convert_generic(inJson={'seriesName': 'LDC'}, dryField='seriesName', dvField='seriesName') s_info = super()._convert_generic(inJson={'seriesInformation': 'Linguistic Data Consortium'}, dryField='seriesInformation', dvField='seriesInformation') s_name.update(s_info) series['value'] = s_name #not a list dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(series) #Data sources series = super()._typeclass('dataSources', True, 'primitive') data_sources = ldc['Data Source(s)'].split(',') data_sources = [x.strip().capitalize() for x in data_sources] series['value'] = data_sources dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(series) #Fix keyword labels that are hardcoded for Dryad #There should be only one keyword block keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion']['metadataBlocks'] ['citation']['fields']) if y.get('typeName') == 'keyword'][0] key_pos = [x for x, y in enumerate(keyword_field[1]['value']) if y['keywordVocabulary']['value'] == 'Dryad'][0] dvjson['datasetVersion']['metadataBlocks']['citation']\\ ['fields'][keyword_field[0]]['value'][key_pos]\\ ['keywordVocabulary']['value'] = 'Linguistic Data Consortium' #The first keyword field is hardcoded in by dryad2dataverse.serializer #So I think it needs to be deleted keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion']['metadataBlocks']['citation']['fields']) if y.get('typeName') == 'otherId'][0] #ibid del dvjson['datasetVersion']['metadataBlocks']['citation']['fields'][keyword_field[0]] #Notes note_index = Ldc.find_block_index(dvjson, 'notesText') if note_index: dvjson['datasetVersion']['metadataBlocks']['citation']\\ ['fields'][note_index]['value'] = self._make_note() else: notes = super()._typeclass('notesText', False, 'primitive') notes['value'] = self._make_note() dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(notes) #Deletes unused \"publication\" fields: rewrite to make it a function call. keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion'] ['metadataBlocks']['citation']['fields']) if y.get('typeName') == 'publication'][0] #ibid del dvjson['datasetVersion']['metadataBlocks']['citation']['fields'][keyword_field[0]] #And now the licence: dvjson['datasetVersion']['license'] = LIC_NAME dvjson['datasetVersion']['termsOfUse'] = LICENCE return dvjson make_ldc_json() \u00b6 Returns a dict with keys created from an LDC catalogue web page. Source code in src/dataverse_utils/ldc.py def make_ldc_json(self): ''' Returns a dict with keys created from an LDC catalogue web page. ''' if not self.ldcHtml: self.fetch_record() soup = bs(self.ldcHtml, 'html.parser') #Should data just look in the *first* table? Specifically tbody? #Is it always the first? I assume yes. tbody = soup.find('tbody')#new data = [x.text.strip() for x in tbody.find_all('td')]#new #data = [x.text.strip() for x in soup.find_all('td')]#original LDC_dict = {data[x][:data[x].find('\\n')].strip(): data[x+1].strip() for x in range(0, len(data), 2)} #Related Works appears to have an extra 'Hide' at the end if LDC_dict.get('Related Works:'): LDC_dict['Related Works'] = (x.strip() for x in LDC_dict['Related Works:'].split('\\n')) del LDC_dict['Related Works:'] #remove the renamed key LDC_dict['Linguistic Data Consortium'] = LDC_dict['LDC Catalog No.'] del LDC_dict['LDC Catalog No.']#This key must be renamed for consistency LDC_dict['Author(s)'] = [x.strip() for x in LDC_dict['Author(s)'].split(',')] #Other metadata probably has HTML in it, so we keep as much as possible other_meta = soup.find_all('div') alldesc = [x for x in other_meta if x.attrs.get('itemprop') == 'description'] #sometimes they format pages oddly and we can use this for a #quick and dirty fix self.__fixdesc = copy.deepcopy(alldesc) #sections use h3, so split on these #24 Jan 23 Apparently, this is all done manually so some of them sometime use h4. #Because reasons. #was: #alldesc = str(alldesc).split('<h3>') #is now alldesc = str(alldesc).replace('h4>', 'h3>').split('<h3>') for i in range(1, len(alldesc)): alldesc[i] = '<h3>' + alldesc[i] #first one is not actually useful, so discard it alldesc.pop(0) #So far, so good. At this point the relative links need fixing #and tables need to be converted to pre. for desc in alldesc: #It's already strings; replace relative links first desc = desc.replace('../../../', 'https://catalog.ldc.upenn.edu/') subsoup = bs(desc, 'html.parser') key = subsoup.h3.text.strip() #don't need the h3 tags anymore subsoup.find('h3').extract() # Convert tables to <pre> for tab in subsoup.find_all('table'): content = str(tab) #convert to markdown content = markdownify.markdownify(content) tab.name = 'pre' tab.string = content #There is not much documentation on the #difference between tab.string and tab.content #That was relatively easy LDC_dict[key] = str(subsoup) LDC_dict['Introduction'] = LDC_dict.get('Introduction', self.__no_intro()) #LDC puts http in front of their DOI identifier if LDC_dict.get('DOI'): LDC_dict['DOI'] = LDC_dict['DOI'].strip('https://doi.org/') return LDC_dict name_parser(name) staticmethod \u00b6 Returns lastName/firstName JSON snippet from a name Parameters: name ( str ) \u2013 A name Notes Can\u2019t be 100% accurate, because names can be split in many ways. However, as they say, 80% is good enough. Source code in src/dataverse_utils/ldc.py @staticmethod def name_parser(name): ''' Returns lastName/firstName JSON snippet from a name Parameters ---------- name : str A name Notes ----- Can't be 100% accurate, because names can be split in many ways. However, as they say, 80% is good enough. ''' names = name.split(' ') return {'lastName': names[-1], 'firstName': ' '.join(names[:-1]), 'affiliation':''} upload_metadata(**kwargs) \u00b6 Uploads metadata to dataverse. Returns json from connection attempt. Parameters: **kwargs ( dict , default: {} ) \u2013 Parameters url ( str ) \u2013 base url to Dataverse installation key ( str ) \u2013 api key dv ( str ) \u2013 Dataverse to which it is being uploaded Source code in src/dataverse_utils/ldc.py def upload_metadata(self, **kwargs) -> dict: ''' Uploads metadata to dataverse. Returns json from connection attempt. Parameters ---------- **kwargs : dict Parameters Other parameters ---------------- url : str base url to Dataverse installation key : str api key dv : str Dataverse to which it is being uploaded ''' url = kwargs['url'].strip('/') key = kwargs['key'] dv = kwargs['dv'] json = kwargs.get('json', self.dvJson) headers = {'X-Dataverse-key':key} headers.update(UAHEADER) try: upload = self.session.post(f'{url}/api/dataverses/{dv}/datasets', headers=headers, json=json) upload.raise_for_status() return upload.json() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError): print(upload.text) raise dataverse_utils.collections \u00b6 Utilities for recursively analysing a Dataverse collection. DvCollection \u00b6 Metadata for an entire dataverse collection, recursively. Source code in src/dataverse_utils/collections.py class DvCollection: ''' Metadata for an *entire* dataverse collection, recursively. ''' #pylint: disable=too-many-instance-attributes def __init__(self, url:str, coll:str, key=None, **kwargs): ''' All you need to start recursively crawling. Parameters ---------- coll : str short collection name or id number url : str base URL of Dataverse collection. eg: https://borealisdata.ca borealisdata.ca key : str API key (optional, only use if you want to see hidden material) **kwargs: dict Other parameters Other parameters ---------------- timeout : int retry timeout in seconds ''' self.coll = coll self.url = self.__clean_url(url) self.headers = None self.__key = key if self.__key: self.headers = {'X-Dataverse-key': self.__key} self.headers.update(UAHEADER) else: self.headers = UAHEADER.copy() if not kwargs.get('retry'): self.retry_strategy = RETRY else: self.retry_strategy = kwargs['retry'] self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=self.retry_strategy)) self.collections = None self.studies = None def __clean_url(self, badurl:str): ''' Sanitize URL, return properly formatted HTTP string. Parameters ---------- badurl: str URL string ''' clean = badurl.strip().strip('/') if not clean.startswith('https://'): clean = f'https://{clean}' return clean def __get_shortname(self, dvid): ''' Get collection short name. ''' shortname = self.session.get(f'{self.url}/api/dataverses/{dvid}', headers=self.headers) shortname.raise_for_status() return shortname.json()['data']['alias'] def get_collections(self, coll:str=None, output=None, **kwargs)->list:#pylint: disable=unused-argument ''' Get a [recursive] listing of all dataverses in a collection. Parameters ---------- coll : str, optional, default=None Collection short name or id output : list, optional, default=[] output list to append to **kwargs : dict Other keyword arguments ''' if not output: output = [] if not coll: coll = self.coll x = self.session.get(f'{self.url}/api/dataverses/{coll}/contents', headers=self.headers) data = x.json().get('data') #--- #Because it's possible that permissions errors can cause API read errors, #we have this insane way of checking errors. #I have no idea what kind of errors would be raised, so it has #a bare except, which is bad. But what can you do? dvs =[] for _ in data: if _['type'] == 'dataverse': try: out=self.__get_shortname(_['id']) dvs.append((_['title'], out)) except Exception as e: obscure_error = f''' An error has occured where a collection can be identified by ID but its name cannot be determined. This is (normally) caused by a configuration error where administrator permissions are not correctly inherited by the child collection. Please check with the system administrator to determine any exact issues. Problematic collection id number: {_.get(\"id\", \"not available\")}''' print(50*'-') print(textwrap.dedent(obscure_error)) print(e) LOGGER.error(textwrap.fill(textwrap.dedent(obscure_error).strip())) traceback.print_exc() print(50*'-') raise e #--- if not dvs: dvs = [] output.extend(dvs) for dv in dvs: LOGGER.debug('%s/api/dataverses/%s/contents', self.url, dv[1]) LOGGER.debug('recursive') self.get_collections(dv[1], output) self.collections = output return output def get_studies(self, root:str=None): ''' return [(pid, title)..(pid_n, title_n)] of a collection. Parameters ---------- root : str Short name or id of *top* level of tree. Default self.coll ''' all_studies = [] if not root: root=self.coll all_studies = self.get_collection_listing(root) #collections = self.get_collections(root, self.url) collections = self.get_collections(root) for collection in collections: all_studies.extend(self.get_collection_listing(collection[1])) self.studies = all_studies return all_studies def get_collection_listing(self, coll_id): ''' Return a listing of studies in a collection, with pid. Parameters ---------- coll_id : str Short name or id of a dataverse collection ''' cl = self.session.get(f'{self.url}/api/dataverses/{coll_id}/contents', headers=self.headers) cl.raise_for_status() pids = [f\"{z['protocol']}:{z['authority']}/{z['identifier']}\" for z in cl.json()['data'] if z['type'] == 'dataset'] out = [(self.get_study_info(pid), pid) for pid in pids] for _ in out: _[0].update({'pid': _[1]}) return [x[0] for x in out] def get_study_info(self, pid): ''' Returns a StudyMetadata object with complete metadata for a study. Parameters ---------- pid : str Persistent ID of a Dataverse study ''' meta = self.session.get(f'{self.url}/api/datasets/:persistentId', params={'persistentId': pid}, headers=self.headers) meta.raise_for_status() LOGGER.debug(pid) return StudyMetadata(study_meta=meta.json(), key=self.__key, url=self.url) __clean_url(badurl) \u00b6 Sanitize URL, return properly formatted HTTP string. Parameters: badurl ( str ) \u2013 URL string Source code in src/dataverse_utils/collections.py def __clean_url(self, badurl:str): ''' Sanitize URL, return properly formatted HTTP string. Parameters ---------- badurl: str URL string ''' clean = badurl.strip().strip('/') if not clean.startswith('https://'): clean = f'https://{clean}' return clean __get_shortname(dvid) \u00b6 Get collection short name. Source code in src/dataverse_utils/collections.py def __get_shortname(self, dvid): ''' Get collection short name. ''' shortname = self.session.get(f'{self.url}/api/dataverses/{dvid}', headers=self.headers) shortname.raise_for_status() return shortname.json()['data']['alias'] __init__(url, coll, key=None, **kwargs) \u00b6 All you need to start recursively crawling. Parameters: coll ( str ) \u2013 short collection name or id number url ( str ) \u2013 base URL of Dataverse collection. eg: https://borealisdata.ca borealisdata.ca key ( str , default: None ) \u2013 API key (optional, only use if you want to see hidden material) **kwargs \u2013 Other parameters timeout ( int ) \u2013 retry timeout in seconds Source code in src/dataverse_utils/collections.py def __init__(self, url:str, coll:str, key=None, **kwargs): ''' All you need to start recursively crawling. Parameters ---------- coll : str short collection name or id number url : str base URL of Dataverse collection. eg: https://borealisdata.ca borealisdata.ca key : str API key (optional, only use if you want to see hidden material) **kwargs: dict Other parameters Other parameters ---------------- timeout : int retry timeout in seconds ''' self.coll = coll self.url = self.__clean_url(url) self.headers = None self.__key = key if self.__key: self.headers = {'X-Dataverse-key': self.__key} self.headers.update(UAHEADER) else: self.headers = UAHEADER.copy() if not kwargs.get('retry'): self.retry_strategy = RETRY else: self.retry_strategy = kwargs['retry'] self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=self.retry_strategy)) self.collections = None self.studies = None get_collection_listing(coll_id) \u00b6 Return a listing of studies in a collection, with pid. Parameters: coll_id ( str ) \u2013 Short name or id of a dataverse collection Source code in src/dataverse_utils/collections.py def get_collection_listing(self, coll_id): ''' Return a listing of studies in a collection, with pid. Parameters ---------- coll_id : str Short name or id of a dataverse collection ''' cl = self.session.get(f'{self.url}/api/dataverses/{coll_id}/contents', headers=self.headers) cl.raise_for_status() pids = [f\"{z['protocol']}:{z['authority']}/{z['identifier']}\" for z in cl.json()['data'] if z['type'] == 'dataset'] out = [(self.get_study_info(pid), pid) for pid in pids] for _ in out: _[0].update({'pid': _[1]}) return [x[0] for x in out] get_collections(coll=None, output=None, **kwargs) \u00b6 Get a [recursive] listing of all dataverses in a collection. Parameters: coll ( str , default: None ) \u2013 Collection short name or id output ( list , default: [] ) \u2013 output list to append to **kwargs ( dict , default: {} ) \u2013 Other keyword arguments Source code in src/dataverse_utils/collections.py def get_collections(self, coll:str=None, output=None, **kwargs)->list:#pylint: disable=unused-argument ''' Get a [recursive] listing of all dataverses in a collection. Parameters ---------- coll : str, optional, default=None Collection short name or id output : list, optional, default=[] output list to append to **kwargs : dict Other keyword arguments ''' if not output: output = [] if not coll: coll = self.coll x = self.session.get(f'{self.url}/api/dataverses/{coll}/contents', headers=self.headers) data = x.json().get('data') #--- #Because it's possible that permissions errors can cause API read errors, #we have this insane way of checking errors. #I have no idea what kind of errors would be raised, so it has #a bare except, which is bad. But what can you do? dvs =[] for _ in data: if _['type'] == 'dataverse': try: out=self.__get_shortname(_['id']) dvs.append((_['title'], out)) except Exception as e: obscure_error = f''' An error has occured where a collection can be identified by ID but its name cannot be determined. This is (normally) caused by a configuration error where administrator permissions are not correctly inherited by the child collection. Please check with the system administrator to determine any exact issues. Problematic collection id number: {_.get(\"id\", \"not available\")}''' print(50*'-') print(textwrap.dedent(obscure_error)) print(e) LOGGER.error(textwrap.fill(textwrap.dedent(obscure_error).strip())) traceback.print_exc() print(50*'-') raise e #--- if not dvs: dvs = [] output.extend(dvs) for dv in dvs: LOGGER.debug('%s/api/dataverses/%s/contents', self.url, dv[1]) LOGGER.debug('recursive') self.get_collections(dv[1], output) self.collections = output return output get_studies(root=None) \u00b6 return [(pid, title)..(pid_n, title_n)] of a collection. Parameters: root ( str , default: None ) \u2013 Short name or id of top level of tree. Default self.coll Source code in src/dataverse_utils/collections.py def get_studies(self, root:str=None): ''' return [(pid, title)..(pid_n, title_n)] of a collection. Parameters ---------- root : str Short name or id of *top* level of tree. Default self.coll ''' all_studies = [] if not root: root=self.coll all_studies = self.get_collection_listing(root) #collections = self.get_collections(root, self.url) collections = self.get_collections(root) for collection in collections: all_studies.extend(self.get_collection_listing(collection[1])) self.studies = all_studies return all_studies get_study_info(pid) \u00b6 Returns a StudyMetadata object with complete metadata for a study. Parameters: pid ( str ) \u2013 Persistent ID of a Dataverse study Source code in src/dataverse_utils/collections.py def get_study_info(self, pid): ''' Returns a StudyMetadata object with complete metadata for a study. Parameters ---------- pid : str Persistent ID of a Dataverse study ''' meta = self.session.get(f'{self.url}/api/datasets/:persistentId', params={'persistentId': pid}, headers=self.headers) meta.raise_for_status() LOGGER.debug(pid) return StudyMetadata(study_meta=meta.json(), key=self.__key, url=self.url) FileAnalysis \u00b6 Bases: dict Download and analyze a file from a dataverse installation and produce useful metadata. Source code in src/dataverse_utils/collections.py class FileAnalysis(dict): ''' Download and analyze a file from a dataverse installation and produce useful metadata. ''' def __init__(self, **kwargs): ''' Intialize the object. Parameters ---------- **kwargs : dict Keyword arguments Other parameters ---------------- local : str Path to local file url : str URL of Dataverse instance key : str API key for downloading fid : int Integer file id pid : str Persistent ID of file filename : str File name (original) filesize_bytes : int File size in bytes Notes ----- Either `local` must be supplied, or `url`, `key` and at least one of `fid` or `pid` must be supplied ''' #self.url = self.__clean_url(url) self.headers = UAHEADER.copy() self.kwargs = kwargs if self.kwargs.get('key'): self.headers.update({'X-Dataverse-key':self.kwargs['key']}) self.local = None if not self.__sufficient: err = ('Insufficient required arguments. ' 'Include (url, key, ' '(pid or id)) or (local) keyword parameters.') raise TypeError(err) self.tempfile = None self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=RETRY)) self.checkable = {'.sav': self.stat_file_metadata, '.sas7bdat': self.stat_file_metadata, '.dta': self.stat_file_metadata, '.csv': self.generic_metadata, '.tsv': self.generic_metadata, '.rdata': self.generic_metadata, '.rda': self.generic_metadata} self.filename = None #get it later self.enhance() def __del__(self): ''' Cleanup old temporary files on object deletion. ''' self.session.close() del self.tempfile def __sufficient(self)->bool: ''' Checks if sufficient information is supplied for intialization, with local files taking preference over remote. ''' if self.kwargs.get('local'): return True if (self.kwargs['url'] and self.kwargs['key'] and (self.kwargs.get('pid') or self.kwargs.get('id'))): return True return False def __clean_url(self, badurl:str)->str: ''' Sanitize URL. Ensures ssl and no trailing slash. Parameters ---------- badurl: str URL ''' clean = badurl.strip().strip('/') if not clean.startswith('https://'): clean = f'https://{clean}' return clean def __get_filename(self, head:dict)->typing.Union[str, None]: ''' Determines whether or not this is a file that should be downloaded for further checking. Parameters ---------- head : dict Header from GET request Returns ------- True if extended metadata can be obtained ''' fname = head.get('content-type') if fname: if 'name=' in fname: start = head['content-type'].find('name=')+5 end = head['content-type'].find(';', start) if end != -1: fname = head['content-type'][start:end].strip('\"') else: fname = head['content-type'][start:].strip('\"') fname = self.kwargs.get('filename', fname) return fname @property def __whichfile(self): ''' Returns the location of the path being analyzed. ''' return self.tempfile.name if self.tempfile else self.local def __check(self): ''' Determines if this is one of the filetypes which supports extra metadata. ''' if pathlib.Path(self.filename).suffix.lower() in self.checkable: return True return False def download(self, block_size:int=1024, force=False, local=None)-> None: ''' Download the file to a temporary location for analysis. -------------------- block_size : int Streaming block size force : bool Download even if not a file that is checkable local : str Path to local file ''' # pylint: disable=consider-using-with self.tempfile = tempfile.NamedTemporaryFile(delete=True, delete_on_close=False) if local: self.local = local self.filename = local self.tempfile.close() del self.tempfile #to erase it self.tempfile = None return start = datetime.datetime.now() params = {'format':'original'} url = self.__clean_url(self.kwargs['url']) if self.kwargs.get('pid'): params.update({'persistentId':self.kwargs['pid']}) data = self.session.get(f'{url}/api/access/datafile/:persistentId', headers=self.headers, params=params, stream=True) else: data = self.session.get(f'{url}/api/access/datafile/{self.kwargs[\"id\"]}', headers=self.headers, params=params, stream=True) data.raise_for_status() finish = datetime.datetime.now() self.filename = self.__get_filename(data.headers) LOGGER.info('Downloaded header for %s. Elapsed time: %s', self.filename, finish-start) if self.__check() or force: filesize = self.kwargs.get('filesize_bytes', data.headers.get('content-length', 9e9)) filesize = int(filesize) # comes out as string from header with tqdm.tqdm(total=filesize, unit='B', unit_scale=True, desc=self.filename) as t: for _ in data.iter_content(block_size): self.tempfile.file.write(_) t.update(len(_)) self.tempfile.close() def enhance(self): ''' Convenience function for downloading and creating extra metadata, ie, \"enhancing\" the metadata. Use this instead of going through the steps manually. ''' self.download(local=self.kwargs.get('local')) do_it = pathlib.Path(self.filename).suffix.lower() if do_it in self.checkable: self.checkable[do_it](ext=do_it) def stat_file_metadata(self, ext:str)->dict: ''' Produces metadata from SAS, SPSS and Stata files. Parameters ---------- ext : str File extension of statistical package file. Include the '.'. Eg. '.sav' ''' matcher = {'.sav': pyreadstat.read_sav, '.dta': pyreadstat.read_dta, '.sas7bdat': pyreadstat.read_sas7bdat} if not self.filename or ext not in matcher: return #whichfile = self.tempfile.name if self.tempfile else self.local statdata, meta = matcher[ext](self.__whichfile) outmeta = {} outmeta['variables'] = {_:{} for _ in meta.column_names_to_labels} for k, v in meta.column_names_to_labels.items(): outmeta['variables'][k]['Variable label'] = v for k, v in meta.original_variable_types.items(): outmeta['variables'][k]['Variable type'] = v for k, v in meta.variable_to_label.items(): outmeta['variables'][k]['Value labels'] = meta.value_labels.get(v, '') outmeta['encoding'] = meta.file_encoding for dt in statdata.columns: desc = {k:str(v) for k, v in dict(statdata[dt].describe()).items()} outmeta['variables'][dt].update(desc) self.update(outmeta) return def generic_metadata(self, ext)->None: ''' Make metadata for a [ct]sv file and RData. Updates self. Parameters ---------- ext : str extension ('.csv' or '.tsv') ''' #if ext == '.tsv': # data = pd.read_csv(self.__whichfile, sep='\\t') #else: # data = pd.read_csv(self.__whichfile) lookuptable ={'.tsv': {'func': pd.read_csv, 'kwargs' : {'sep':'\\t'}}, '.csv': {'func' : pd.read_csv}, '.rda': {'func' : pyreadr.read_r}, '.rdata':{'func' : pyreadr.read_r}} data = lookuptable[ext]['func'](self.__whichfile, **lookuptable[ext].get('kwargs', {})) if ext in ['.rda', '.rdata']: data = data[None] #why pyreadr why outmeta = {} outmeta['variables'] = {_:{} for _ in data.columns} for dt in data.columns: outmeta['variables'][dt]['Variable type'] = str(data[dt].dtype) # Make something from nothing desc = {k:str(v) for k, v in dict(data[dt].describe()).items()} outmeta['variables'][dt].update(desc) self.update(outmeta) @property def md(self): ''' Create Markdown text out of a FileAnalysis object. ''' out = io.StringIO() indent = '\\u00A0' # &nbsp; if not self.get('variables'): return None for k, v in self.items(): if k != 'variables': out.write(f'**{k.capitalize()}** : {v} \\n') for k, v in self.get('variables',{}).items(): out.write(f\"**{k}**: {v.get('Variable label', 'Description N/A')} \\n\") for kk, vv, in v.items(): if kk == 'Variable label': continue if not isinstance(vv, dict): out.write(f'**{kk.capitalize()}**: {vv} \\n') else: out.write(f'**{kk.capitalize()}**: \\n') for kkk, vvv in vv.items(): #this one only originally out.write(f'{4*indent}{kkk}: {vvv} \\n') out.write('\\n') out.seek(0) return out.read() __whichfile property \u00b6 Returns the location of the path being analyzed. md property \u00b6 Create Markdown text out of a FileAnalysis object. __check() \u00b6 Determines if this is one of the filetypes which supports extra metadata. Source code in src/dataverse_utils/collections.py def __check(self): ''' Determines if this is one of the filetypes which supports extra metadata. ''' if pathlib.Path(self.filename).suffix.lower() in self.checkable: return True return False __clean_url(badurl) \u00b6 Sanitize URL. Ensures ssl and no trailing slash. Parameters: badurl ( str ) \u2013 URL Source code in src/dataverse_utils/collections.py def __clean_url(self, badurl:str)->str: ''' Sanitize URL. Ensures ssl and no trailing slash. Parameters ---------- badurl: str URL ''' clean = badurl.strip().strip('/') if not clean.startswith('https://'): clean = f'https://{clean}' return clean __del__() \u00b6 Cleanup old temporary files on object deletion. Source code in src/dataverse_utils/collections.py def __del__(self): ''' Cleanup old temporary files on object deletion. ''' self.session.close() del self.tempfile __get_filename(head) \u00b6 Determines whether or not this is a file that should be downloaded for further checking. Parameters: head ( dict ) \u2013 Header from GET request Returns: True if extended metadata can be obtained \u2013 Source code in src/dataverse_utils/collections.py def __get_filename(self, head:dict)->typing.Union[str, None]: ''' Determines whether or not this is a file that should be downloaded for further checking. Parameters ---------- head : dict Header from GET request Returns ------- True if extended metadata can be obtained ''' fname = head.get('content-type') if fname: if 'name=' in fname: start = head['content-type'].find('name=')+5 end = head['content-type'].find(';', start) if end != -1: fname = head['content-type'][start:end].strip('\"') else: fname = head['content-type'][start:].strip('\"') fname = self.kwargs.get('filename', fname) return fname __init__(**kwargs) \u00b6 Intialize the object. Parameters: **kwargs ( dict , default: {} ) \u2013 Keyword arguments local ( str ) \u2013 Path to local file url ( str ) \u2013 URL of Dataverse instance key ( str ) \u2013 API key for downloading fid ( int ) \u2013 Integer file id pid ( str ) \u2013 Persistent ID of file filename ( str ) \u2013 File name (original) filesize_bytes ( int ) \u2013 File size in bytes Notes Either local must be supplied, or url , key and at least one of fid or pid must be supplied Source code in src/dataverse_utils/collections.py def __init__(self, **kwargs): ''' Intialize the object. Parameters ---------- **kwargs : dict Keyword arguments Other parameters ---------------- local : str Path to local file url : str URL of Dataverse instance key : str API key for downloading fid : int Integer file id pid : str Persistent ID of file filename : str File name (original) filesize_bytes : int File size in bytes Notes ----- Either `local` must be supplied, or `url`, `key` and at least one of `fid` or `pid` must be supplied ''' #self.url = self.__clean_url(url) self.headers = UAHEADER.copy() self.kwargs = kwargs if self.kwargs.get('key'): self.headers.update({'X-Dataverse-key':self.kwargs['key']}) self.local = None if not self.__sufficient: err = ('Insufficient required arguments. ' 'Include (url, key, ' '(pid or id)) or (local) keyword parameters.') raise TypeError(err) self.tempfile = None self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=RETRY)) self.checkable = {'.sav': self.stat_file_metadata, '.sas7bdat': self.stat_file_metadata, '.dta': self.stat_file_metadata, '.csv': self.generic_metadata, '.tsv': self.generic_metadata, '.rdata': self.generic_metadata, '.rda': self.generic_metadata} self.filename = None #get it later self.enhance() __sufficient() \u00b6 Checks if sufficient information is supplied for intialization, with local files taking preference over remote. Source code in src/dataverse_utils/collections.py def __sufficient(self)->bool: ''' Checks if sufficient information is supplied for intialization, with local files taking preference over remote. ''' if self.kwargs.get('local'): return True if (self.kwargs['url'] and self.kwargs['key'] and (self.kwargs.get('pid') or self.kwargs.get('id'))): return True return False download(block_size=1024, force=False, local=None) \u00b6 Download the file to a temporary location for analysis. block_size : int Streaming block size force : bool Download even if not a file that is checkable local : str Path to local file Source code in src/dataverse_utils/collections.py def download(self, block_size:int=1024, force=False, local=None)-> None: ''' Download the file to a temporary location for analysis. -------------------- block_size : int Streaming block size force : bool Download even if not a file that is checkable local : str Path to local file ''' # pylint: disable=consider-using-with self.tempfile = tempfile.NamedTemporaryFile(delete=True, delete_on_close=False) if local: self.local = local self.filename = local self.tempfile.close() del self.tempfile #to erase it self.tempfile = None return start = datetime.datetime.now() params = {'format':'original'} url = self.__clean_url(self.kwargs['url']) if self.kwargs.get('pid'): params.update({'persistentId':self.kwargs['pid']}) data = self.session.get(f'{url}/api/access/datafile/:persistentId', headers=self.headers, params=params, stream=True) else: data = self.session.get(f'{url}/api/access/datafile/{self.kwargs[\"id\"]}', headers=self.headers, params=params, stream=True) data.raise_for_status() finish = datetime.datetime.now() self.filename = self.__get_filename(data.headers) LOGGER.info('Downloaded header for %s. Elapsed time: %s', self.filename, finish-start) if self.__check() or force: filesize = self.kwargs.get('filesize_bytes', data.headers.get('content-length', 9e9)) filesize = int(filesize) # comes out as string from header with tqdm.tqdm(total=filesize, unit='B', unit_scale=True, desc=self.filename) as t: for _ in data.iter_content(block_size): self.tempfile.file.write(_) t.update(len(_)) self.tempfile.close() enhance() \u00b6 Convenience function for downloading and creating extra metadata, ie, \u201cenhancing\u201d the metadata. Use this instead of going through the steps manually. Source code in src/dataverse_utils/collections.py def enhance(self): ''' Convenience function for downloading and creating extra metadata, ie, \"enhancing\" the metadata. Use this instead of going through the steps manually. ''' self.download(local=self.kwargs.get('local')) do_it = pathlib.Path(self.filename).suffix.lower() if do_it in self.checkable: self.checkable[do_it](ext=do_it) generic_metadata(ext) \u00b6 Make metadata for a [ct]sv file and RData. Updates self. Parameters: ext ( str ) \u2013 extension (\u2018.csv\u2019 or \u2018.tsv\u2019) Source code in src/dataverse_utils/collections.py def generic_metadata(self, ext)->None: ''' Make metadata for a [ct]sv file and RData. Updates self. Parameters ---------- ext : str extension ('.csv' or '.tsv') ''' #if ext == '.tsv': # data = pd.read_csv(self.__whichfile, sep='\\t') #else: # data = pd.read_csv(self.__whichfile) lookuptable ={'.tsv': {'func': pd.read_csv, 'kwargs' : {'sep':'\\t'}}, '.csv': {'func' : pd.read_csv}, '.rda': {'func' : pyreadr.read_r}, '.rdata':{'func' : pyreadr.read_r}} data = lookuptable[ext]['func'](self.__whichfile, **lookuptable[ext].get('kwargs', {})) if ext in ['.rda', '.rdata']: data = data[None] #why pyreadr why outmeta = {} outmeta['variables'] = {_:{} for _ in data.columns} for dt in data.columns: outmeta['variables'][dt]['Variable type'] = str(data[dt].dtype) # Make something from nothing desc = {k:str(v) for k, v in dict(data[dt].describe()).items()} outmeta['variables'][dt].update(desc) self.update(outmeta) stat_file_metadata(ext) \u00b6 Produces metadata from SAS, SPSS and Stata files. Parameters: ext ( str ) \u2013 File extension of statistical package file. Include the \u2018.\u2019. Eg. \u2018.sav\u2019 Source code in src/dataverse_utils/collections.py def stat_file_metadata(self, ext:str)->dict: ''' Produces metadata from SAS, SPSS and Stata files. Parameters ---------- ext : str File extension of statistical package file. Include the '.'. Eg. '.sav' ''' matcher = {'.sav': pyreadstat.read_sav, '.dta': pyreadstat.read_dta, '.sas7bdat': pyreadstat.read_sas7bdat} if not self.filename or ext not in matcher: return #whichfile = self.tempfile.name if self.tempfile else self.local statdata, meta = matcher[ext](self.__whichfile) outmeta = {} outmeta['variables'] = {_:{} for _ in meta.column_names_to_labels} for k, v in meta.column_names_to_labels.items(): outmeta['variables'][k]['Variable label'] = v for k, v in meta.original_variable_types.items(): outmeta['variables'][k]['Variable type'] = v for k, v in meta.variable_to_label.items(): outmeta['variables'][k]['Value labels'] = meta.value_labels.get(v, '') outmeta['encoding'] = meta.file_encoding for dt in statdata.columns: desc = {k:str(v) for k, v in dict(statdata[dt].describe()).items()} outmeta['variables'][dt].update(desc) self.update(outmeta) return MetadataError \u00b6 Bases: Exception MetadataError Source code in src/dataverse_utils/collections.py class MetadataError(Exception): ''' MetadataError ''' ReadmeCreator \u00b6 Make formatted README documents out of a StudyMetadata object. Source code in src/dataverse_utils/collections.py class ReadmeCreator: ''' Make formatted README documents out of a StudyMetadata object. ''' def __init__(self, study_metadata_obj: StudyMetadata, **kwargs): ''' Send in StudyMetadata dict to create a nicely formatted README document Parameters ---------- study_metadata_obj : StudyMetadata A study metadata object **kwargs : dict Keyword arguments Other parameters ---------------- url : str The base URL for a Dataverse instance pid : typing.Union[str, int] The persistent identifier of a file or a file id key : str A valid API key for performing operations on Dataverse studies local : str Path to the top level directory which holds study files. If present, the Readme creator will try to create extended data from local files instead of downloading. Notes ----- Either `local` must be supplied, or `url`, `pid` and `key` must supplied ''' self.meta = study_metadata_obj self.kwargs = kwargs warnings.filterwarnings('ignore', category=bs4.MarkupResemblesLocatorWarning) #These values are the first part of the keys that need #concatenation to make them more legible. self.concat = ['author', 'datasetContact','otherId', 'keyword', 'topic', 'publication', 'producer', 'production', 'distributor', 'series', 'software', 'dsDescription', 'grant', 'contributor'] def __html_to_md(self, inval:str)->str: ''' Convert any HTML to markdown, or as much as possible. Parameters ---------- inval : str HTML string to convert ''' if isinstance(inval, str): #markdownify kwargs are here: #https://github.com/matthewwithanm/python-markdownify return markdownify.markdownify(inval) return str(inval) def make_md_heads(self, inkey:str)->str: ''' Make markdown H2 headings for selected sections, currently title, description, licence and terms of use. Parameters ---------- inkey : str Section heading ''' section_heads = {'Title':'## ', 'Description':'**Description**\\n\\n', 'Licence': '### Licence\\n\\n', 'Terms of Use': '### Terms of Use\\n\\n'} if inkey in section_heads: return section_heads[inkey] multi = [self.rename_field(_) for _ in self.concat] if inkey in multi: if inkey not in ['Series', 'Software', 'Production']: return f'{inkey}(s): \\n' return f'{inkey}: \\n' return f'{inkey}: ' @property def file_metadata_md(self)->str: ''' Produce pretty markdown for file metadata. Outputs markdown text string. ''' fmeta = [] for fil in self.meta.files: fileout = {} fileout['File'] = fil['filename'] for k, v in fil.items(): fileout[k.capitalize().replace('_',' ').replace('Pid', 'Persistent Identifier')] = v fileout['Message digest'] = f'{fileout[\"Chk type\"]}: {fileout[\"Chk digest\"]}' for rem in ['Chk type', 'Chk digest', 'Id', 'Has tab file', 'Study pid', 'File label', 'Filename']: del fileout[rem] #not everyone has a pid for the file if not fileout.get('Persistent Identifier'): del fileout['Persistent Identifier'] # Should I only have remote material here? What about # local files? if self.kwargs.get('local'): #TODO, if local fpath = pathlib.Path(self.kwargs['local']) #And from here you have to walk the tree to get the file in fil['filename'] #One day I will do this elif self.meta.kwargs.get('url'): # Should this be optional? ie, # and self.kwargs.get('download') or summat d_dict = FileAnalysis(url=self.meta.kwargs['url'], key=self.meta.kwargs.get('key'), **fil).md #I test here #d_dict = FileAnalysis(local='tmp/eics_2023_pumf_v1.sav').md if d_dict: fileout['Data Dictionary'] = d_dict fmeta.append(fileout) #----- original #outtmp = [] #for li in fmeta: # outtmp.append(' \\n'.join(f'{k}: {v}' for k, v in li.items())) #return '\\n\\n'.join(outtmp) #------- outtmp = [] for li in fmeta: o2 = [] for k, v in li.items(): if k == 'Data Dictionary': o2.append(f'### {k} for {li[\"File\"]} \\n{v}') else: o2.append(f'{k}: {v}') outtmp.append(' \\n'.join(o2)) outtmp = '\\n\\n'.join(outtmp) return outtmp @property def readme_md(self)->str: ''' Generate a Markdown text string (ie, the entire README) for entire an entire StudyMetadata object. ''' metatmp = self.meta.copy() neworder = self.reorder_fields(metatmp) addme = self.concatenator(metatmp) metatmp.update(addme) out = {_:None for _ in neworder} # A new dictionary with the correct order for k, v in metatmp.items(): out[k]=v #Now remove keys that should be gone for rem in self.concat: out = {k:v for k,v in out.items() if not (k.startswith(rem) and len(k) > len(rem))} fout = {self.rename_field(k): self.__fix_relation_type(self.__html_to_md(v)) for k, v in out.items()} #cludgy geometry hack is best hack if self.bbox(): fout.update(self.bbox()) delme = [_ for _ in fout if _.endswith('tude')] for _ in delme: del fout[_] outstr = '\\n\\n'.join(f'{self.make_md_heads(k)}{v}' for k, v in fout.items()) outstr += '\\n\\n## File information\\n\\n' outstr += self.file_metadata_md return outstr def bbox(self)->dict: ''' Produce sane bounding boxes from Dataverse metadata. Note that older versions of Dataverse used North and South *longitude*. Outputs a dict with bounding boxes contcatenated into a single line with each coordinate suffixed by its direction (eg: '42.97 E'), with coordinates separated by commas and multiple boxes separated by semi-colons. ''' #Yes, northLongitude, etc. Blame Harvard. bbox_order =['westLongitude', 'southLongitude', 'southLatitude', 'eastLongitude', 'northLongitude', 'northLatitude'] geog_me = {_: self.meta[_].split(';') for _ in bbox_order if self.meta.get(_)}# Checking for existence causes problems if not geog_me: #Sometimes there is no bounding box return {} bbox = {k: [f'{v} {k[0].capitalize()}'.strip() for v in geog_me[k]] for k in bbox_order if geog_me.get(k)} boxes = self.max_zip(*bbox.values()) boxes = [', '.join(_) for _ in boxes] boxes = [f'({_})' for _ in boxes] return {'Bounding box(es)': '; '.join(boxes)} def __fix_relation_type(self, badstr:str)->str: ''' For some reason, Dataverse puts camelCase values in the 'values' field for publication relation. This will make it more readable. Parameters ---------- badstr : str Input string; problematic values will be fixed, all others returned as-is. ''' fixthese = ['IsCitedBy', 'IsSupplementTo', 'IsSupplementedBy', 'IsReferencedBy'] for val in fixthese: badstr=badstr.replace(val, self.rename_field(val)) return badstr def reorder_fields(self, indict:dict)->list: ''' Create a list which contains a list of keys in the right (corrected) order. This ensures that concatenated fields are inserted into the right place and not at the end of the dictionary, keeping the structure of Dataverse metadata intact while concatenating values that need combining. Parameters ---------- indict : dict Metadata dictionary ''' fieldlist = list(indict) for val in self.concat: pts = [n for n, x in enumerate(fieldlist) if x.startswith(val)] if pts: ins_point = min(pts) fieldlist.insert(ins_point, val) #Geography fields are a special case yay. #westLongitude is the fist one if 'westLongitude' in fieldlist: ins_here = fieldlist.index('westLongitude') fieldlist.insert(ins_here, 'Bounding box(es)') return fieldlist def rename_field(self, instr:str)->str: ''' Split and capitalize camelCase fields as required. eg: keywordValue -> Keyword Value eg: termsOfUse -> Terms of Use Parameters ---------- instr : str Camel case tring to split into words and capitalize. ''' noncap = ['A', 'Of', 'The'] wordsp = ''.join(map(lambda x: x if x not in string.ascii_uppercase else f' {x}', list(instr))) wordsp = wordsp.split(' ') #wordsp[0] = wordsp[0].capitalize() #wordsp = ' '.join(map(lambda x: x if x not in noncap else x.lower(), wordsp)) wordsp = list(map(lambda x: x if x not in noncap else x.lower(), wordsp)) wordsp[0] = wordsp[0].capitalize() wordsp = ' '.join(wordsp) #because they can't even use camelCaseConsistently #Also pluralization of concatenated fields fixthese ={'U R L': 'URL', 'U R I': 'URI', 'I D': 'ID', 'Ds': '', 'Country':'Country(ies)', 'State':'State(s)', 'City':'City(ies)', 'Geographic Unit':'Geographic unit(s)'} for k, v in fixthese.items(): wordsp = wordsp.replace(k, v) return wordsp.strip() def concatenator(self, meta:dict)->dict: ''' Produce a concatenated dictionary with the key being just the prefix. For fields like author[whatever], etc, where there are multiple *components* of similar metadata held in completely separated fields. Parameters ---------- meta : dict Input metadata ''' #The keys are the first part of the fields that need concatenation concat = {_:[] for _ in self.concat} for k, v in meta.items(): for fk in concat: if k.startswith(fk): if v: if concat[fk]: concat[fk].append(v.split(';')) else: concat[fk] = [v.split(';')] outdict = {} for ke, va in concat.items(): if va: interim = self.max_zip(*va) interim = [' - '.join([y.strip() for y in _ if y]) for _ in interim ] #interim = '; '.join(interim) # Should it be newline? #interim = ' \\n'.join(interim) # Should it be newline? interim= '<br/>'.join(interim)# Markdownify strips internal spaces #if ke.startswith('keyw'): outdict[ke] = interim return outdict def max_zip(self, *args): ''' Like built-in zip, only uses the *maximum* length and appends None if not found instead of stopping at the shortest iterable. Parameters ---------- *args : iterable Any iterable ''' length = max(map(len, args)) outlist=[] for n in range(length): vals = [] for arg in args: try: vals.append(arg[n]) except IndexError: vals.append(None) outlist.append(vals) return outlist def write_pdf(self, dest:str)->None: ''' Make the PDF of a README and save it to a file. Parameters ---------- dest : str Destination of file, optionally including path. eg: /Users/foo/study/README.pdf or ~/tmp/README_I_AM_METADATA.pdf ''' dest = pathlib.Path(dest).expanduser().absolute() output = markdown_pdf.MarkdownPdf(toc_level=1) content = markdown_pdf.Section(self.readme_md, toc=False) output.add_section(content) output.save(dest) def write_md(self, dest:str)->None: ''' Write Markdown text of the complete documentation to a file. Parameters ---------- dest : str Destination of file, optionally including path. eg: /Users/foo/study/README.md or ~/tmp/README_I_AM_METADATA.md ''' dest = pathlib.Path(dest).expanduser().absolute() with open(file=dest, mode='w', encoding='utf=8') as f: f.write(self.readme_md) file_metadata_md property \u00b6 Produce pretty markdown for file metadata. Outputs markdown text string. readme_md property \u00b6 Generate a Markdown text string (ie, the entire README) for entire an entire StudyMetadata object. __fix_relation_type(badstr) \u00b6 For some reason, Dataverse puts camelCase values in the \u2018values\u2019 field for publication relation. This will make it more readable. Parameters: badstr ( str ) \u2013 Input string; problematic values will be fixed, all others returned as-is. Source code in src/dataverse_utils/collections.py def __fix_relation_type(self, badstr:str)->str: ''' For some reason, Dataverse puts camelCase values in the 'values' field for publication relation. This will make it more readable. Parameters ---------- badstr : str Input string; problematic values will be fixed, all others returned as-is. ''' fixthese = ['IsCitedBy', 'IsSupplementTo', 'IsSupplementedBy', 'IsReferencedBy'] for val in fixthese: badstr=badstr.replace(val, self.rename_field(val)) return badstr __html_to_md(inval) \u00b6 Convert any HTML to markdown, or as much as possible. Parameters: inval ( str ) \u2013 HTML string to convert Source code in src/dataverse_utils/collections.py def __html_to_md(self, inval:str)->str: ''' Convert any HTML to markdown, or as much as possible. Parameters ---------- inval : str HTML string to convert ''' if isinstance(inval, str): #markdownify kwargs are here: #https://github.com/matthewwithanm/python-markdownify return markdownify.markdownify(inval) return str(inval) __init__(study_metadata_obj, **kwargs) \u00b6 Send in StudyMetadata dict to create a nicely formatted README document Parameters: study_metadata_obj ( StudyMetadata ) \u2013 A study metadata object **kwargs ( dict , default: {} ) \u2013 Keyword arguments url ( str ) \u2013 The base URL for a Dataverse instance pid ( Union [ str , int ] ) \u2013 The persistent identifier of a file or a file id key ( str ) \u2013 A valid API key for performing operations on Dataverse studies local ( str ) \u2013 Path to the top level directory which holds study files. If present, the Readme creator will try to create extended data from local files instead of downloading. Notes Either local must be supplied, or url , pid and key must supplied Source code in src/dataverse_utils/collections.py def __init__(self, study_metadata_obj: StudyMetadata, **kwargs): ''' Send in StudyMetadata dict to create a nicely formatted README document Parameters ---------- study_metadata_obj : StudyMetadata A study metadata object **kwargs : dict Keyword arguments Other parameters ---------------- url : str The base URL for a Dataverse instance pid : typing.Union[str, int] The persistent identifier of a file or a file id key : str A valid API key for performing operations on Dataverse studies local : str Path to the top level directory which holds study files. If present, the Readme creator will try to create extended data from local files instead of downloading. Notes ----- Either `local` must be supplied, or `url`, `pid` and `key` must supplied ''' self.meta = study_metadata_obj self.kwargs = kwargs warnings.filterwarnings('ignore', category=bs4.MarkupResemblesLocatorWarning) #These values are the first part of the keys that need #concatenation to make them more legible. self.concat = ['author', 'datasetContact','otherId', 'keyword', 'topic', 'publication', 'producer', 'production', 'distributor', 'series', 'software', 'dsDescription', 'grant', 'contributor'] bbox() \u00b6 Produce sane bounding boxes from Dataverse metadata. Note that older versions of Dataverse used North and South longitude . Outputs a dict with bounding boxes contcatenated into a single line with each coordinate suffixed by its direction (eg: \u201842.97 E\u2019), with coordinates separated by commas and multiple boxes separated by semi-colons. Source code in src/dataverse_utils/collections.py def bbox(self)->dict: ''' Produce sane bounding boxes from Dataverse metadata. Note that older versions of Dataverse used North and South *longitude*. Outputs a dict with bounding boxes contcatenated into a single line with each coordinate suffixed by its direction (eg: '42.97 E'), with coordinates separated by commas and multiple boxes separated by semi-colons. ''' #Yes, northLongitude, etc. Blame Harvard. bbox_order =['westLongitude', 'southLongitude', 'southLatitude', 'eastLongitude', 'northLongitude', 'northLatitude'] geog_me = {_: self.meta[_].split(';') for _ in bbox_order if self.meta.get(_)}# Checking for existence causes problems if not geog_me: #Sometimes there is no bounding box return {} bbox = {k: [f'{v} {k[0].capitalize()}'.strip() for v in geog_me[k]] for k in bbox_order if geog_me.get(k)} boxes = self.max_zip(*bbox.values()) boxes = [', '.join(_) for _ in boxes] boxes = [f'({_})' for _ in boxes] return {'Bounding box(es)': '; '.join(boxes)} concatenator(meta) \u00b6 Produce a concatenated dictionary with the key being just the prefix. For fields like author[whatever], etc, where there are multiple components of similar metadata held in completely separated fields. Parameters: meta ( dict ) \u2013 Input metadata Source code in src/dataverse_utils/collections.py def concatenator(self, meta:dict)->dict: ''' Produce a concatenated dictionary with the key being just the prefix. For fields like author[whatever], etc, where there are multiple *components* of similar metadata held in completely separated fields. Parameters ---------- meta : dict Input metadata ''' #The keys are the first part of the fields that need concatenation concat = {_:[] for _ in self.concat} for k, v in meta.items(): for fk in concat: if k.startswith(fk): if v: if concat[fk]: concat[fk].append(v.split(';')) else: concat[fk] = [v.split(';')] outdict = {} for ke, va in concat.items(): if va: interim = self.max_zip(*va) interim = [' - '.join([y.strip() for y in _ if y]) for _ in interim ] #interim = '; '.join(interim) # Should it be newline? #interim = ' \\n'.join(interim) # Should it be newline? interim= '<br/>'.join(interim)# Markdownify strips internal spaces #if ke.startswith('keyw'): outdict[ke] = interim return outdict make_md_heads(inkey) \u00b6 Make markdown H2 headings for selected sections, currently title, description, licence and terms of use. Parameters: inkey ( str ) \u2013 Section heading Source code in src/dataverse_utils/collections.py def make_md_heads(self, inkey:str)->str: ''' Make markdown H2 headings for selected sections, currently title, description, licence and terms of use. Parameters ---------- inkey : str Section heading ''' section_heads = {'Title':'## ', 'Description':'**Description**\\n\\n', 'Licence': '### Licence\\n\\n', 'Terms of Use': '### Terms of Use\\n\\n'} if inkey in section_heads: return section_heads[inkey] multi = [self.rename_field(_) for _ in self.concat] if inkey in multi: if inkey not in ['Series', 'Software', 'Production']: return f'{inkey}(s): \\n' return f'{inkey}: \\n' return f'{inkey}: ' max_zip(*args) \u00b6 Like built-in zip, only uses the maximum length and appends None if not found instead of stopping at the shortest iterable. Parameters: *args ( iterable , default: () ) \u2013 Any iterable Source code in src/dataverse_utils/collections.py def max_zip(self, *args): ''' Like built-in zip, only uses the *maximum* length and appends None if not found instead of stopping at the shortest iterable. Parameters ---------- *args : iterable Any iterable ''' length = max(map(len, args)) outlist=[] for n in range(length): vals = [] for arg in args: try: vals.append(arg[n]) except IndexError: vals.append(None) outlist.append(vals) return outlist rename_field(instr) \u00b6 Split and capitalize camelCase fields as required. eg: keywordValue -> Keyword Value eg: termsOfUse -> Terms of Use Parameters: instr ( str ) \u2013 Camel case tring to split into words and capitalize. Source code in src/dataverse_utils/collections.py def rename_field(self, instr:str)->str: ''' Split and capitalize camelCase fields as required. eg: keywordValue -> Keyword Value eg: termsOfUse -> Terms of Use Parameters ---------- instr : str Camel case tring to split into words and capitalize. ''' noncap = ['A', 'Of', 'The'] wordsp = ''.join(map(lambda x: x if x not in string.ascii_uppercase else f' {x}', list(instr))) wordsp = wordsp.split(' ') #wordsp[0] = wordsp[0].capitalize() #wordsp = ' '.join(map(lambda x: x if x not in noncap else x.lower(), wordsp)) wordsp = list(map(lambda x: x if x not in noncap else x.lower(), wordsp)) wordsp[0] = wordsp[0].capitalize() wordsp = ' '.join(wordsp) #because they can't even use camelCaseConsistently #Also pluralization of concatenated fields fixthese ={'U R L': 'URL', 'U R I': 'URI', 'I D': 'ID', 'Ds': '', 'Country':'Country(ies)', 'State':'State(s)', 'City':'City(ies)', 'Geographic Unit':'Geographic unit(s)'} for k, v in fixthese.items(): wordsp = wordsp.replace(k, v) return wordsp.strip() reorder_fields(indict) \u00b6 Create a list which contains a list of keys in the right (corrected) order. This ensures that concatenated fields are inserted into the right place and not at the end of the dictionary, keeping the structure of Dataverse metadata intact while concatenating values that need combining. Parameters: indict ( dict ) \u2013 Metadata dictionary Source code in src/dataverse_utils/collections.py def reorder_fields(self, indict:dict)->list: ''' Create a list which contains a list of keys in the right (corrected) order. This ensures that concatenated fields are inserted into the right place and not at the end of the dictionary, keeping the structure of Dataverse metadata intact while concatenating values that need combining. Parameters ---------- indict : dict Metadata dictionary ''' fieldlist = list(indict) for val in self.concat: pts = [n for n, x in enumerate(fieldlist) if x.startswith(val)] if pts: ins_point = min(pts) fieldlist.insert(ins_point, val) #Geography fields are a special case yay. #westLongitude is the fist one if 'westLongitude' in fieldlist: ins_here = fieldlist.index('westLongitude') fieldlist.insert(ins_here, 'Bounding box(es)') return fieldlist write_md(dest) \u00b6 Write Markdown text of the complete documentation to a file. Parameters: dest ( str ) \u2013 Destination of file, optionally including path. eg: /Users/foo/study/README.md or ~/tmp/README_I_AM_METADATA.md Source code in src/dataverse_utils/collections.py def write_md(self, dest:str)->None: ''' Write Markdown text of the complete documentation to a file. Parameters ---------- dest : str Destination of file, optionally including path. eg: /Users/foo/study/README.md or ~/tmp/README_I_AM_METADATA.md ''' dest = pathlib.Path(dest).expanduser().absolute() with open(file=dest, mode='w', encoding='utf=8') as f: f.write(self.readme_md) write_pdf(dest) \u00b6 Make the PDF of a README and save it to a file. Parameters: dest ( str ) \u2013 Destination of file, optionally including path. eg: /Users/foo/study/README.pdf or ~/tmp/README_I_AM_METADATA.pdf Source code in src/dataverse_utils/collections.py def write_pdf(self, dest:str)->None: ''' Make the PDF of a README and save it to a file. Parameters ---------- dest : str Destination of file, optionally including path. eg: /Users/foo/study/README.pdf or ~/tmp/README_I_AM_METADATA.pdf ''' dest = pathlib.Path(dest).expanduser().absolute() output = markdown_pdf.MarkdownPdf(toc_level=1) content = markdown_pdf.Section(self.readme_md, toc=False) output.add_section(content) output.save(dest) StudyMetadata \u00b6 Bases: dict The metadata container for a single study. Source code in src/dataverse_utils/collections.py class StudyMetadata(dict): ''' The metadata container for a single study. ''' def __init__(self, **kwargs): ''' Intializize a StudyMetadata object. Parameters ---------- **kwargs: dict At least some of the following Other parameters ---------------- study_meta : dict, optional The dataverse study metadata JSON url : str, optional Base URL to dataverse instance pid : str, optional Persistent ID of a study key : str Dataverse instance API key (needed for unpublished studies) Notes ----- Either `study_meta` is required OR `pid` and `url`. `key` _may_ be required if either a draft study is being accessed or the Dataverse installation requires API keys for all requests. ''' self.kwargs = kwargs self.study_meta = kwargs.get('study_meta') self.url = kwargs.get('url') self.pid = kwargs.get('pid') self.headers = UAHEADER.copy() if not (('study_meta' in kwargs) or ('url' in kwargs and 'pid' in kwargs)): raise TypeError('At least one of a URL/pid combo (url, pid) (and possibly key) or ' 'study metadata json (study_meta) is required.') if not self.study_meta: self.study_meta = self.__obtain_metadata() try: self.extract_metadata() except KeyError as e: raise MetadataError(f'Unable to parse study metadata. Do you need an API key?\\n' f'{e} key not found.\\n' f'Offending JSON: {self.study_meta}') from e self.__files = None def __obtain_metadata(self): ''' Obtain study metadata as required. ''' if self.kwargs.get('key'): self.headers.update({'X-Dataverse-key':self.kwargs['key']}) params = {'persistentId': self.pid} self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=RETRY)) self.url = self.url.strip('/') if not self.url.startswith('https://'): self.url = f'https://{self.url}' data = self.session.get(f'{self.url}/api/datasets/:persistentId', headers=self.headers, params=params) return data.json() def __has_metadata(self)->bool: ''' Returns a boolean to ensure if there *is* study metadata. Deacessioned items are notable for their lack of any indication that they are deacessioned. However, they lack the \"latestVersion\" key, which serves as a proxy. Ideally. ''' #try: # t = self.study_meta['data'] # del t #OMG This is so dumb #except KeyError as e: # raise e if not self.study_meta.get('data'): raise KeyError('data') testfields = ['id', 'identifier', 'authority', 'latestVersion'] if all(self.study_meta['data'].get(_) for _ in testfields): return True return False def extract_metadata(self): ''' Convenience function for parsing the study metadata of the latest version. Results are written to self, accessible as a dictionary. ''' if not self.__has_metadata(): return for v in self.study_meta['data']['latestVersion']['metadataBlocks'].values(): for field in v['fields']: self.extract_field_metadata(field) self.__extract_licence_info() self.__version() #['data']['latestVersion']['versionNumber'] #['data']['latestVersion']['versionMinorNumber'] def extract_field_metadata(self, field): ''' Extract the metadata from a single field and make it into a human-readable dict. Output updates self. ''' #pylint: disable=too-many-branches, too-many-nested-blocks #typeClass: compound = dict, primitive = string #multiple: false= one thing, true=list # so typeClass:compound AND multiple:true = a list of dicts. # also, typeClass can be \"controlledVocabulary\" because reasons. #is this crap recursive or is one level enough? #[[x['typeName'], x['typeClass'], x['multiple']] for x in citation['fields']] # {('primitive', False), ('compound', True), ('compound', False), # ('primitive', True), ('controlledVocabulary', True)} if not field['multiple']: if field['typeClass']=='primitive': self.update({field['typeName']: field['value']}) if field['typeClass'] == 'compound': for v2 in field['value']: self.extract_field_metadata(field['value'][v2]) if field['multiple']: if field['typeClass'] == 'compound': #produce a list of similar values concatenated for v3 in field['value']: interim = {} for insane_dict in field['value']: for v3 in insane_dict.values(): if interim.get(v3['typeName']): interim.update({v3['typeName']: interim[v3['typeName']]+ [v3['value']]}) else: #sometimes value is None because reasons. interim[v3['typeName']] = [v3.get('value', [] )] LOGGER.debug(interim) for k9, v9 in interim.items(): self.update({k9: '; '.join(v9)}) if field['typeClass'] == 'primitive': self.update({field['typeName'] : '; '.join(field['value'])}) if field['typeClass'] == 'controlledVocabulary': if isinstance(field['value'], list): self.update({field['typeName'] : '; '.join(field['value'])}) else: self.update({field['typeName'] : field['value']}) # And that should cover every option! @property def files(self)->list: ''' Return a list of of dicts with file metadata. ''' if not self.__files: self.__extract_files() return self.__files def __extract_files(self): ''' Extract file level metadata, and write to self.__files. ''' #Note: ALL other dict values for this object are single values, #but files would (usually) be an arbitrary number of files. #That bothers me on an intellectual level. Therefore, it will be attribute. #Iterate over StudyMetadata.files if you want to know the contents if not self.__files: outie = [] for v in self.study_meta['data']['latestVersion']['files']: innie = {} fpath = v.get('directoryLabel', '').strip('/') innie['filename'] = v['dataFile'].get('originalFileName', v['dataFile']['filename']) #innie['full_path'] = '/'.join([fpath, innie['filename']]) #In case it's pathless, drop any leading slash, because #'' is not the same as None, and None can't be joined. innie['filename'] = '/'.join([fpath, innie['filename']]).strip('/') innie['file_label'] = v.get('label') innie['description'] = v.get('description') innie['filesize_bytes'] = v['dataFile'].get('originalFileSize', v['dataFile']['filesize']) innie['chk_type'] = v['dataFile']['checksum']['type'] innie['chk_digest'] =v['dataFile']['checksum']['value'] innie['id'] = v['dataFile']['id'] innie['pid'] = v['dataFile'].get('persistentId') innie['has_tab_file'] = v['dataFile'].get('tabularData', False) innie['study_pid'] = (f\"{self.study_meta['data']['protocol']}:\" f\"{self.study_meta['data']['authority']}/\" f\"{self.study_meta['data']['identifier']}\") innie['tags'] = ', '.join(v.get('categories', [])) if not innie['tags']: del innie['tags']#tagless #innie['path'] = v.get('directoryLabel', '') outie.append(innie) self.__files = outie def __extract_licence_info(self): ''' Extract all the licence information fields and add them to self['licence'] *if present*. ''' lic_fields = ('termsOfUse', 'confidentialityDeclaration', 'specialPermissions', 'restrictions', 'citationRequirements', 'depositorRequirements', 'conditions', 'disclaimer', 'dataAccessPlace', 'originalArchive', 'availabilityStatus', 'contactForAccess', 'sizeOfCollection', 'studyCompletion', 'fileAccessRequest') for field in self.study_meta['data']['latestVersion']: if field in lic_fields: self[field] = self.study_meta['data']['latestVersion'][field] common_lic = self.study_meta['data']['latestVersion'].get('license') if isinstance(common_lic, str) and common_lic != 'NONE': self['licence'] = common_lic elif isinstance(common_lic, dict): self['licence'] = self.study_meta['data']['latestVersion']['license'].get('name') link = self.study_meta['data']['latestVersion']['license'].get('uri') if link: self['licenceLink'] = link def __version(self): ''' Obtain the current version and add it to self['studyVersion']. ''' if self.study_meta['data']['latestVersion']['versionState'] == 'RELEASED': self['studyVersion'] = (f\"{self.study_meta['data']['latestVersion']['versionNumber']}.\" f\"{self.study_meta['data']['latestVersion']['versionMinorNumber']}\") return self['studyVersion'] = self.study_meta['data']['latestVersion']['versionState'] return files property \u00b6 Return a list of of dicts with file metadata. __extract_files() \u00b6 Extract file level metadata, and write to self.__files. Source code in src/dataverse_utils/collections.py def __extract_files(self): ''' Extract file level metadata, and write to self.__files. ''' #Note: ALL other dict values for this object are single values, #but files would (usually) be an arbitrary number of files. #That bothers me on an intellectual level. Therefore, it will be attribute. #Iterate over StudyMetadata.files if you want to know the contents if not self.__files: outie = [] for v in self.study_meta['data']['latestVersion']['files']: innie = {} fpath = v.get('directoryLabel', '').strip('/') innie['filename'] = v['dataFile'].get('originalFileName', v['dataFile']['filename']) #innie['full_path'] = '/'.join([fpath, innie['filename']]) #In case it's pathless, drop any leading slash, because #'' is not the same as None, and None can't be joined. innie['filename'] = '/'.join([fpath, innie['filename']]).strip('/') innie['file_label'] = v.get('label') innie['description'] = v.get('description') innie['filesize_bytes'] = v['dataFile'].get('originalFileSize', v['dataFile']['filesize']) innie['chk_type'] = v['dataFile']['checksum']['type'] innie['chk_digest'] =v['dataFile']['checksum']['value'] innie['id'] = v['dataFile']['id'] innie['pid'] = v['dataFile'].get('persistentId') innie['has_tab_file'] = v['dataFile'].get('tabularData', False) innie['study_pid'] = (f\"{self.study_meta['data']['protocol']}:\" f\"{self.study_meta['data']['authority']}/\" f\"{self.study_meta['data']['identifier']}\") innie['tags'] = ', '.join(v.get('categories', [])) if not innie['tags']: del innie['tags']#tagless #innie['path'] = v.get('directoryLabel', '') outie.append(innie) self.__files = outie __extract_licence_info() \u00b6 Extract all the licence information fields and add them to self[\u2018licence\u2019] if present . Source code in src/dataverse_utils/collections.py def __extract_licence_info(self): ''' Extract all the licence information fields and add them to self['licence'] *if present*. ''' lic_fields = ('termsOfUse', 'confidentialityDeclaration', 'specialPermissions', 'restrictions', 'citationRequirements', 'depositorRequirements', 'conditions', 'disclaimer', 'dataAccessPlace', 'originalArchive', 'availabilityStatus', 'contactForAccess', 'sizeOfCollection', 'studyCompletion', 'fileAccessRequest') for field in self.study_meta['data']['latestVersion']: if field in lic_fields: self[field] = self.study_meta['data']['latestVersion'][field] common_lic = self.study_meta['data']['latestVersion'].get('license') if isinstance(common_lic, str) and common_lic != 'NONE': self['licence'] = common_lic elif isinstance(common_lic, dict): self['licence'] = self.study_meta['data']['latestVersion']['license'].get('name') link = self.study_meta['data']['latestVersion']['license'].get('uri') if link: self['licenceLink'] = link __has_metadata() \u00b6 Returns a boolean to ensure if there is study metadata. Deacessioned items are notable for their lack of any indication that they are deacessioned. However, they lack the \u201clatestVersion\u201d key, which serves as a proxy. Ideally. Source code in src/dataverse_utils/collections.py def __has_metadata(self)->bool: ''' Returns a boolean to ensure if there *is* study metadata. Deacessioned items are notable for their lack of any indication that they are deacessioned. However, they lack the \"latestVersion\" key, which serves as a proxy. Ideally. ''' #try: # t = self.study_meta['data'] # del t #OMG This is so dumb #except KeyError as e: # raise e if not self.study_meta.get('data'): raise KeyError('data') testfields = ['id', 'identifier', 'authority', 'latestVersion'] if all(self.study_meta['data'].get(_) for _ in testfields): return True return False __init__(**kwargs) \u00b6 Intializize a StudyMetadata object. Parameters: **kwargs \u2013 At least some of the following study_meta ( dict ) \u2013 The dataverse study metadata JSON url ( str ) \u2013 Base URL to dataverse instance pid ( str ) \u2013 Persistent ID of a study key ( str ) \u2013 Dataverse instance API key (needed for unpublished studies) Notes Either study_meta is required OR pid and url . key may be required if either a draft study is being accessed or the Dataverse installation requires API keys for all requests. Source code in src/dataverse_utils/collections.py def __init__(self, **kwargs): ''' Intializize a StudyMetadata object. Parameters ---------- **kwargs: dict At least some of the following Other parameters ---------------- study_meta : dict, optional The dataverse study metadata JSON url : str, optional Base URL to dataverse instance pid : str, optional Persistent ID of a study key : str Dataverse instance API key (needed for unpublished studies) Notes ----- Either `study_meta` is required OR `pid` and `url`. `key` _may_ be required if either a draft study is being accessed or the Dataverse installation requires API keys for all requests. ''' self.kwargs = kwargs self.study_meta = kwargs.get('study_meta') self.url = kwargs.get('url') self.pid = kwargs.get('pid') self.headers = UAHEADER.copy() if not (('study_meta' in kwargs) or ('url' in kwargs and 'pid' in kwargs)): raise TypeError('At least one of a URL/pid combo (url, pid) (and possibly key) or ' 'study metadata json (study_meta) is required.') if not self.study_meta: self.study_meta = self.__obtain_metadata() try: self.extract_metadata() except KeyError as e: raise MetadataError(f'Unable to parse study metadata. Do you need an API key?\\n' f'{e} key not found.\\n' f'Offending JSON: {self.study_meta}') from e self.__files = None __obtain_metadata() \u00b6 Obtain study metadata as required. Source code in src/dataverse_utils/collections.py def __obtain_metadata(self): ''' Obtain study metadata as required. ''' if self.kwargs.get('key'): self.headers.update({'X-Dataverse-key':self.kwargs['key']}) params = {'persistentId': self.pid} self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=RETRY)) self.url = self.url.strip('/') if not self.url.startswith('https://'): self.url = f'https://{self.url}' data = self.session.get(f'{self.url}/api/datasets/:persistentId', headers=self.headers, params=params) return data.json() __version() \u00b6 Obtain the current version and add it to self[\u2018studyVersion\u2019]. Source code in src/dataverse_utils/collections.py def __version(self): ''' Obtain the current version and add it to self['studyVersion']. ''' if self.study_meta['data']['latestVersion']['versionState'] == 'RELEASED': self['studyVersion'] = (f\"{self.study_meta['data']['latestVersion']['versionNumber']}.\" f\"{self.study_meta['data']['latestVersion']['versionMinorNumber']}\") return self['studyVersion'] = self.study_meta['data']['latestVersion']['versionState'] return extract_field_metadata(field) \u00b6 Extract the metadata from a single field and make it into a human-readable dict. Output updates self. Source code in src/dataverse_utils/collections.py def extract_field_metadata(self, field): ''' Extract the metadata from a single field and make it into a human-readable dict. Output updates self. ''' #pylint: disable=too-many-branches, too-many-nested-blocks #typeClass: compound = dict, primitive = string #multiple: false= one thing, true=list # so typeClass:compound AND multiple:true = a list of dicts. # also, typeClass can be \"controlledVocabulary\" because reasons. #is this crap recursive or is one level enough? #[[x['typeName'], x['typeClass'], x['multiple']] for x in citation['fields']] # {('primitive', False), ('compound', True), ('compound', False), # ('primitive', True), ('controlledVocabulary', True)} if not field['multiple']: if field['typeClass']=='primitive': self.update({field['typeName']: field['value']}) if field['typeClass'] == 'compound': for v2 in field['value']: self.extract_field_metadata(field['value'][v2]) if field['multiple']: if field['typeClass'] == 'compound': #produce a list of similar values concatenated for v3 in field['value']: interim = {} for insane_dict in field['value']: for v3 in insane_dict.values(): if interim.get(v3['typeName']): interim.update({v3['typeName']: interim[v3['typeName']]+ [v3['value']]}) else: #sometimes value is None because reasons. interim[v3['typeName']] = [v3.get('value', [] )] LOGGER.debug(interim) for k9, v9 in interim.items(): self.update({k9: '; '.join(v9)}) if field['typeClass'] == 'primitive': self.update({field['typeName'] : '; '.join(field['value'])}) if field['typeClass'] == 'controlledVocabulary': if isinstance(field['value'], list): self.update({field['typeName'] : '; '.join(field['value'])}) else: self.update({field['typeName'] : field['value']}) extract_metadata() \u00b6 Convenience function for parsing the study metadata of the latest version. Results are written to self, accessible as a dictionary. Source code in src/dataverse_utils/collections.py def extract_metadata(self): ''' Convenience function for parsing the study metadata of the latest version. Results are written to self, accessible as a dictionary. ''' if not self.__has_metadata(): return for v in self.study_meta['data']['latestVersion']['metadataBlocks'].values(): for field in v['fields']: self.extract_field_metadata(field) self.__extract_licence_info() self.__version()","title":"API reference"},{"location":"api_ref/#api-reference","text":"","title":"API Reference"},{"location":"api_ref/#dataverse_utils","text":"Generalized dataverse utilities. Note that import dataverse_utils is the equivalent of import dataverse_utils.dataverse_utils","title":"dataverse_utils"},{"location":"api_ref/#dataverse_utils.DvGeneralUploadError","text":"Bases: Exception Raised on non-200 URL response Source code in src/dataverse_utils/dataverse_utils.py class DvGeneralUploadError(Exception): ''' Raised on non-200 URL response '''","title":"DvGeneralUploadError"},{"location":"api_ref/#dataverse_utils.Md5Error","text":"Bases: Exception Raised on md5 mismatch Source code in src/dataverse_utils/dataverse_utils.py class Md5Error(Exception): ''' Raised on md5 mismatch '''","title":"Md5Error"},{"location":"api_ref/#dataverse_utils.check_lock","text":"Checks study lock status; returns True if locked. Parameters: dv_url ( str ) \u2013 URL of Dataverse installation study \u2013 Persistent ID of study apikey ( str ) \u2013 API key for user Source code in src/dataverse_utils/dataverse_utils.py def check_lock(dv_url, study, apikey) -> bool: ''' Checks study lock status; returns True if locked. Parameters ---------- dv_url : str URL of Dataverse installation study: str Persistent ID of study apikey : str API key for user ''' dv_url, headers, params = _make_info(dv_url, study, apikey) lock_status = requests.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() data = lock_status.json().get('data') if data: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) return True return False","title":"check_lock"},{"location":"api_ref/#dataverse_utils.dump_tsv","text":"Dumps output of make_tsv manifest to a file. Parameters: start_dir ( str ) \u2013 Path to start directory in_list ( list , default: None ) \u2013 List of files for which to create manifest entries. Will default to recursive directory crawl **kwargs ( dict , default: {} ) \u2013 Other parameters def_tag ( str ) \u2013 Default Dataverse tag (eg, Data, Documentation, etc). Separate tags with an easily splitable character: eg. (\u2018Data, 2016\u2019) inc_header ( bool ) \u2013 Include header for tsv. quotype ( int ) \u2013 integer value or csv quote type. Acceptable values: * csv.QUOTE_MINIMAL / 0 * csv.QUOTE_ALL / 1 * csv.QUOTE_NONNUMERIC / 2 * csv.QUOTE_NONE / 3 Source code in src/dataverse_utils/dataverse_utils.py def dump_tsv(start_dir, filename, in_list=None, **kwargs): ''' Dumps output of make_tsv manifest to a file. Parameters ---------- start_dir : str Path to start directory in_list : list List of files for which to create manifest entries. Will default to recursive directory crawl **kwargs : dict Other parameters Other parameters ---------------- def_tag : str, optional, default='Data' Default Dataverse tag (eg, Data, Documentation, etc). Separate tags with an easily splitable character: eg. ('Data, 2016') inc_header : bool, optional, default=True Include header for tsv. quotype : int, optional, default=csv.QUOTE_MINIMAL integer value or csv quote type. Acceptable values: * csv.QUOTE_MINIMAL / 0 * csv.QUOTE_ALL / 1 * csv.QUOTE_NONNUMERIC / 2 * csv.QUOTE_NONE / 3 ''' def_tag= kwargs.get('def_tag', 'Data') inc_header =kwargs.get('inc_header', True) mime = kwargs.get('mime', False) path = kwargs.get('path', False) quotype = kwargs.get('quotype', csv.QUOTE_MINIMAL) dumper = make_tsv(start_dir, in_list, def_tag, inc_header, mime, quotype, path=path) with open(filename, 'w', newline='', encoding='utf-8') as tsvfile: tsvfile.write(dumper)","title":"dump_tsv"},{"location":"api_ref/#dataverse_utils.file_path","text":"Create relative file path from full path string Parameters: fpath ( str ) \u2013 File location (ie, complete path) trunc ( str , default: '' ) \u2013 Leftmost portion of path to remove Notes >>> file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp/') 'Data/2011' >>> file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp') 'Data/2011' Source code in src/dataverse_utils/dataverse_utils.py def file_path(fpath, trunc='') -> str: ''' Create relative file path from full path string Parameters ---------- fpath : str File location (ie, complete path) trunc : str Leftmost portion of path to remove Notes ----- ``` >>> file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp/') 'Data/2011' >>> file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp') 'Data/2011' ``` ''' if trunc and not trunc.endswith(os.sep): trunc += os.sep path = os.path.dirname(fpath) try: if fpath.find(trunc) == -1: dirlabel = os.path.relpath(os.path.split(path)[0]) dirlabel = os.path.relpath(path[path.find(trunc)+len(trunc):]) if dirlabel == '.': dirlabel = '' return dirlabel except ValueError: return ''","title":"file_path"},{"location":"api_ref/#dataverse_utils.force_notab_unlock","text":"Forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Returns 0 if unlocked, file id if locked (and then unlocked). Parameters: study ( str ) \u2013 Persistent indentifer of study dv_url ( str ) \u2013 URL to base Dataverse installation fid ( str ) \u2013 File ID for file object apikey ( str ) \u2013 API key for user try_uningest ( bool , default: True ) \u2013 Try to uningest the file that was locked. Default: True Source code in src/dataverse_utils/dataverse_utils.py def force_notab_unlock(study, dv_url, fid, apikey, try_uningest=True) -> int: ''' Forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Returns 0 if unlocked, file id if locked (and then unlocked). Parameters ---------- study : str Persistent indentifer of study dv_url : str URL to base Dataverse installation fid : str File ID for file object apikey : str API key for user try_uningest : bool Try to uningest the file that was locked. Default: True ''' dv_url, headers, params = _make_info(dv_url, study, apikey) force_unlock = requests.delete(f'{dv_url}/api/datasets/:persistentId/locks', params=params, headers=headers, timeout=300) LOGGER.warning('Lock removed for %s', study) LOGGER.warning('Lock status:\\n %s', force_unlock.json()) if try_uningest: uningest_file(dv_url, fid, apikey, study) return int(fid) return 0","title":"force_notab_unlock"},{"location":"api_ref/#dataverse_utils.make_tsv","text":"Recurses the tree for files and produces tsv output with with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. The \u2018description\u2019 is the filename without an extension. Returns tsv as string. Parameters: start_dir ( str ) \u2013 Path to start directory in_list ( list , default: None ) \u2013 Input file list. Defaults to recursive walk of current directory. def_tag ( str , default: 'Data' ) \u2013 Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with a comma: eg. (\u2018Data, 2016\u2019) inc_header ( bool , default: True ) \u2013 Include header row mime ( bool , default: False ) \u2013 Include automatically determined mimetype quotype \u2013 integer value or csv quote type. Default = csv.QUOTE_MINIMAL Acceptable values: csv.QUOTE_MINIMAL / 0 csv.QUOTE_ALL / 1 csv.QUOTE_NONNUMERIC / 2 csv.QUOTE_NONE / 3 **kwargs ( dict , default: {} ) \u2013 Other parameters path ( bool ) \u2013 If true include a \u2018path\u2019 field so that you can type in a custom path instead of actually structuring your data Source code in src/dataverse_utils/dataverse_utils.py def make_tsv(start_dir, in_list=None, def_tag='Data', inc_header=True, mime=False, quotype=csv.QUOTE_MINIMAL, **kwargs) -> str: # pylint: disable=too-many-positional-arguments # pylint: disable=too-many-arguments ''' Recurses the tree for files and produces tsv output with with headers 'file', 'description', 'tags'. The 'description' is the filename without an extension. Returns tsv as string. Parameters ---------- start_dir : str Path to start directory in_list : list Input file list. Defaults to recursive walk of current directory. def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with a comma: eg. ('Data, 2016') inc_header : bool Include header row mime : bool Include automatically determined mimetype quotype: int integer value or csv quote type. Default = csv.QUOTE_MINIMAL Acceptable values: csv.QUOTE_MINIMAL / 0 csv.QUOTE_ALL / 1 csv.QUOTE_NONNUMERIC / 2 csv.QUOTE_NONE / 3 **kwargs : dict Other parameters Other parameters ---------------- path : bool If true include a 'path' field so that you can type in a custom path instead of actually structuring your data ''' if start_dir.endswith(os.sep): #start_dir += os.sep start_dir = start_dir[:-1] if not in_list: in_list = [f'{x[0]}{os.sep}{y}' for x in os.walk(start_dir) for y in x[2] if not y.startswith('.')] if isinstance(in_list, set): in_list=list(in_list) in_list.sort() def_tag = \", \".join([x.strip() for x in def_tag.split(',')]) headers = ['file', 'description', 'tags'] if mime: headers.append('mimetype') if kwargs.get('path'): headers.insert(1, 'path') outf = io.StringIO(newline='') tsv_writer = csv.DictWriter(outf, delimiter='\\t', quoting=quotype, fieldnames=headers, extrasaction='ignore') if inc_header: tsv_writer.writeheader() for row in in_list: #the columns r = {} r['file'] = row r['description'] = os.path.splitext(os.path.basename(row))[0] r['mimetype'] = mimetypes.guess_type(row)[0] r['tags'] = def_tag r['path'] = '' tsv_writer.writerow(r) outf.seek(0) outfile = outf.read() outf.close() return outfile","title":"make_tsv"},{"location":"api_ref/#dataverse_utils.restrict_file","text":"Restrict file in Dataverse study. Parameters: **kwargs ( dict , default: {} ) \u2013 pid ( str ) \u2013 file persistent ID fid ( str ) \u2013 file database ID dv ( ( str , required ) ) \u2013 url to base Dataverse installation eg: \u2018https://abacus.library.ubc.ca\u2019 apikey ( ( str , required ) ) \u2013 API key for user rest ( bool ) \u2013 On True, restrict. Default True Notes One of pid or fid is required Source code in src/dataverse_utils/dataverse_utils.py def restrict_file(**kwargs): ''' Restrict file in Dataverse study. Parameters ---------- **kwargs : dict Other parameters ---------------- pid : str, optional file persistent ID fid : str, optional file database ID dv : str, required url to base Dataverse installation eg: 'https://abacus.library.ubc.ca' apikey : str, required API key for user rest : bool On True, restrict. Default True Notes -------- One of `pid` or `fid` is **required** ''' headers = {'X-Dataverse-key': kwargs['apikey']} headers.update(dataverse_utils.UAHEADER) #Requires a true/false *string* for the API. if kwargs.get('rest', True): rest = 'true' else: rest= 'false' if kwargs.get('pid'): params={'persistentId':kwargs['pid']} rest = requests.put(f'{kwargs[\"dv\"]}/api/files/:persistentId/restrict', headers=headers, params=params, data=rest, timeout=300) elif kwargs.get('fid'): rest = requests.put(f'{kwargs[\"dv\"]}/api/files/{kwargs[\"fid\"]}/restrict', headers=headers, data=rest, timeout=300) else: LOGGER.error('No file ID/PID supplied for file restriction') raise KeyError('One of persistentId (pid) or database ID' '(fid) is required for file restriction')","title":"restrict_file"},{"location":"api_ref/#dataverse_utils.script_ver_stmt","text":"Returns a formatted version statement for any script Parameters: name ( str ) \u2013 Name of utility to join to create version statement. Normally %prog from argparse. Source code in src/dataverse_utils/__init__.py def script_ver_stmt(name:str)->str: ''' Returns a formatted version statement for any script Parameters ---------- name : str Name of utility to join to create version statement. Normally %prog from argparse. ''' key = pathlib.Path(name).stem if not SCRIPT_VERSIONS.get(key): return f'dataverse_utils: v{__version__}' return (f\"{key} v{'.'.join(map(str, SCRIPT_VERSIONS[key]))} / \" f'dataverse_utils v{__version__}')","title":"script_ver_stmt"},{"location":"api_ref/#dataverse_utils.uningest_file","text":"Tries to uningest a file that has been ingested. Requires superuser API key. Parameters: dv_url ( str ) \u2013 URL to base Dataverse installation fid ( int or str ) \u2013 File ID of file to uningest apikey ( str ) \u2013 API key for superuser study ( str , default: 'n/a' ) \u2013 Optional handle parameter for log messages Source code in src/dataverse_utils/dataverse_utils.py def uningest_file(dv_url, fid, apikey, study='n/a'): ''' Tries to uningest a file that has been ingested. Requires superuser API key. Parameters ---------- dv_url : str URL to base Dataverse installation fid : int or str File ID of file to uningest apikey : str API key for superuser study : str, optional Optional handle parameter for log messages ''' dv_url, headers, params = _make_info(dv_url, fid, apikey) fid = params['persistentId'] #TODONE: Awaiting answer from Harvard on how to remove progress bar #for uploaded tab files that squeak through. #Answer: you can't! try: uningest = requests.post(f'{dv_url}/api/files/{fid}/uningest', headers=headers, timeout=300) LOGGER.warning('Ingest halted for file %s for fileID %s', fid, study) uningest.raise_for_status() except requests.exceptions.HTTPError: LOGGER.error('Uningestion error: %s', uningest.reason) print(uningest.reason)","title":"uningest_file"},{"location":"api_ref/#dataverse_utils.upload_file","text":"Uploads file to Dataverse study and sets file metadata and tags. Parameters: fpath ( str ) \u2013 file location (ie, complete path) hdl ( str ) \u2013 Dataverse persistent ID for study (handle or DOI) **kwargs ( dict , default: {} ) \u2013 Other parameters dv ( ( str , required ) ) \u2013 URL to base Dataverse installation eg: \u2018https://abacus.library.ubc.ca\u2019 apikey ( ( str , required ) ) \u2013 API key for user descr ( str ) \u2013 File description md5 ( str ) \u2013 md5sum for file checking tags ( list ) \u2013 list of text file tags. Eg [\u2018Data\u2019, \u2018June 2020\u2019] dirlabel ( str ) \u2013 Unix style relative pathname for Dataverse file path: eg: path/to/file/ nowait ( bool ) \u2013 Force a file unlock and uningest instead of waiting for processing to finish trunc ( str ) \u2013 Leftmost portion of path to remove rest ( bool ) \u2013 Restrict file. Defaults to false unless True supplied mimetype ( str ) \u2013 Mimetype of file. Useful if using File Previewers. Mimetype for zip files (application/zip) will be ignored to circumvent Dataverse\u2019s automatic unzipping function. label ( str ) \u2013 If included in kwargs, this value will be used for the label timeout ( int ) \u2013 Timeout in seconds override ( bool ) \u2013 Ignore NOTAB (ie, NOTAB = []) Source code in src/dataverse_utils/dataverse_utils.py def upload_file(fpath, hdl, **kwargs): ''' Uploads file to Dataverse study and sets file metadata and tags. Parameters ---------- fpath : str file location (ie, complete path) hdl : str Dataverse persistent ID for study (handle or DOI) **kwargs : dict Other parameters Other parameters ---------------- dv : str, required URL to base Dataverse installation eg: 'https://abacus.library.ubc.ca' apikey : str, required API key for user descr : str, optional File description md5 : str, optional md5sum for file checking tags : list, optional list of text file tags. Eg ['Data', 'June 2020'] dirlabel : str, optional Unix style relative pathname for Dataverse file path: eg: path/to/file/ nowait : bool, optional Force a file unlock and uningest instead of waiting for processing to finish trunc : str, optional Leftmost portion of path to remove rest : bool, optional Restrict file. Defaults to false unless True supplied mimetype : str, optional Mimetype of file. Useful if using File Previewers. Mimetype for zip files (application/zip) will be ignored to circumvent Dataverse's automatic unzipping function. label : str, optional If included in kwargs, this value will be used for the label timeout : int, optional Timeout in seconds override : bool, optional Ignore NOTAB (ie, NOTAB = []) ''' #Why are SPSS files getting processed anyway? #Does SPSS detection happen *after* upload #Does the file need to be renamed post hoc? #I don't think this can be fixed. Goddamitsomuch. dvurl = kwargs['dv'].strip('\\\\ /') if os.path.splitext(fpath)[1].lower() in NOTAB and not kwargs.get('override'): file_name_clean = os.path.basename(fpath) #file_name = os.path.basename(fpath) + '.NOPROCESS' # using .NOPROCESS doesn't seem to work? file_name = os.path.basename(fpath) + '.NOPROCESS' else: file_name = os.path.basename(fpath) file_name_clean = file_name #My workstation python on Windows produces null for isos for some reason if mimetypes.guess_type('test.iso') == (None, None): mimetypes.add_type('application/x-iso9660-image', '.iso') mime = mimetypes.guess_type(fpath)[0] if kwargs.get('mimetype'): mime = kwargs['mimetype'] if file_name.endswith('.NOPROCESS') or mime == 'application/zip': mime = 'application/octet-stream' #create file metadata in nice, simple, chunks dv4_meta = {'label' : kwargs.get('label', file_name_clean), 'description' : kwargs.get('descr', ''), 'directoryLabel': kwargs.get('dirlabel', ''), 'categories': kwargs.get('tags', []), 'mimetype' : mime} fpath = os.path.abspath(fpath) fields = {'file': (file_name, open(fpath, 'rb'), mime)}#pylint: disable=consider-using-with fields.update({'jsonData' : json.dumps(dv4_meta)}) multi = MultipartEncoder(fields=fields) # use multipart streaming for large files headers = {'X-Dataverse-key' : kwargs.get('apikey'), 'Content-type' : multi.content_type} headers.update(dataverse_utils.UAHEADER) params = {'persistentId' : hdl} LOGGER.info('Uploading %s to %s', fpath, hdl) upload = requests.post(f\"{dvurl}/api/datasets/:persistentId/add\", params=params, headers=headers, data=multi, timeout=kwargs.get('timeout',1000)) try: print(upload.json()) except json.decoder.JSONDecodeError: #This can happend when Glassfish crashes LOGGER.critical(upload.text) print(upload.text) err = ('It\\'s possible Glassfish may have crashed. ' 'Check server logs for anomalies') LOGGER.exception(err) print(err) raise #SPSS files still process despite spoof, so there's #a forcible unlock check fid = upload.json()['data']['files'][0]['dataFile']['id'] print(f'FID: {fid}') if kwargs.get('nowait') and check_lock(dvurl, hdl, kwargs['apikey']): force_notab_unlock(hdl, dvurl, fid, kwargs['apikey']) else: while check_lock(dvurl, hdl, kwargs['apikey']): time.sleep(10) if upload.status_code != 200: LOGGER.critical('Upload failure: %s', (upload.status_code, upload.reason)) raise DvGeneralUploadError(f'\\nReason: {(upload.status_code, upload.reason)}' f'\\n{upload.text}') if kwargs.get('md5'): if upload.json()['data']['files'][0]['dataFile']['md5'] != kwargs.get('md5'): LOGGER.warning('md5sum mismatch on %s', fpath) raise Md5Error('md5sum mismatch') restrict_file(fid=fid, dv=dvurl, apikey=kwargs.get('apikey'), rest=kwargs.get('rest', False))","title":"upload_file"},{"location":"api_ref/#dataverse_utils.upload_from_tsv","text":"Utility for bulk uploading. Assumes fil is formatted as tsv with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. \u2018tags\u2019 field will be split on commas. Parameters: fil \u2013 Open file object or io.IOStream() hdl ( str ) \u2013 Dataverse persistent ID for study (handle or DOI) **kwargs ( dict , default: {} ) \u2013 Other parameters trunc ( str ) \u2013 Leftmost portion of Dataverse study file path to remove. eg: trunc =\u2019/home/user/\u2019 if the tsv field is \u2018/home/user/Data/ASCII\u2019 would set the path for that line of the tsv to \u2018Data/ASCII\u2019. Defaults to None. dv ( ( str , required ) ) \u2013 url to base Dataverse installation eg: \u2018https://abacus.library.ubc.ca\u2019 apikey ( ( str , required ) ) \u2013 API key for user rest ( bool ) \u2013 On True, restrict access. Default False Source code in src/dataverse_utils/dataverse_utils.py def upload_from_tsv(fil, hdl, **kwargs): ''' Utility for bulk uploading. Assumes fil is formatted as tsv with headers 'file', 'description', 'tags'. 'tags' field will be split on commas. Parameters ---------- fil Open file object or io.IOStream() hdl : str Dataverse persistent ID for study (handle or DOI) **kwargs : dict Other parameters Other parameters ---------------- trunc : str Leftmost portion of Dataverse study file path to remove. eg: trunc ='/home/user/' if the tsv field is '/home/user/Data/ASCII' would set the path for that line of the tsv to 'Data/ASCII'. Defaults to None. dv : str, required url to base Dataverse installation eg: 'https://abacus.library.ubc.ca' apikey : str, required API key for user rest : bool, optional On True, restrict access. Default False ''' #reader = csv.reader(fil, delimiter='\\t', quotechar='\"') #new, optional mimetype column allows using GeoJSONS. #Read the headers from the file first before using DictReader headers = fil.readline().strip('\\n\\r').split('\\t')#Goddamn it Windows fil.seek(0) reader = csv.DictReader(fil, fieldnames=headers, quotechar='\"', delimiter='\\t') #See API call for \"Adding File Metadata\" for num, row in enumerate(reader): if num == 0: continue #dirlabel = file_path(row[0], './') if row.get('path'): #Explicit separate path because that way you can organize #on upload dirlabel = row.get('path') else: dirlabel = file_path(row['file'], kwargs.get('trunc', '')) tags = row['tags'].split(',') tags = [x.strip() for x in tags] descr = row['description'] mimetype = row.get('mimetype') params = {'dv' : kwargs.get('dv'), 'tags' : tags, 'descr' : descr, 'dirlabel' : dirlabel, 'apikey' : kwargs.get('apikey'), 'md5' : kwargs.get('md5', ''), 'rest': kwargs.get('rest', False)} if mimetype: params['mimetype'] = mimetype #So that you can pass everything all at once, params #is merged onto kwargs. This is for easier upgradability kwargs.update(params) upload_file(row['file'], hdl, **kwargs)","title":"upload_from_tsv"},{"location":"api_ref/#dataverse_utils.dvdata","text":"Dataverse studies and files","title":"dvdata"},{"location":"api_ref/#dataverse_utils.dvdata.File","text":"Bases: dict Class representing a file on a Dataverse instance Source code in src/dataverse_utils/dvdata.py class File(dict): ''' Class representing a file on a Dataverse instance ''' def __init__(self, url:str, key:str, **kwargs): ''' Dataverse file object Parameters ---------- url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges **kwargs : dict Other parameters Notes ----- To initialize correctly, pass a value from Study['file_info']. Eg: `File('https://test.invalid', 'ABC123', **Study_instance['file_info'][0])` Not to be confused with the FileAnalysis object in `dataverse_utils.collections`. ''' self['url'] = url self.__key = key self['downloaded'] = False self['downloaded_file_name'] = None self['downloaded_checksum'] = None self['verified'] = None #self['dv_file_metadata'] = None # if not self['dv_file_metadata']: # self['dv_file_metadata'] = self._get_file_metadata() for keey, val in kwargs.items(): self[keey] = val self['timeout'] = kwargs.get('timeout', TIMEOUT) def download_file(self): ''' Downloads the file to a temporary location. Data will be in the ORIGINAL format, not Dataverse-processed TSVs ''' if not self['downloaded'] or not os.path.exists(self.get('downloaded_file_name', '')): headers = headers={'X-Dataverse-key':self.__key} headers.update(UAHEADER) try: #curl \"$SERVER_URL/api/access/datafile/:persistentId/?persistentId=$PERSISTENT_ID\" dwnld = requests.get(self['url']+'/api/access/datafile/'+ str(self['dataFile']['id']), headers=headers, params = {'format':'original'}, timeout=self['timeout']) with tempfile.NamedTemporaryFile(delete=False) as fil: self['downloaded_file_name'] = fil.name fil.write(dwnld.content) self['downloaded'] = True return True except requests.exceptions.HTTPError as err: LOGGER.exception(err) LOGGER.exception(traceback.format_exc()) self['downloaded'] = False return False return None def del_tempfile(self): ''' Delete tempfile if it exists ''' if os.path.exists(self['downloaded_file_name']): os.remove(self['downloaded_file_name']) self['downloaded'] = False self['downloaded_file_name'] = None self['verified'] = None def produce_digest(self, prot: str = 'md5', blocksize: int = 2**16) -> str: ''' Returns hex digest for object Parameters ---------- prot : str, optional, default='md5' Hash type. Supported hashes: 'sha1', 'sha224', 'sha256', 'sha384', 'sha512', 'blake2b', 'blake2s', 'md5'. Default: 'md5' blocksize : int, optional, default=2**16 Read block size in bytes ''' if not self['downloaded_file_name']: return None ok_hash = {'sha1' : hashlib.sha1(), 'sha224' : hashlib.sha224(), 'sha256' : hashlib.sha256(), 'sha384' : hashlib.sha384(), 'sha512' : hashlib.sha512(), 'blake2b' : hashlib.blake2b(), 'blake2s' : hashlib.blake2s(), 'md5': hashlib.md5()} with open(self['downloaded_file_name'], 'rb') as _fobj: try: _hash = ok_hash[prot] except (UnboundLocalError, KeyError) as err: message = ('Unsupported hash type. Valid values are ' f'{list(ok_hash)}.' ) LOGGER.exception(err) LOGGER.exception(message) LOGGER.exception(traceback.format_exc()) raise fblock = _fobj.read(blocksize) while fblock: _hash.update(fblock) fblock = _fobj.read(blocksize) return _hash.hexdigest() def verify(self)->None: ''' Compares actual checksum with stated checksum ''' if not self.get('downloaded_file_name') or not self.get('downloaded'): LOGGER.error('File has not been downloaded') self['verified'] = None self['downloaded_checksum'] = None return None _hash = self.produce_digest(self['dataFile']['checksum']['type'].lower()) if _hash == self['dataFile']['checksum']['value']: self['verified'] = True self['downloaded_checksum'] = hash return True LOGGER.error('Checksum mismatch in %s', self.get('label')) self['verified'] = False self['downloaded_checksum'] = _hash return False","title":"File"},{"location":"api_ref/#dataverse_utils.dvdata.File.__init__","text":"Dataverse file object Parameters: url ( str ) \u2013 Base URL to host Dataverse instance key ( str ) \u2013 Dataverse API key with downloader privileges **kwargs ( dict , default: {} ) \u2013 Other parameters Notes To initialize correctly, pass a value from Study[\u2018file_info\u2019]. Eg: File('https://test.invalid', 'ABC123', **Study_instance['file_info'][0]) Not to be confused with the FileAnalysis object in dataverse_utils.collections . Source code in src/dataverse_utils/dvdata.py def __init__(self, url:str, key:str, **kwargs): ''' Dataverse file object Parameters ---------- url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges **kwargs : dict Other parameters Notes ----- To initialize correctly, pass a value from Study['file_info']. Eg: `File('https://test.invalid', 'ABC123', **Study_instance['file_info'][0])` Not to be confused with the FileAnalysis object in `dataverse_utils.collections`. ''' self['url'] = url self.__key = key self['downloaded'] = False self['downloaded_file_name'] = None self['downloaded_checksum'] = None self['verified'] = None #self['dv_file_metadata'] = None # if not self['dv_file_metadata']: # self['dv_file_metadata'] = self._get_file_metadata() for keey, val in kwargs.items(): self[keey] = val self['timeout'] = kwargs.get('timeout', TIMEOUT)","title":"__init__"},{"location":"api_ref/#dataverse_utils.dvdata.File.del_tempfile","text":"Delete tempfile if it exists Source code in src/dataverse_utils/dvdata.py def del_tempfile(self): ''' Delete tempfile if it exists ''' if os.path.exists(self['downloaded_file_name']): os.remove(self['downloaded_file_name']) self['downloaded'] = False self['downloaded_file_name'] = None self['verified'] = None","title":"del_tempfile"},{"location":"api_ref/#dataverse_utils.dvdata.File.download_file","text":"Downloads the file to a temporary location. Data will be in the ORIGINAL format, not Dataverse-processed TSVs Source code in src/dataverse_utils/dvdata.py def download_file(self): ''' Downloads the file to a temporary location. Data will be in the ORIGINAL format, not Dataverse-processed TSVs ''' if not self['downloaded'] or not os.path.exists(self.get('downloaded_file_name', '')): headers = headers={'X-Dataverse-key':self.__key} headers.update(UAHEADER) try: #curl \"$SERVER_URL/api/access/datafile/:persistentId/?persistentId=$PERSISTENT_ID\" dwnld = requests.get(self['url']+'/api/access/datafile/'+ str(self['dataFile']['id']), headers=headers, params = {'format':'original'}, timeout=self['timeout']) with tempfile.NamedTemporaryFile(delete=False) as fil: self['downloaded_file_name'] = fil.name fil.write(dwnld.content) self['downloaded'] = True return True except requests.exceptions.HTTPError as err: LOGGER.exception(err) LOGGER.exception(traceback.format_exc()) self['downloaded'] = False return False return None","title":"download_file"},{"location":"api_ref/#dataverse_utils.dvdata.File.produce_digest","text":"Returns hex digest for object Parameters: prot ( str , default: 'md5' ) \u2013 Hash type. Supported hashes: \u2018sha1\u2019, \u2018sha224\u2019, \u2018sha256\u2019, \u2018sha384\u2019, \u2018sha512\u2019, \u2018blake2b\u2019, \u2018blake2s\u2019, \u2018md5\u2019. Default: \u2018md5\u2019 blocksize ( int , default: 2**16 ) \u2013 Read block size in bytes Source code in src/dataverse_utils/dvdata.py def produce_digest(self, prot: str = 'md5', blocksize: int = 2**16) -> str: ''' Returns hex digest for object Parameters ---------- prot : str, optional, default='md5' Hash type. Supported hashes: 'sha1', 'sha224', 'sha256', 'sha384', 'sha512', 'blake2b', 'blake2s', 'md5'. Default: 'md5' blocksize : int, optional, default=2**16 Read block size in bytes ''' if not self['downloaded_file_name']: return None ok_hash = {'sha1' : hashlib.sha1(), 'sha224' : hashlib.sha224(), 'sha256' : hashlib.sha256(), 'sha384' : hashlib.sha384(), 'sha512' : hashlib.sha512(), 'blake2b' : hashlib.blake2b(), 'blake2s' : hashlib.blake2s(), 'md5': hashlib.md5()} with open(self['downloaded_file_name'], 'rb') as _fobj: try: _hash = ok_hash[prot] except (UnboundLocalError, KeyError) as err: message = ('Unsupported hash type. Valid values are ' f'{list(ok_hash)}.' ) LOGGER.exception(err) LOGGER.exception(message) LOGGER.exception(traceback.format_exc()) raise fblock = _fobj.read(blocksize) while fblock: _hash.update(fblock) fblock = _fobj.read(blocksize) return _hash.hexdigest()","title":"produce_digest"},{"location":"api_ref/#dataverse_utils.dvdata.File.verify","text":"Compares actual checksum with stated checksum Source code in src/dataverse_utils/dvdata.py def verify(self)->None: ''' Compares actual checksum with stated checksum ''' if not self.get('downloaded_file_name') or not self.get('downloaded'): LOGGER.error('File has not been downloaded') self['verified'] = None self['downloaded_checksum'] = None return None _hash = self.produce_digest(self['dataFile']['checksum']['type'].lower()) if _hash == self['dataFile']['checksum']['value']: self['verified'] = True self['downloaded_checksum'] = hash return True LOGGER.error('Checksum mismatch in %s', self.get('label')) self['verified'] = False self['downloaded_checksum'] = _hash return False","title":"verify"},{"location":"api_ref/#dataverse_utils.dvdata.FileInfo","text":"Bases: dict An object representing all of a dataverse study\u2019s files. Easily parseable as a dict. Source code in src/dataverse_utils/dvdata.py class FileInfo(dict): ''' An object representing all of a dataverse study's files. Easily parseable as a dict. ''' #Should this be incorporated into the above class? Probably. def __init__(self, **kwargs)->None: ''' Intialize a File object Parameters ---------- **kwargs : dict Keyword arguments as below Other parameters ---------------- url : str, required Base URL of dataverse installation pid : str, required Handle or DOI of study apikey : str, optional Dataverse API key; required for DRAFT or restricted material. Or if the platform policy requires an API key. timeout : int, optional Optional timeout in seconds ''' self.kwargs = kwargs self['version_list'] = [] self.dv = None self._get_json() self._get_all_files() self['headers'] = list(self[self['current_version']][0].keys()) def _get_json(self) -> None: ''' Get study file json ''' try: headers={'X-Dataverse-key' : self.kwargs.get('apikey')} headers.update(UAHEADER) params = {'persistentId': self.kwargs['pid']} self.dv = requests.get(f'{self.kwargs[\"url\"]}/api/datasets/:persistentId/versions', params=params, timeout=self.kwargs.get('timeout', 100), headers=headers) self.dv.raise_for_status() except (requests.exceptions.RequestException, requests.exceptions.ConnectionError, requests.exceptions.HTTPError, requests.exceptions.TooManyRedirects, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.Timeout, requests.exceptions.JSONDecodeError, requests.exceptions.InvalidSchema) as err: err.add_note(f'Connection error: {\"\\n\".join((str(x) for x in err.args))}') msg = '\\n'.join(getattr(err, '__notes__', [])) LOGGER.critical(msg) raise err def _get_all_files(self): ''' Iterates over self.dv_json()['data']. to produce a list of files in self['files'] ''' try: for num, version in enumerate(self.dv.json()['data']): self._get_version_files(version, current=num) except AttributeError as err: err.add_note('No JSON present') #LOGGER.exception('FileInfo AttributeError: %s', err) #LOGGER.exception(traceback.format_exc()) raise err except KeyError as err: err.add_note(f'JSON parsing error: {err}') err.add_note('Offending JSON:') err.add_note(f'{self.dv.json()}') msg = '\\n'.join(getattr(err, '__notes__', [])) LOGGER.exception('FileInfo KeyError: %s', msg) #LOGGER.exception(traceback.format_exc()) raise err def _get_version_files(self, flist: list, current=1)->None: ''' Set version number and assign file info a version key Parameters ---------- flist : list list of file metadata for a particular version current: int, optional, default=1 Value of zero represents most current version ''' if flist['versionState'] == 'DRAFT': ver_info='DRAFT' else: ver_info = f\"{flist['versionNumber']}.{flist['versionMinorNumber']}\" if current == 0: self['current_version'] = ver_info self['version_list'].append(ver_info) self[ver_info] = [] for fil in flist['files']: self[ver_info].append(self._get_file_info(fil, ver_info=ver_info, state_info=flist['versionState'])) def _get_file_info(self, file:dict, **kwargs)->dict: ''' Returns a dict of required info from a chunk of dataverse study version metadata Parameters ---------- file : dict The dict containing one file's metadata **kwargs : dict Keyword arguments version_info: str Version info string state_info : str Publication state ''' # headers = ['file', 'description', 'pidURL','downloadURL', 'version', 'state'] file_name = file['dataFile'].get('originalFileName', file['label']) filepath = pathlib.Path(file.get('directoryLabel', ''), file_name) description = file.get('description', '') try: pid_url = file['dataFile']['pidURL'] except KeyError: pid_url = f'{self.kwargs[\"url\"]}/file.xhtml?fileId={file[\"dataFile\"][\"id\"]}' fid = file['dataFile']['id'] download_url = f'{self.kwargs[\"url\"]}/api/access/datafile/{fid}?format=original' out = {'file': str(filepath).strip(), 'description': description.strip(), 'pid_url': pid_url, 'download_url':download_url, 'version': kwargs['ver_info'], 'state' : kwargs['state_info']} return out","title":"FileInfo"},{"location":"api_ref/#dataverse_utils.dvdata.FileInfo.__init__","text":"Intialize a File object Parameters: **kwargs ( dict , default: {} ) \u2013 Keyword arguments as below url ( ( str , required ) ) \u2013 Base URL of dataverse installation pid ( ( str , required ) ) \u2013 Handle or DOI of study apikey ( str ) \u2013 Dataverse API key; required for DRAFT or restricted material. Or if the platform policy requires an API key. timeout ( int ) \u2013 Optional timeout in seconds Source code in src/dataverse_utils/dvdata.py def __init__(self, **kwargs)->None: ''' Intialize a File object Parameters ---------- **kwargs : dict Keyword arguments as below Other parameters ---------------- url : str, required Base URL of dataverse installation pid : str, required Handle or DOI of study apikey : str, optional Dataverse API key; required for DRAFT or restricted material. Or if the platform policy requires an API key. timeout : int, optional Optional timeout in seconds ''' self.kwargs = kwargs self['version_list'] = [] self.dv = None self._get_json() self._get_all_files() self['headers'] = list(self[self['current_version']][0].keys())","title":"__init__"},{"location":"api_ref/#dataverse_utils.dvdata.Study","text":"Bases: dict Dataverse record. Dataverse study records are pure metadata so this is represented with a dictionary. Source code in src/dataverse_utils/dvdata.py class Study(dict): #pylint: disable=too-few-public-methods ''' Dataverse record. Dataverse study records are pure metadata so this is represented with a dictionary. ''' def __init__(self, pid: str, url:str, key:str, **kwargs): ''' Initialize a Study object Parameters ---------- pid : str Record persistent identifier: hdl or doi url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges **kwargs : dict Keyword arguments Other parameters ---------------- timeout : int Request timeout in seconds ''' self['pid'] = pid self['url'] = url self.__key = key self['orig_json'] = None self['timeout'] = kwargs.get('timeout',TIMEOUT) if not self['orig_json']: self['orig_json'] = self._orig_json() self['upload_json'] = self._upload_json self['file_info'] = self['orig_json']['files'] self['file_ids'] = [x['dataFile'].get('id') for x in self['orig_json']['files']] self['file_persistentIds'] = self._get_file_pids() self['source_version'] = Study.get_version(url) self['target_version'] = None if not self['target_version']: self['target_version'] = Study.get_version(url) @classmethod def get_version(cls, url:str, timeout:int=100)->float: ''' Returns a float representing a Dataverse version number. Floating point value composed of: float(f'{major_version}.{minor_verson:03d}{patch:03d}') ie, version 5.9.2 would be 5.009002 Parameters ---------- url : str URL of base Dataverse instance. eg: 'https://abacus.library.ubc.ca' timeout : int, default=100 Request timeout in seconds ''' ver = requests.get(f'{url}/api/info/version', headers=UAHEADER, #headers = {'X-Dataverse-key' : key}, timeout = timeout) try: ver.raise_for_status() except requests.exceptions.HTTPError as exc: LOGGER.error(r'Error getting version for {url}') LOGGER.exception(exc) LOGGER.exception(traceback.format_exc()) raise requests.exceptions.HTTPError #Scholars Portal version is formatted as v5.13.9-SP, so. . . verf = ver.json()['data']['version'].strip('v ').split('.') verf = [x.split('-')[0] for x in verf] verf =[int(b)/10**(3*a) for a,b in enumerate(verf)] #it's 3*a in case for some reason we hit, say v5.99.99 and there's more before v6. verf = sum(verf) return verf def set_version(self, url:str, timeout:int=100)->None: ''' Sets self['target_version'] to appropriate integer value *AND* formats self['upload_json'] to correct JSON format Parameters ---------- url : str URL of *target* Dataverse instance timeout : int, optional, default=100 request timeout in seconds ''' self['target_version'] = Study.get_version(url, timeout) # Now fix the metadata to work with various versions if self['target_version'] >= 5.010: self.fix_licence() if self['target_version'] >= 5.013: self.production_location() def _orig_json(self) -> dict: ''' Latest study version record JSON. Retrieved from Dataverse installation so an internet connection is required. ''' #curl -H \"X-Dataverse-key:$API_TOKEN\" / #$SERVER_URL/api/datasets/:persistentId/?persistentId=$PERSISTENT_IDENTIFIER headers = {'X-Dataverse-key' : self.__key} headers.update(UAHEADER) getjson = requests.get(self['url']+'/api/datasets/:persistentId', headers=headers, params = {'persistentId': self['pid']}, timeout = self['timeout']) getjson.raise_for_status() return getjson.json()['data']['latestVersion'] def __add_email(self, upjson): ''' Adds contact information if it's not there. Fills with dummy data Parameters ---------- upjson : dict Metadata ''' #pylint: disable=possibly-used-before-assignment for n, v in enumerate((upjson['datasetVersion'] ['metadataBlocks']['citation']['fields'])): if v['typeName'] == 'datasetContact': contact_no = n for _x in (upjson['datasetVersion']['metadataBlocks'] ['citation']['fields'][contact_no]['value']): if not _x.get('datasetContactEmail'): _x['datasetContactEmail'] = {'typeName':'datasetContactEmail', 'multiple': False, 'typeClass':'primitive', 'value': 'suppressed_value@test.invalid'} return upjson @property def _upload_json(self)->dict: ''' A Dataverse JSON record with with PIDs and other information stripped suitable for upload as a new Dataverse study record. ''' upj = {'datasetVersion': {'license': self['orig_json']['license'], 'termsOfUse': self['orig_json'].get('termsOfUse',''), 'metadataBlocks': self['orig_json']['metadataBlocks'] } } return self.__add_email(upj) @property def _oldupload_json(self)->dict: ''' A Dataverse JSON record with with PIDs and other information stripped suitable for upload as a new Dataverse study record. ''' return {'datasetVersion': {'license': self['orig_json']['license'], 'termsOfUse': self['orig_json'].get('termsOfUse',''), 'metadataBlocks': self['orig_json']['metadataBlocks'] } } def _get_file_pids(self)->list: ''' Returns a list of file ids representing the file objects in dataverse record ''' pids = [x['dataFile'].get('persistentId') for x in self['orig_json']['files']] if not all(pids): return None return pids ###### #JSON metdata fixes for different versions ###### def fix_licence(self)->None: ''' Replaces non-standard licence with None Notes ----- With Dataverse v5.10+, a licence type of 'NONE' is now forbidden. Now, as per <https://guides.dataverse.org/en/5.14/api/sword.html\\ ?highlight=invalid%20license>, non-standard licences may be replaced with None. This function edits the same Study object *in place*, so returns nothing. ''' if self['upload_json']['datasetVersion']['license'] == 'NONE': self['upload_json']['datasetVersion']['license'] = None if not self['upload_json']['datasetVersion']['termsOfUse']: #This shouldn't happen, but UBC has datasets from the early 1970s self['upload_json']['datasetVersion']['termsOfUse'] = 'Not available' def production_location(self)->None: ''' Changes \"multiple\" to True where typeName == 'productionPlace' in Study['upload_json'] Changes are done *in place*. Notes ----- Multiple production places came into effect with Dataverse v5.13 ''' #{'typeName': 'productionPlace', 'multiple': True, 'typeClass': 'primitive', #'value': ['Vancouver, BC', 'Ottawa, ON']} # get index indy = None for ind, val in enumerate(self['upload_json']['datasetVersion']\\ ['metadataBlocks']['citation']['fields']): if val['typeName'] == 'productionPlace': indy = ind break if indy and not self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['multiple']: self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['multiple'] = True self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['value'] = [self['upload_json']['datasetVersion']\\ ['metadataBlocks']['citation']\\ ['fields'][indy]['value']]","title":"Study"},{"location":"api_ref/#dataverse_utils.dvdata.Study.__add_email","text":"Adds contact information if it\u2019s not there. Fills with dummy data Parameters: upjson ( dict ) \u2013 Metadata Source code in src/dataverse_utils/dvdata.py def __add_email(self, upjson): ''' Adds contact information if it's not there. Fills with dummy data Parameters ---------- upjson : dict Metadata ''' #pylint: disable=possibly-used-before-assignment for n, v in enumerate((upjson['datasetVersion'] ['metadataBlocks']['citation']['fields'])): if v['typeName'] == 'datasetContact': contact_no = n for _x in (upjson['datasetVersion']['metadataBlocks'] ['citation']['fields'][contact_no]['value']): if not _x.get('datasetContactEmail'): _x['datasetContactEmail'] = {'typeName':'datasetContactEmail', 'multiple': False, 'typeClass':'primitive', 'value': 'suppressed_value@test.invalid'} return upjson","title":"__add_email"},{"location":"api_ref/#dataverse_utils.dvdata.Study.__init__","text":"Initialize a Study object Parameters: pid ( str ) \u2013 Record persistent identifier: hdl or doi url ( str ) \u2013 Base URL to host Dataverse instance key ( str ) \u2013 Dataverse API key with downloader privileges **kwargs ( dict , default: {} ) \u2013 Keyword arguments timeout ( int ) \u2013 Request timeout in seconds Source code in src/dataverse_utils/dvdata.py def __init__(self, pid: str, url:str, key:str, **kwargs): ''' Initialize a Study object Parameters ---------- pid : str Record persistent identifier: hdl or doi url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges **kwargs : dict Keyword arguments Other parameters ---------------- timeout : int Request timeout in seconds ''' self['pid'] = pid self['url'] = url self.__key = key self['orig_json'] = None self['timeout'] = kwargs.get('timeout',TIMEOUT) if not self['orig_json']: self['orig_json'] = self._orig_json() self['upload_json'] = self._upload_json self['file_info'] = self['orig_json']['files'] self['file_ids'] = [x['dataFile'].get('id') for x in self['orig_json']['files']] self['file_persistentIds'] = self._get_file_pids() self['source_version'] = Study.get_version(url) self['target_version'] = None if not self['target_version']: self['target_version'] = Study.get_version(url)","title":"__init__"},{"location":"api_ref/#dataverse_utils.dvdata.Study.fix_licence","text":"Replaces non-standard licence with None Notes With Dataverse v5.10+, a licence type of \u2018NONE\u2019 is now forbidden. Now, as per https://guides.dataverse.org/en/5.14/api/sword.html ?highlight=invalid%20license , non-standard licences may be replaced with None. This function edits the same Study object in place , so returns nothing. Source code in src/dataverse_utils/dvdata.py def fix_licence(self)->None: ''' Replaces non-standard licence with None Notes ----- With Dataverse v5.10+, a licence type of 'NONE' is now forbidden. Now, as per <https://guides.dataverse.org/en/5.14/api/sword.html\\ ?highlight=invalid%20license>, non-standard licences may be replaced with None. This function edits the same Study object *in place*, so returns nothing. ''' if self['upload_json']['datasetVersion']['license'] == 'NONE': self['upload_json']['datasetVersion']['license'] = None if not self['upload_json']['datasetVersion']['termsOfUse']: #This shouldn't happen, but UBC has datasets from the early 1970s self['upload_json']['datasetVersion']['termsOfUse'] = 'Not available'","title":"fix_licence"},{"location":"api_ref/#dataverse_utils.dvdata.Study.get_version","text":"Returns a float representing a Dataverse version number. Floating point value composed of: float(f\u2019{major_version}.{minor_verson:03d}{patch:03d}\u2019) ie, version 5.9.2 would be 5.009002 Parameters: url ( str ) \u2013 URL of base Dataverse instance. eg: \u2018https://abacus.library.ubc.ca\u2019 timeout ( int , default: 100 ) \u2013 Request timeout in seconds Source code in src/dataverse_utils/dvdata.py @classmethod def get_version(cls, url:str, timeout:int=100)->float: ''' Returns a float representing a Dataverse version number. Floating point value composed of: float(f'{major_version}.{minor_verson:03d}{patch:03d}') ie, version 5.9.2 would be 5.009002 Parameters ---------- url : str URL of base Dataverse instance. eg: 'https://abacus.library.ubc.ca' timeout : int, default=100 Request timeout in seconds ''' ver = requests.get(f'{url}/api/info/version', headers=UAHEADER, #headers = {'X-Dataverse-key' : key}, timeout = timeout) try: ver.raise_for_status() except requests.exceptions.HTTPError as exc: LOGGER.error(r'Error getting version for {url}') LOGGER.exception(exc) LOGGER.exception(traceback.format_exc()) raise requests.exceptions.HTTPError #Scholars Portal version is formatted as v5.13.9-SP, so. . . verf = ver.json()['data']['version'].strip('v ').split('.') verf = [x.split('-')[0] for x in verf] verf =[int(b)/10**(3*a) for a,b in enumerate(verf)] #it's 3*a in case for some reason we hit, say v5.99.99 and there's more before v6. verf = sum(verf) return verf","title":"get_version"},{"location":"api_ref/#dataverse_utils.dvdata.Study.production_location","text":"Changes \u201cmultiple\u201d to True where typeName == \u2018productionPlace\u2019 in Study[\u2018upload_json\u2019] Changes are done in place . Notes Multiple production places came into effect with Dataverse v5.13 Source code in src/dataverse_utils/dvdata.py def production_location(self)->None: ''' Changes \"multiple\" to True where typeName == 'productionPlace' in Study['upload_json'] Changes are done *in place*. Notes ----- Multiple production places came into effect with Dataverse v5.13 ''' #{'typeName': 'productionPlace', 'multiple': True, 'typeClass': 'primitive', #'value': ['Vancouver, BC', 'Ottawa, ON']} # get index indy = None for ind, val in enumerate(self['upload_json']['datasetVersion']\\ ['metadataBlocks']['citation']['fields']): if val['typeName'] == 'productionPlace': indy = ind break if indy and not self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['multiple']: self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['multiple'] = True self['upload_json']['datasetVersion']['metadataBlocks']\\ ['citation']['fields'][indy]['value'] = [self['upload_json']['datasetVersion']\\ ['metadataBlocks']['citation']\\ ['fields'][indy]['value']]","title":"production_location"},{"location":"api_ref/#dataverse_utils.dvdata.Study.set_version","text":"Sets self[\u2018target_version\u2019] to appropriate integer value AND formats self[\u2018upload_json\u2019] to correct JSON format Parameters: url ( str ) \u2013 URL of target Dataverse instance timeout ( int , default: 100 ) \u2013 request timeout in seconds Source code in src/dataverse_utils/dvdata.py def set_version(self, url:str, timeout:int=100)->None: ''' Sets self['target_version'] to appropriate integer value *AND* formats self['upload_json'] to correct JSON format Parameters ---------- url : str URL of *target* Dataverse instance timeout : int, optional, default=100 request timeout in seconds ''' self['target_version'] = Study.get_version(url, timeout) # Now fix the metadata to work with various versions if self['target_version'] >= 5.010: self.fix_licence() if self['target_version'] >= 5.013: self.production_location()","title":"set_version"},{"location":"api_ref/#dataverse_utils.ldc","text":"Creates dataverse JSON from Linguistic Data Consortium website page.","title":"ldc"},{"location":"api_ref/#dataverse_utils.ldc.Ldc","text":"Bases: Serializer An LDC item (eg, LDC2021T01) Source code in src/dataverse_utils/ldc.py class Ldc(ds.Serializer):#pylint: disable=too-many-instance-attributes ''' An LDC item (eg, LDC2021T01) ''' #pylint: disable=super-init-not-called, arguments-differ def __init__(self, ldc, cert=None): ''' Returns a dict with keys created from an LDC catalogue web page. Parameters ---------- ldc : str Linguistic Consortium Catalogue Number (eg. 'LDC2015T05'. This is what forms the last part of the LDC catalogue URL. cert : str, optional, default=None Path to certificate chain; LDC has had a problem with intermediate certificates, so you can download the chain with a browser and supply a path to the .pem with this parameter ''' self.ldc = ldc.strip().upper() self.ldcHtml = None self._ldcJson = None self._dryadJson = None self._dvJson = None self.cert = cert self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=ds.constants.RETRY_STRATEGY)) if self.cert: self.cert = os.path.expanduser(self.cert) self.__fixdesc = None @property def ldcJson(self): ''' Returns a JSON based on the LDC web page scraping ''' if not self._ldcJson: self._ldcJson = self.make_ldc_json() return self._ldcJson @property def dryadJson(self): ''' LDC metadata in Dryad JSON format ''' if not self._dryadJson: self._dryadJson = self.make_dryad_json() return self._dryadJson @property def dvJson(self): ''' LDC metadata in Dataverse JSON format ''' #return False if not self._dvJson: self._dvJson = self.make_dv_json() return self._dvJson @property def embargo(self)->bool: ''' Boolean indicating embargo status ''' return False @property def fileJson(self): ''' Returns False: No attached files possible at LDC ''' return False @property def files(self): ''' Returns None. No files possible ''' return None @property def oversize(self, maxsize=None): ''' Make sure file is not too big for the Dataverse instance Parameters ---------- maxsize : int, optional, default=None Maximum size in bytes ''' #pylint: disable=property-with-parameters if not maxsize: maxsize = ds.constants.MAX_UPLOAD @property def id(self): ''' Returns LDC ID ''' return self.ldc def fetch_record(self, timeout=45): ''' Downloads record from LDC website Parameters ---------- timeout : int, optional, default=45 Request timeout in seconds ''' interim = self.session.get(f'https://catalog.ldc.upenn.edu/{self.ldc}', verify=self.cert, timeout=timeout) interim.raise_for_status() self.ldcHtml = interim.text def make_ldc_json(self): ''' Returns a dict with keys created from an LDC catalogue web page. ''' if not self.ldcHtml: self.fetch_record() soup = bs(self.ldcHtml, 'html.parser') #Should data just look in the *first* table? Specifically tbody? #Is it always the first? I assume yes. tbody = soup.find('tbody')#new data = [x.text.strip() for x in tbody.find_all('td')]#new #data = [x.text.strip() for x in soup.find_all('td')]#original LDC_dict = {data[x][:data[x].find('\\n')].strip(): data[x+1].strip() for x in range(0, len(data), 2)} #Related Works appears to have an extra 'Hide' at the end if LDC_dict.get('Related Works:'): LDC_dict['Related Works'] = (x.strip() for x in LDC_dict['Related Works:'].split('\\n')) del LDC_dict['Related Works:'] #remove the renamed key LDC_dict['Linguistic Data Consortium'] = LDC_dict['LDC Catalog No.'] del LDC_dict['LDC Catalog No.']#This key must be renamed for consistency LDC_dict['Author(s)'] = [x.strip() for x in LDC_dict['Author(s)'].split(',')] #Other metadata probably has HTML in it, so we keep as much as possible other_meta = soup.find_all('div') alldesc = [x for x in other_meta if x.attrs.get('itemprop') == 'description'] #sometimes they format pages oddly and we can use this for a #quick and dirty fix self.__fixdesc = copy.deepcopy(alldesc) #sections use h3, so split on these #24 Jan 23 Apparently, this is all done manually so some of them sometime use h4. #Because reasons. #was: #alldesc = str(alldesc).split('<h3>') #is now alldesc = str(alldesc).replace('h4>', 'h3>').split('<h3>') for i in range(1, len(alldesc)): alldesc[i] = '<h3>' + alldesc[i] #first one is not actually useful, so discard it alldesc.pop(0) #So far, so good. At this point the relative links need fixing #and tables need to be converted to pre. for desc in alldesc: #It's already strings; replace relative links first desc = desc.replace('../../../', 'https://catalog.ldc.upenn.edu/') subsoup = bs(desc, 'html.parser') key = subsoup.h3.text.strip() #don't need the h3 tags anymore subsoup.find('h3').extract() # Convert tables to <pre> for tab in subsoup.find_all('table'): content = str(tab) #convert to markdown content = markdownify.markdownify(content) tab.name = 'pre' tab.string = content #There is not much documentation on the #difference between tab.string and tab.content #That was relatively easy LDC_dict[key] = str(subsoup) LDC_dict['Introduction'] = LDC_dict.get('Introduction', self.__no_intro()) #LDC puts http in front of their DOI identifier if LDC_dict.get('DOI'): LDC_dict['DOI'] = LDC_dict['DOI'].strip('https://doi.org/') return LDC_dict def __no_intro(self)->str: ''' Makes an introduction even if they forgot to include the word \"Introduction\" ''' #self.__fixdesc is set in make_ldc_json intro = [x for x in self.__fixdesc if self.__fixdesc[0]['itemprop']=='description'][0] while intro.find('div'): #nested?, not cleaning properly intro.find('div').unwrap() # remove the div tag intro = str(intro) #Normally, there's an <h3>Introduction</h3> but sometimes there's not #Assumes that the first section up to \"<h\" is an intro. #You know what they say about assuming intro = intro[:intro.find('<h')] start = intro.find('<div') if start != -1: end = intro.find('>',start)+1 intro = intro.replace(intro[start:end], '').strip() return intro @staticmethod def name_parser(name): ''' Returns lastName/firstName JSON snippet from a name Parameters ---------- name : str A name Notes ----- Can't be 100% accurate, because names can be split in many ways. However, as they say, 80% is good enough. ''' names = name.split(' ') return {'lastName': names[-1], 'firstName': ' '.join(names[:-1]), 'affiliation':''} def make_dryad_json(self, ldc=None): ''' Creates a Dryad-style dict from an LDC dictionary Parameters ---------- ldc : dict, optional, default=self.ldcJson Dictionary containing LDC data. Defaults to self.ldcJson ''' if not ldc: ldc = self.ldcJson print(ldc) dryad = {} dryad['title'] = ldc['Item Name'] dryad['authors'] = [Ldc.name_parser(x) for x in ldc['Author(s)']] abstract = ('<p><b>Introduction</b></p>' f\"<p>{ldc['Introduction']}</p>\" '<p><b>Data</b></p>' f\"<p>{ldc['Data']}</p>\") if ldc.get('Acknowledgement'): abstract += ('<p><b>Acknowledgement</b></p>' f\"<p>{ldc['Acknowledgement']}</p>\") dryad['abstract'] = abstract dryad['keywords'] = ['Linguistics'] #Dataverse accepts only ISO formatted date try: releaseDate = time.strptime(ldc['Release Date'], '%B %d, %Y') releaseDate = time.strftime('%Y-%m-%d', releaseDate) except KeyError: #Older surveys don't have a release date field #so it must be created from the record number if self.ldc[3] == '9': releaseDate = '19' + self.ldc[3:5] dryad['lastModificationDate'] = releaseDate dryad['publicationDate'] = releaseDate return dryad def _make_note(self, ldc=None)->str: ''' Creates a generalizes HTML notes field from a bunch of LDC fields that don't fit into dataverse Parameters ---------- ldc : dict, optional, default=self.ldcJson Dictionary containing LDC data ''' if not ldc: ldc = self.ldcJson note_fields = ['DCMI Type(s)', 'Sample Type', 'Sample Rate', 'Application(s)', 'Language(s)', 'Language ID(s)'] outhtml = [] for note in note_fields: if ldc.get(note): data = ldc[note].split(',') data = [x.strip() for x in data] data = ', '.join(data) if note != 'Language ID(s)': data = data[0].capitalize() + data[1:] #data = [x.capitalize() for x in data] outhtml.append(f'{note}: {data}') outhtml.append(f'Metadata automatically created from ' f'<a href=\"https://catalog.ldc.upenn.edu/{self.ldc}\">' f'https://catalog.ldc.upenn.edu/{self.ldc}</a> ' f'[{time.strftime(\"%d %b %Y\", time.localtime())}]') return '<br />'.join(outhtml) @staticmethod def find_block_index(dvjson, key): ''' Finds the index number of an item in Dataverse's idiotic JSON list Parameters ---------- dvjson : dict Dataverse JSON key : str key for which to find list index ''' for num, item in enumerate(dvjson['datasetVersion'] ['metadataBlocks']['citation']['fields']): if item['typeName'] == key: return num return None def make_dv_json(self, ldc=None):#pylint: disable=too-many-locals, too-many-statements ''' Returns complete Dataverse JSON Parameters ---------- ldc : dict, optional, default=self.ldcJson LDC dictionary. ''' if not ldc: ldc = self.ldcJson dvjson = super().dvJson.copy() #ID Numbers otherid = super()._typeclass('otherId', True, 'compound') ids = [] for item in ['Linguistic Data Consortium', 'ISBN', 'ISLRN', 'DOI']: if ldc.get(item): out = {} agency = super()._convert_generic(inJson={item:item}, dryField=item, dvField='otherIdAgency') value = super()._convert_generic(inJson={item:ldc[item]}, dryField=item, dvField='otherIdValue') out.update(agency) out.update(value) ids.append(out) otherid['value'] = ids dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(otherid) #Producer and publisher prod = super()._typeclass('producer', True, 'compound') p_name = super()._convert_generic(inJson={'producerName': 'Linguistic Data Consortium'}, dryField='producerName', dvField='producerName') p_affil = super()._convert_generic(inJson={'producerAffiliation': 'University of Pennsylvania'}, dryField='producerName', dvField='producerName') p_url = super()._convert_generic(inJson={'producerURL': 'https://www.ldc.upenn.edu/'}, dryField='producerURL', dvField='producerURL') p_name.update(p_affil) p_name.update(p_url) prod['value'] = [p_name] dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(prod) #Kind of data kind = super()._typeclass('kindOfData', True, 'primitive') kind['value'] = 'Linguistic data' #Series series = super()._typeclass('series', False, 'compound') s_name = super()._convert_generic(inJson={'seriesName': 'LDC'}, dryField='seriesName', dvField='seriesName') s_info = super()._convert_generic(inJson={'seriesInformation': 'Linguistic Data Consortium'}, dryField='seriesInformation', dvField='seriesInformation') s_name.update(s_info) series['value'] = s_name #not a list dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(series) #Data sources series = super()._typeclass('dataSources', True, 'primitive') data_sources = ldc['Data Source(s)'].split(',') data_sources = [x.strip().capitalize() for x in data_sources] series['value'] = data_sources dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(series) #Fix keyword labels that are hardcoded for Dryad #There should be only one keyword block keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion']['metadataBlocks'] ['citation']['fields']) if y.get('typeName') == 'keyword'][0] key_pos = [x for x, y in enumerate(keyword_field[1]['value']) if y['keywordVocabulary']['value'] == 'Dryad'][0] dvjson['datasetVersion']['metadataBlocks']['citation']\\ ['fields'][keyword_field[0]]['value'][key_pos]\\ ['keywordVocabulary']['value'] = 'Linguistic Data Consortium' #The first keyword field is hardcoded in by dryad2dataverse.serializer #So I think it needs to be deleted keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion']['metadataBlocks']['citation']['fields']) if y.get('typeName') == 'otherId'][0] #ibid del dvjson['datasetVersion']['metadataBlocks']['citation']['fields'][keyword_field[0]] #Notes note_index = Ldc.find_block_index(dvjson, 'notesText') if note_index: dvjson['datasetVersion']['metadataBlocks']['citation']\\ ['fields'][note_index]['value'] = self._make_note() else: notes = super()._typeclass('notesText', False, 'primitive') notes['value'] = self._make_note() dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(notes) #Deletes unused \"publication\" fields: rewrite to make it a function call. keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion'] ['metadataBlocks']['citation']['fields']) if y.get('typeName') == 'publication'][0] #ibid del dvjson['datasetVersion']['metadataBlocks']['citation']['fields'][keyword_field[0]] #And now the licence: dvjson['datasetVersion']['license'] = LIC_NAME dvjson['datasetVersion']['termsOfUse'] = LICENCE return dvjson def upload_metadata(self, **kwargs) -> dict: ''' Uploads metadata to dataverse. Returns json from connection attempt. Parameters ---------- **kwargs : dict Parameters Other parameters ---------------- url : str base url to Dataverse installation key : str api key dv : str Dataverse to which it is being uploaded ''' url = kwargs['url'].strip('/') key = kwargs['key'] dv = kwargs['dv'] json = kwargs.get('json', self.dvJson) headers = {'X-Dataverse-key':key} headers.update(UAHEADER) try: upload = self.session.post(f'{url}/api/dataverses/{dv}/datasets', headers=headers, json=json) upload.raise_for_status() return upload.json() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError): print(upload.text) raise","title":"Ldc"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.dryadJson","text":"LDC metadata in Dryad JSON format","title":"dryadJson"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.dvJson","text":"LDC metadata in Dataverse JSON format","title":"dvJson"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.embargo","text":"Boolean indicating embargo status","title":"embargo"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.fileJson","text":"Returns False: No attached files possible at LDC","title":"fileJson"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.files","text":"Returns None. No files possible","title":"files"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.id","text":"Returns LDC ID","title":"id"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.ldcJson","text":"Returns a JSON based on the LDC web page scraping","title":"ldcJson"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.oversize","text":"Make sure file is not too big for the Dataverse instance Parameters: maxsize ( int , default: None ) \u2013 Maximum size in bytes","title":"oversize"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.__init__","text":"Returns a dict with keys created from an LDC catalogue web page. Parameters: ldc ( str ) \u2013 Linguistic Consortium Catalogue Number (eg. \u2018LDC2015T05\u2019. This is what forms the last part of the LDC catalogue URL. cert ( str , default: None ) \u2013 Path to certificate chain; LDC has had a problem with intermediate certificates, so you can download the chain with a browser and supply a path to the .pem with this parameter Source code in src/dataverse_utils/ldc.py def __init__(self, ldc, cert=None): ''' Returns a dict with keys created from an LDC catalogue web page. Parameters ---------- ldc : str Linguistic Consortium Catalogue Number (eg. 'LDC2015T05'. This is what forms the last part of the LDC catalogue URL. cert : str, optional, default=None Path to certificate chain; LDC has had a problem with intermediate certificates, so you can download the chain with a browser and supply a path to the .pem with this parameter ''' self.ldc = ldc.strip().upper() self.ldcHtml = None self._ldcJson = None self._dryadJson = None self._dvJson = None self.cert = cert self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=ds.constants.RETRY_STRATEGY)) if self.cert: self.cert = os.path.expanduser(self.cert) self.__fixdesc = None","title":"__init__"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.__no_intro","text":"Makes an introduction even if they forgot to include the word \u201cIntroduction\u201d Source code in src/dataverse_utils/ldc.py def __no_intro(self)->str: ''' Makes an introduction even if they forgot to include the word \"Introduction\" ''' #self.__fixdesc is set in make_ldc_json intro = [x for x in self.__fixdesc if self.__fixdesc[0]['itemprop']=='description'][0] while intro.find('div'): #nested?, not cleaning properly intro.find('div').unwrap() # remove the div tag intro = str(intro) #Normally, there's an <h3>Introduction</h3> but sometimes there's not #Assumes that the first section up to \"<h\" is an intro. #You know what they say about assuming intro = intro[:intro.find('<h')] start = intro.find('<div') if start != -1: end = intro.find('>',start)+1 intro = intro.replace(intro[start:end], '').strip() return intro","title":"__no_intro"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.fetch_record","text":"Downloads record from LDC website Parameters: timeout ( int , default: 45 ) \u2013 Request timeout in seconds Source code in src/dataverse_utils/ldc.py def fetch_record(self, timeout=45): ''' Downloads record from LDC website Parameters ---------- timeout : int, optional, default=45 Request timeout in seconds ''' interim = self.session.get(f'https://catalog.ldc.upenn.edu/{self.ldc}', verify=self.cert, timeout=timeout) interim.raise_for_status() self.ldcHtml = interim.text","title":"fetch_record"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.find_block_index","text":"Finds the index number of an item in Dataverse\u2019s idiotic JSON list Parameters: dvjson ( dict ) \u2013 Dataverse JSON key ( str ) \u2013 key for which to find list index Source code in src/dataverse_utils/ldc.py @staticmethod def find_block_index(dvjson, key): ''' Finds the index number of an item in Dataverse's idiotic JSON list Parameters ---------- dvjson : dict Dataverse JSON key : str key for which to find list index ''' for num, item in enumerate(dvjson['datasetVersion'] ['metadataBlocks']['citation']['fields']): if item['typeName'] == key: return num return None","title":"find_block_index"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.make_dryad_json","text":"Creates a Dryad-style dict from an LDC dictionary Parameters: ldc ( dict , default: self.ldcJson ) \u2013 Dictionary containing LDC data. Defaults to self.ldcJson Source code in src/dataverse_utils/ldc.py def make_dryad_json(self, ldc=None): ''' Creates a Dryad-style dict from an LDC dictionary Parameters ---------- ldc : dict, optional, default=self.ldcJson Dictionary containing LDC data. Defaults to self.ldcJson ''' if not ldc: ldc = self.ldcJson print(ldc) dryad = {} dryad['title'] = ldc['Item Name'] dryad['authors'] = [Ldc.name_parser(x) for x in ldc['Author(s)']] abstract = ('<p><b>Introduction</b></p>' f\"<p>{ldc['Introduction']}</p>\" '<p><b>Data</b></p>' f\"<p>{ldc['Data']}</p>\") if ldc.get('Acknowledgement'): abstract += ('<p><b>Acknowledgement</b></p>' f\"<p>{ldc['Acknowledgement']}</p>\") dryad['abstract'] = abstract dryad['keywords'] = ['Linguistics'] #Dataverse accepts only ISO formatted date try: releaseDate = time.strptime(ldc['Release Date'], '%B %d, %Y') releaseDate = time.strftime('%Y-%m-%d', releaseDate) except KeyError: #Older surveys don't have a release date field #so it must be created from the record number if self.ldc[3] == '9': releaseDate = '19' + self.ldc[3:5] dryad['lastModificationDate'] = releaseDate dryad['publicationDate'] = releaseDate return dryad","title":"make_dryad_json"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.make_dv_json","text":"Returns complete Dataverse JSON Parameters: ldc ( dict , default: self.ldcJson ) \u2013 LDC dictionary. Source code in src/dataverse_utils/ldc.py def make_dv_json(self, ldc=None):#pylint: disable=too-many-locals, too-many-statements ''' Returns complete Dataverse JSON Parameters ---------- ldc : dict, optional, default=self.ldcJson LDC dictionary. ''' if not ldc: ldc = self.ldcJson dvjson = super().dvJson.copy() #ID Numbers otherid = super()._typeclass('otherId', True, 'compound') ids = [] for item in ['Linguistic Data Consortium', 'ISBN', 'ISLRN', 'DOI']: if ldc.get(item): out = {} agency = super()._convert_generic(inJson={item:item}, dryField=item, dvField='otherIdAgency') value = super()._convert_generic(inJson={item:ldc[item]}, dryField=item, dvField='otherIdValue') out.update(agency) out.update(value) ids.append(out) otherid['value'] = ids dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(otherid) #Producer and publisher prod = super()._typeclass('producer', True, 'compound') p_name = super()._convert_generic(inJson={'producerName': 'Linguistic Data Consortium'}, dryField='producerName', dvField='producerName') p_affil = super()._convert_generic(inJson={'producerAffiliation': 'University of Pennsylvania'}, dryField='producerName', dvField='producerName') p_url = super()._convert_generic(inJson={'producerURL': 'https://www.ldc.upenn.edu/'}, dryField='producerURL', dvField='producerURL') p_name.update(p_affil) p_name.update(p_url) prod['value'] = [p_name] dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(prod) #Kind of data kind = super()._typeclass('kindOfData', True, 'primitive') kind['value'] = 'Linguistic data' #Series series = super()._typeclass('series', False, 'compound') s_name = super()._convert_generic(inJson={'seriesName': 'LDC'}, dryField='seriesName', dvField='seriesName') s_info = super()._convert_generic(inJson={'seriesInformation': 'Linguistic Data Consortium'}, dryField='seriesInformation', dvField='seriesInformation') s_name.update(s_info) series['value'] = s_name #not a list dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(series) #Data sources series = super()._typeclass('dataSources', True, 'primitive') data_sources = ldc['Data Source(s)'].split(',') data_sources = [x.strip().capitalize() for x in data_sources] series['value'] = data_sources dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(series) #Fix keyword labels that are hardcoded for Dryad #There should be only one keyword block keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion']['metadataBlocks'] ['citation']['fields']) if y.get('typeName') == 'keyword'][0] key_pos = [x for x, y in enumerate(keyword_field[1]['value']) if y['keywordVocabulary']['value'] == 'Dryad'][0] dvjson['datasetVersion']['metadataBlocks']['citation']\\ ['fields'][keyword_field[0]]['value'][key_pos]\\ ['keywordVocabulary']['value'] = 'Linguistic Data Consortium' #The first keyword field is hardcoded in by dryad2dataverse.serializer #So I think it needs to be deleted keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion']['metadataBlocks']['citation']['fields']) if y.get('typeName') == 'otherId'][0] #ibid del dvjson['datasetVersion']['metadataBlocks']['citation']['fields'][keyword_field[0]] #Notes note_index = Ldc.find_block_index(dvjson, 'notesText') if note_index: dvjson['datasetVersion']['metadataBlocks']['citation']\\ ['fields'][note_index]['value'] = self._make_note() else: notes = super()._typeclass('notesText', False, 'primitive') notes['value'] = self._make_note() dvjson['datasetVersion']['metadataBlocks']['citation']['fields'].append(notes) #Deletes unused \"publication\" fields: rewrite to make it a function call. keyword_field = [(x, y) for x, y in enumerate(dvjson['datasetVersion'] ['metadataBlocks']['citation']['fields']) if y.get('typeName') == 'publication'][0] #ibid del dvjson['datasetVersion']['metadataBlocks']['citation']['fields'][keyword_field[0]] #And now the licence: dvjson['datasetVersion']['license'] = LIC_NAME dvjson['datasetVersion']['termsOfUse'] = LICENCE return dvjson","title":"make_dv_json"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.make_ldc_json","text":"Returns a dict with keys created from an LDC catalogue web page. Source code in src/dataverse_utils/ldc.py def make_ldc_json(self): ''' Returns a dict with keys created from an LDC catalogue web page. ''' if not self.ldcHtml: self.fetch_record() soup = bs(self.ldcHtml, 'html.parser') #Should data just look in the *first* table? Specifically tbody? #Is it always the first? I assume yes. tbody = soup.find('tbody')#new data = [x.text.strip() for x in tbody.find_all('td')]#new #data = [x.text.strip() for x in soup.find_all('td')]#original LDC_dict = {data[x][:data[x].find('\\n')].strip(): data[x+1].strip() for x in range(0, len(data), 2)} #Related Works appears to have an extra 'Hide' at the end if LDC_dict.get('Related Works:'): LDC_dict['Related Works'] = (x.strip() for x in LDC_dict['Related Works:'].split('\\n')) del LDC_dict['Related Works:'] #remove the renamed key LDC_dict['Linguistic Data Consortium'] = LDC_dict['LDC Catalog No.'] del LDC_dict['LDC Catalog No.']#This key must be renamed for consistency LDC_dict['Author(s)'] = [x.strip() for x in LDC_dict['Author(s)'].split(',')] #Other metadata probably has HTML in it, so we keep as much as possible other_meta = soup.find_all('div') alldesc = [x for x in other_meta if x.attrs.get('itemprop') == 'description'] #sometimes they format pages oddly and we can use this for a #quick and dirty fix self.__fixdesc = copy.deepcopy(alldesc) #sections use h3, so split on these #24 Jan 23 Apparently, this is all done manually so some of them sometime use h4. #Because reasons. #was: #alldesc = str(alldesc).split('<h3>') #is now alldesc = str(alldesc).replace('h4>', 'h3>').split('<h3>') for i in range(1, len(alldesc)): alldesc[i] = '<h3>' + alldesc[i] #first one is not actually useful, so discard it alldesc.pop(0) #So far, so good. At this point the relative links need fixing #and tables need to be converted to pre. for desc in alldesc: #It's already strings; replace relative links first desc = desc.replace('../../../', 'https://catalog.ldc.upenn.edu/') subsoup = bs(desc, 'html.parser') key = subsoup.h3.text.strip() #don't need the h3 tags anymore subsoup.find('h3').extract() # Convert tables to <pre> for tab in subsoup.find_all('table'): content = str(tab) #convert to markdown content = markdownify.markdownify(content) tab.name = 'pre' tab.string = content #There is not much documentation on the #difference between tab.string and tab.content #That was relatively easy LDC_dict[key] = str(subsoup) LDC_dict['Introduction'] = LDC_dict.get('Introduction', self.__no_intro()) #LDC puts http in front of their DOI identifier if LDC_dict.get('DOI'): LDC_dict['DOI'] = LDC_dict['DOI'].strip('https://doi.org/') return LDC_dict","title":"make_ldc_json"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.name_parser","text":"Returns lastName/firstName JSON snippet from a name Parameters: name ( str ) \u2013 A name Notes Can\u2019t be 100% accurate, because names can be split in many ways. However, as they say, 80% is good enough. Source code in src/dataverse_utils/ldc.py @staticmethod def name_parser(name): ''' Returns lastName/firstName JSON snippet from a name Parameters ---------- name : str A name Notes ----- Can't be 100% accurate, because names can be split in many ways. However, as they say, 80% is good enough. ''' names = name.split(' ') return {'lastName': names[-1], 'firstName': ' '.join(names[:-1]), 'affiliation':''}","title":"name_parser"},{"location":"api_ref/#dataverse_utils.ldc.Ldc.upload_metadata","text":"Uploads metadata to dataverse. Returns json from connection attempt. Parameters: **kwargs ( dict , default: {} ) \u2013 Parameters url ( str ) \u2013 base url to Dataverse installation key ( str ) \u2013 api key dv ( str ) \u2013 Dataverse to which it is being uploaded Source code in src/dataverse_utils/ldc.py def upload_metadata(self, **kwargs) -> dict: ''' Uploads metadata to dataverse. Returns json from connection attempt. Parameters ---------- **kwargs : dict Parameters Other parameters ---------------- url : str base url to Dataverse installation key : str api key dv : str Dataverse to which it is being uploaded ''' url = kwargs['url'].strip('/') key = kwargs['key'] dv = kwargs['dv'] json = kwargs.get('json', self.dvJson) headers = {'X-Dataverse-key':key} headers.update(UAHEADER) try: upload = self.session.post(f'{url}/api/dataverses/{dv}/datasets', headers=headers, json=json) upload.raise_for_status() return upload.json() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError): print(upload.text) raise","title":"upload_metadata"},{"location":"api_ref/#dataverse_utils.collections","text":"Utilities for recursively analysing a Dataverse collection.","title":"collections"},{"location":"api_ref/#dataverse_utils.collections.DvCollection","text":"Metadata for an entire dataverse collection, recursively. Source code in src/dataverse_utils/collections.py class DvCollection: ''' Metadata for an *entire* dataverse collection, recursively. ''' #pylint: disable=too-many-instance-attributes def __init__(self, url:str, coll:str, key=None, **kwargs): ''' All you need to start recursively crawling. Parameters ---------- coll : str short collection name or id number url : str base URL of Dataverse collection. eg: https://borealisdata.ca borealisdata.ca key : str API key (optional, only use if you want to see hidden material) **kwargs: dict Other parameters Other parameters ---------------- timeout : int retry timeout in seconds ''' self.coll = coll self.url = self.__clean_url(url) self.headers = None self.__key = key if self.__key: self.headers = {'X-Dataverse-key': self.__key} self.headers.update(UAHEADER) else: self.headers = UAHEADER.copy() if not kwargs.get('retry'): self.retry_strategy = RETRY else: self.retry_strategy = kwargs['retry'] self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=self.retry_strategy)) self.collections = None self.studies = None def __clean_url(self, badurl:str): ''' Sanitize URL, return properly formatted HTTP string. Parameters ---------- badurl: str URL string ''' clean = badurl.strip().strip('/') if not clean.startswith('https://'): clean = f'https://{clean}' return clean def __get_shortname(self, dvid): ''' Get collection short name. ''' shortname = self.session.get(f'{self.url}/api/dataverses/{dvid}', headers=self.headers) shortname.raise_for_status() return shortname.json()['data']['alias'] def get_collections(self, coll:str=None, output=None, **kwargs)->list:#pylint: disable=unused-argument ''' Get a [recursive] listing of all dataverses in a collection. Parameters ---------- coll : str, optional, default=None Collection short name or id output : list, optional, default=[] output list to append to **kwargs : dict Other keyword arguments ''' if not output: output = [] if not coll: coll = self.coll x = self.session.get(f'{self.url}/api/dataverses/{coll}/contents', headers=self.headers) data = x.json().get('data') #--- #Because it's possible that permissions errors can cause API read errors, #we have this insane way of checking errors. #I have no idea what kind of errors would be raised, so it has #a bare except, which is bad. But what can you do? dvs =[] for _ in data: if _['type'] == 'dataverse': try: out=self.__get_shortname(_['id']) dvs.append((_['title'], out)) except Exception as e: obscure_error = f''' An error has occured where a collection can be identified by ID but its name cannot be determined. This is (normally) caused by a configuration error where administrator permissions are not correctly inherited by the child collection. Please check with the system administrator to determine any exact issues. Problematic collection id number: {_.get(\"id\", \"not available\")}''' print(50*'-') print(textwrap.dedent(obscure_error)) print(e) LOGGER.error(textwrap.fill(textwrap.dedent(obscure_error).strip())) traceback.print_exc() print(50*'-') raise e #--- if not dvs: dvs = [] output.extend(dvs) for dv in dvs: LOGGER.debug('%s/api/dataverses/%s/contents', self.url, dv[1]) LOGGER.debug('recursive') self.get_collections(dv[1], output) self.collections = output return output def get_studies(self, root:str=None): ''' return [(pid, title)..(pid_n, title_n)] of a collection. Parameters ---------- root : str Short name or id of *top* level of tree. Default self.coll ''' all_studies = [] if not root: root=self.coll all_studies = self.get_collection_listing(root) #collections = self.get_collections(root, self.url) collections = self.get_collections(root) for collection in collections: all_studies.extend(self.get_collection_listing(collection[1])) self.studies = all_studies return all_studies def get_collection_listing(self, coll_id): ''' Return a listing of studies in a collection, with pid. Parameters ---------- coll_id : str Short name or id of a dataverse collection ''' cl = self.session.get(f'{self.url}/api/dataverses/{coll_id}/contents', headers=self.headers) cl.raise_for_status() pids = [f\"{z['protocol']}:{z['authority']}/{z['identifier']}\" for z in cl.json()['data'] if z['type'] == 'dataset'] out = [(self.get_study_info(pid), pid) for pid in pids] for _ in out: _[0].update({'pid': _[1]}) return [x[0] for x in out] def get_study_info(self, pid): ''' Returns a StudyMetadata object with complete metadata for a study. Parameters ---------- pid : str Persistent ID of a Dataverse study ''' meta = self.session.get(f'{self.url}/api/datasets/:persistentId', params={'persistentId': pid}, headers=self.headers) meta.raise_for_status() LOGGER.debug(pid) return StudyMetadata(study_meta=meta.json(), key=self.__key, url=self.url)","title":"DvCollection"},{"location":"api_ref/#dataverse_utils.collections.DvCollection.__clean_url","text":"Sanitize URL, return properly formatted HTTP string. Parameters: badurl ( str ) \u2013 URL string Source code in src/dataverse_utils/collections.py def __clean_url(self, badurl:str): ''' Sanitize URL, return properly formatted HTTP string. Parameters ---------- badurl: str URL string ''' clean = badurl.strip().strip('/') if not clean.startswith('https://'): clean = f'https://{clean}' return clean","title":"__clean_url"},{"location":"api_ref/#dataverse_utils.collections.DvCollection.__get_shortname","text":"Get collection short name. Source code in src/dataverse_utils/collections.py def __get_shortname(self, dvid): ''' Get collection short name. ''' shortname = self.session.get(f'{self.url}/api/dataverses/{dvid}', headers=self.headers) shortname.raise_for_status() return shortname.json()['data']['alias']","title":"__get_shortname"},{"location":"api_ref/#dataverse_utils.collections.DvCollection.__init__","text":"All you need to start recursively crawling. Parameters: coll ( str ) \u2013 short collection name or id number url ( str ) \u2013 base URL of Dataverse collection. eg: https://borealisdata.ca borealisdata.ca key ( str , default: None ) \u2013 API key (optional, only use if you want to see hidden material) **kwargs \u2013 Other parameters timeout ( int ) \u2013 retry timeout in seconds Source code in src/dataverse_utils/collections.py def __init__(self, url:str, coll:str, key=None, **kwargs): ''' All you need to start recursively crawling. Parameters ---------- coll : str short collection name or id number url : str base URL of Dataverse collection. eg: https://borealisdata.ca borealisdata.ca key : str API key (optional, only use if you want to see hidden material) **kwargs: dict Other parameters Other parameters ---------------- timeout : int retry timeout in seconds ''' self.coll = coll self.url = self.__clean_url(url) self.headers = None self.__key = key if self.__key: self.headers = {'X-Dataverse-key': self.__key} self.headers.update(UAHEADER) else: self.headers = UAHEADER.copy() if not kwargs.get('retry'): self.retry_strategy = RETRY else: self.retry_strategy = kwargs['retry'] self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=self.retry_strategy)) self.collections = None self.studies = None","title":"__init__"},{"location":"api_ref/#dataverse_utils.collections.DvCollection.get_collection_listing","text":"Return a listing of studies in a collection, with pid. Parameters: coll_id ( str ) \u2013 Short name or id of a dataverse collection Source code in src/dataverse_utils/collections.py def get_collection_listing(self, coll_id): ''' Return a listing of studies in a collection, with pid. Parameters ---------- coll_id : str Short name or id of a dataverse collection ''' cl = self.session.get(f'{self.url}/api/dataverses/{coll_id}/contents', headers=self.headers) cl.raise_for_status() pids = [f\"{z['protocol']}:{z['authority']}/{z['identifier']}\" for z in cl.json()['data'] if z['type'] == 'dataset'] out = [(self.get_study_info(pid), pid) for pid in pids] for _ in out: _[0].update({'pid': _[1]}) return [x[0] for x in out]","title":"get_collection_listing"},{"location":"api_ref/#dataverse_utils.collections.DvCollection.get_collections","text":"Get a [recursive] listing of all dataverses in a collection. Parameters: coll ( str , default: None ) \u2013 Collection short name or id output ( list , default: [] ) \u2013 output list to append to **kwargs ( dict , default: {} ) \u2013 Other keyword arguments Source code in src/dataverse_utils/collections.py def get_collections(self, coll:str=None, output=None, **kwargs)->list:#pylint: disable=unused-argument ''' Get a [recursive] listing of all dataverses in a collection. Parameters ---------- coll : str, optional, default=None Collection short name or id output : list, optional, default=[] output list to append to **kwargs : dict Other keyword arguments ''' if not output: output = [] if not coll: coll = self.coll x = self.session.get(f'{self.url}/api/dataverses/{coll}/contents', headers=self.headers) data = x.json().get('data') #--- #Because it's possible that permissions errors can cause API read errors, #we have this insane way of checking errors. #I have no idea what kind of errors would be raised, so it has #a bare except, which is bad. But what can you do? dvs =[] for _ in data: if _['type'] == 'dataverse': try: out=self.__get_shortname(_['id']) dvs.append((_['title'], out)) except Exception as e: obscure_error = f''' An error has occured where a collection can be identified by ID but its name cannot be determined. This is (normally) caused by a configuration error where administrator permissions are not correctly inherited by the child collection. Please check with the system administrator to determine any exact issues. Problematic collection id number: {_.get(\"id\", \"not available\")}''' print(50*'-') print(textwrap.dedent(obscure_error)) print(e) LOGGER.error(textwrap.fill(textwrap.dedent(obscure_error).strip())) traceback.print_exc() print(50*'-') raise e #--- if not dvs: dvs = [] output.extend(dvs) for dv in dvs: LOGGER.debug('%s/api/dataverses/%s/contents', self.url, dv[1]) LOGGER.debug('recursive') self.get_collections(dv[1], output) self.collections = output return output","title":"get_collections"},{"location":"api_ref/#dataverse_utils.collections.DvCollection.get_studies","text":"return [(pid, title)..(pid_n, title_n)] of a collection. Parameters: root ( str , default: None ) \u2013 Short name or id of top level of tree. Default self.coll Source code in src/dataverse_utils/collections.py def get_studies(self, root:str=None): ''' return [(pid, title)..(pid_n, title_n)] of a collection. Parameters ---------- root : str Short name or id of *top* level of tree. Default self.coll ''' all_studies = [] if not root: root=self.coll all_studies = self.get_collection_listing(root) #collections = self.get_collections(root, self.url) collections = self.get_collections(root) for collection in collections: all_studies.extend(self.get_collection_listing(collection[1])) self.studies = all_studies return all_studies","title":"get_studies"},{"location":"api_ref/#dataverse_utils.collections.DvCollection.get_study_info","text":"Returns a StudyMetadata object with complete metadata for a study. Parameters: pid ( str ) \u2013 Persistent ID of a Dataverse study Source code in src/dataverse_utils/collections.py def get_study_info(self, pid): ''' Returns a StudyMetadata object with complete metadata for a study. Parameters ---------- pid : str Persistent ID of a Dataverse study ''' meta = self.session.get(f'{self.url}/api/datasets/:persistentId', params={'persistentId': pid}, headers=self.headers) meta.raise_for_status() LOGGER.debug(pid) return StudyMetadata(study_meta=meta.json(), key=self.__key, url=self.url)","title":"get_study_info"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis","text":"Bases: dict Download and analyze a file from a dataverse installation and produce useful metadata. Source code in src/dataverse_utils/collections.py class FileAnalysis(dict): ''' Download and analyze a file from a dataverse installation and produce useful metadata. ''' def __init__(self, **kwargs): ''' Intialize the object. Parameters ---------- **kwargs : dict Keyword arguments Other parameters ---------------- local : str Path to local file url : str URL of Dataverse instance key : str API key for downloading fid : int Integer file id pid : str Persistent ID of file filename : str File name (original) filesize_bytes : int File size in bytes Notes ----- Either `local` must be supplied, or `url`, `key` and at least one of `fid` or `pid` must be supplied ''' #self.url = self.__clean_url(url) self.headers = UAHEADER.copy() self.kwargs = kwargs if self.kwargs.get('key'): self.headers.update({'X-Dataverse-key':self.kwargs['key']}) self.local = None if not self.__sufficient: err = ('Insufficient required arguments. ' 'Include (url, key, ' '(pid or id)) or (local) keyword parameters.') raise TypeError(err) self.tempfile = None self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=RETRY)) self.checkable = {'.sav': self.stat_file_metadata, '.sas7bdat': self.stat_file_metadata, '.dta': self.stat_file_metadata, '.csv': self.generic_metadata, '.tsv': self.generic_metadata, '.rdata': self.generic_metadata, '.rda': self.generic_metadata} self.filename = None #get it later self.enhance() def __del__(self): ''' Cleanup old temporary files on object deletion. ''' self.session.close() del self.tempfile def __sufficient(self)->bool: ''' Checks if sufficient information is supplied for intialization, with local files taking preference over remote. ''' if self.kwargs.get('local'): return True if (self.kwargs['url'] and self.kwargs['key'] and (self.kwargs.get('pid') or self.kwargs.get('id'))): return True return False def __clean_url(self, badurl:str)->str: ''' Sanitize URL. Ensures ssl and no trailing slash. Parameters ---------- badurl: str URL ''' clean = badurl.strip().strip('/') if not clean.startswith('https://'): clean = f'https://{clean}' return clean def __get_filename(self, head:dict)->typing.Union[str, None]: ''' Determines whether or not this is a file that should be downloaded for further checking. Parameters ---------- head : dict Header from GET request Returns ------- True if extended metadata can be obtained ''' fname = head.get('content-type') if fname: if 'name=' in fname: start = head['content-type'].find('name=')+5 end = head['content-type'].find(';', start) if end != -1: fname = head['content-type'][start:end].strip('\"') else: fname = head['content-type'][start:].strip('\"') fname = self.kwargs.get('filename', fname) return fname @property def __whichfile(self): ''' Returns the location of the path being analyzed. ''' return self.tempfile.name if self.tempfile else self.local def __check(self): ''' Determines if this is one of the filetypes which supports extra metadata. ''' if pathlib.Path(self.filename).suffix.lower() in self.checkable: return True return False def download(self, block_size:int=1024, force=False, local=None)-> None: ''' Download the file to a temporary location for analysis. -------------------- block_size : int Streaming block size force : bool Download even if not a file that is checkable local : str Path to local file ''' # pylint: disable=consider-using-with self.tempfile = tempfile.NamedTemporaryFile(delete=True, delete_on_close=False) if local: self.local = local self.filename = local self.tempfile.close() del self.tempfile #to erase it self.tempfile = None return start = datetime.datetime.now() params = {'format':'original'} url = self.__clean_url(self.kwargs['url']) if self.kwargs.get('pid'): params.update({'persistentId':self.kwargs['pid']}) data = self.session.get(f'{url}/api/access/datafile/:persistentId', headers=self.headers, params=params, stream=True) else: data = self.session.get(f'{url}/api/access/datafile/{self.kwargs[\"id\"]}', headers=self.headers, params=params, stream=True) data.raise_for_status() finish = datetime.datetime.now() self.filename = self.__get_filename(data.headers) LOGGER.info('Downloaded header for %s. Elapsed time: %s', self.filename, finish-start) if self.__check() or force: filesize = self.kwargs.get('filesize_bytes', data.headers.get('content-length', 9e9)) filesize = int(filesize) # comes out as string from header with tqdm.tqdm(total=filesize, unit='B', unit_scale=True, desc=self.filename) as t: for _ in data.iter_content(block_size): self.tempfile.file.write(_) t.update(len(_)) self.tempfile.close() def enhance(self): ''' Convenience function for downloading and creating extra metadata, ie, \"enhancing\" the metadata. Use this instead of going through the steps manually. ''' self.download(local=self.kwargs.get('local')) do_it = pathlib.Path(self.filename).suffix.lower() if do_it in self.checkable: self.checkable[do_it](ext=do_it) def stat_file_metadata(self, ext:str)->dict: ''' Produces metadata from SAS, SPSS and Stata files. Parameters ---------- ext : str File extension of statistical package file. Include the '.'. Eg. '.sav' ''' matcher = {'.sav': pyreadstat.read_sav, '.dta': pyreadstat.read_dta, '.sas7bdat': pyreadstat.read_sas7bdat} if not self.filename or ext not in matcher: return #whichfile = self.tempfile.name if self.tempfile else self.local statdata, meta = matcher[ext](self.__whichfile) outmeta = {} outmeta['variables'] = {_:{} for _ in meta.column_names_to_labels} for k, v in meta.column_names_to_labels.items(): outmeta['variables'][k]['Variable label'] = v for k, v in meta.original_variable_types.items(): outmeta['variables'][k]['Variable type'] = v for k, v in meta.variable_to_label.items(): outmeta['variables'][k]['Value labels'] = meta.value_labels.get(v, '') outmeta['encoding'] = meta.file_encoding for dt in statdata.columns: desc = {k:str(v) for k, v in dict(statdata[dt].describe()).items()} outmeta['variables'][dt].update(desc) self.update(outmeta) return def generic_metadata(self, ext)->None: ''' Make metadata for a [ct]sv file and RData. Updates self. Parameters ---------- ext : str extension ('.csv' or '.tsv') ''' #if ext == '.tsv': # data = pd.read_csv(self.__whichfile, sep='\\t') #else: # data = pd.read_csv(self.__whichfile) lookuptable ={'.tsv': {'func': pd.read_csv, 'kwargs' : {'sep':'\\t'}}, '.csv': {'func' : pd.read_csv}, '.rda': {'func' : pyreadr.read_r}, '.rdata':{'func' : pyreadr.read_r}} data = lookuptable[ext]['func'](self.__whichfile, **lookuptable[ext].get('kwargs', {})) if ext in ['.rda', '.rdata']: data = data[None] #why pyreadr why outmeta = {} outmeta['variables'] = {_:{} for _ in data.columns} for dt in data.columns: outmeta['variables'][dt]['Variable type'] = str(data[dt].dtype) # Make something from nothing desc = {k:str(v) for k, v in dict(data[dt].describe()).items()} outmeta['variables'][dt].update(desc) self.update(outmeta) @property def md(self): ''' Create Markdown text out of a FileAnalysis object. ''' out = io.StringIO() indent = '\\u00A0' # &nbsp; if not self.get('variables'): return None for k, v in self.items(): if k != 'variables': out.write(f'**{k.capitalize()}** : {v} \\n') for k, v in self.get('variables',{}).items(): out.write(f\"**{k}**: {v.get('Variable label', 'Description N/A')} \\n\") for kk, vv, in v.items(): if kk == 'Variable label': continue if not isinstance(vv, dict): out.write(f'**{kk.capitalize()}**: {vv} \\n') else: out.write(f'**{kk.capitalize()}**: \\n') for kkk, vvv in vv.items(): #this one only originally out.write(f'{4*indent}{kkk}: {vvv} \\n') out.write('\\n') out.seek(0) return out.read()","title":"FileAnalysis"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.__whichfile","text":"Returns the location of the path being analyzed.","title":"__whichfile"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.md","text":"Create Markdown text out of a FileAnalysis object.","title":"md"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.__check","text":"Determines if this is one of the filetypes which supports extra metadata. Source code in src/dataverse_utils/collections.py def __check(self): ''' Determines if this is one of the filetypes which supports extra metadata. ''' if pathlib.Path(self.filename).suffix.lower() in self.checkable: return True return False","title":"__check"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.__clean_url","text":"Sanitize URL. Ensures ssl and no trailing slash. Parameters: badurl ( str ) \u2013 URL Source code in src/dataverse_utils/collections.py def __clean_url(self, badurl:str)->str: ''' Sanitize URL. Ensures ssl and no trailing slash. Parameters ---------- badurl: str URL ''' clean = badurl.strip().strip('/') if not clean.startswith('https://'): clean = f'https://{clean}' return clean","title":"__clean_url"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.__del__","text":"Cleanup old temporary files on object deletion. Source code in src/dataverse_utils/collections.py def __del__(self): ''' Cleanup old temporary files on object deletion. ''' self.session.close() del self.tempfile","title":"__del__"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.__get_filename","text":"Determines whether or not this is a file that should be downloaded for further checking. Parameters: head ( dict ) \u2013 Header from GET request Returns: True if extended metadata can be obtained \u2013 Source code in src/dataverse_utils/collections.py def __get_filename(self, head:dict)->typing.Union[str, None]: ''' Determines whether or not this is a file that should be downloaded for further checking. Parameters ---------- head : dict Header from GET request Returns ------- True if extended metadata can be obtained ''' fname = head.get('content-type') if fname: if 'name=' in fname: start = head['content-type'].find('name=')+5 end = head['content-type'].find(';', start) if end != -1: fname = head['content-type'][start:end].strip('\"') else: fname = head['content-type'][start:].strip('\"') fname = self.kwargs.get('filename', fname) return fname","title":"__get_filename"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.__init__","text":"Intialize the object. Parameters: **kwargs ( dict , default: {} ) \u2013 Keyword arguments local ( str ) \u2013 Path to local file url ( str ) \u2013 URL of Dataverse instance key ( str ) \u2013 API key for downloading fid ( int ) \u2013 Integer file id pid ( str ) \u2013 Persistent ID of file filename ( str ) \u2013 File name (original) filesize_bytes ( int ) \u2013 File size in bytes Notes Either local must be supplied, or url , key and at least one of fid or pid must be supplied Source code in src/dataverse_utils/collections.py def __init__(self, **kwargs): ''' Intialize the object. Parameters ---------- **kwargs : dict Keyword arguments Other parameters ---------------- local : str Path to local file url : str URL of Dataverse instance key : str API key for downloading fid : int Integer file id pid : str Persistent ID of file filename : str File name (original) filesize_bytes : int File size in bytes Notes ----- Either `local` must be supplied, or `url`, `key` and at least one of `fid` or `pid` must be supplied ''' #self.url = self.__clean_url(url) self.headers = UAHEADER.copy() self.kwargs = kwargs if self.kwargs.get('key'): self.headers.update({'X-Dataverse-key':self.kwargs['key']}) self.local = None if not self.__sufficient: err = ('Insufficient required arguments. ' 'Include (url, key, ' '(pid or id)) or (local) keyword parameters.') raise TypeError(err) self.tempfile = None self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=RETRY)) self.checkable = {'.sav': self.stat_file_metadata, '.sas7bdat': self.stat_file_metadata, '.dta': self.stat_file_metadata, '.csv': self.generic_metadata, '.tsv': self.generic_metadata, '.rdata': self.generic_metadata, '.rda': self.generic_metadata} self.filename = None #get it later self.enhance()","title":"__init__"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.__sufficient","text":"Checks if sufficient information is supplied for intialization, with local files taking preference over remote. Source code in src/dataverse_utils/collections.py def __sufficient(self)->bool: ''' Checks if sufficient information is supplied for intialization, with local files taking preference over remote. ''' if self.kwargs.get('local'): return True if (self.kwargs['url'] and self.kwargs['key'] and (self.kwargs.get('pid') or self.kwargs.get('id'))): return True return False","title":"__sufficient"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.download","text":"Download the file to a temporary location for analysis. block_size : int Streaming block size force : bool Download even if not a file that is checkable local : str Path to local file Source code in src/dataverse_utils/collections.py def download(self, block_size:int=1024, force=False, local=None)-> None: ''' Download the file to a temporary location for analysis. -------------------- block_size : int Streaming block size force : bool Download even if not a file that is checkable local : str Path to local file ''' # pylint: disable=consider-using-with self.tempfile = tempfile.NamedTemporaryFile(delete=True, delete_on_close=False) if local: self.local = local self.filename = local self.tempfile.close() del self.tempfile #to erase it self.tempfile = None return start = datetime.datetime.now() params = {'format':'original'} url = self.__clean_url(self.kwargs['url']) if self.kwargs.get('pid'): params.update({'persistentId':self.kwargs['pid']}) data = self.session.get(f'{url}/api/access/datafile/:persistentId', headers=self.headers, params=params, stream=True) else: data = self.session.get(f'{url}/api/access/datafile/{self.kwargs[\"id\"]}', headers=self.headers, params=params, stream=True) data.raise_for_status() finish = datetime.datetime.now() self.filename = self.__get_filename(data.headers) LOGGER.info('Downloaded header for %s. Elapsed time: %s', self.filename, finish-start) if self.__check() or force: filesize = self.kwargs.get('filesize_bytes', data.headers.get('content-length', 9e9)) filesize = int(filesize) # comes out as string from header with tqdm.tqdm(total=filesize, unit='B', unit_scale=True, desc=self.filename) as t: for _ in data.iter_content(block_size): self.tempfile.file.write(_) t.update(len(_)) self.tempfile.close()","title":"download"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.enhance","text":"Convenience function for downloading and creating extra metadata, ie, \u201cenhancing\u201d the metadata. Use this instead of going through the steps manually. Source code in src/dataverse_utils/collections.py def enhance(self): ''' Convenience function for downloading and creating extra metadata, ie, \"enhancing\" the metadata. Use this instead of going through the steps manually. ''' self.download(local=self.kwargs.get('local')) do_it = pathlib.Path(self.filename).suffix.lower() if do_it in self.checkable: self.checkable[do_it](ext=do_it)","title":"enhance"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.generic_metadata","text":"Make metadata for a [ct]sv file and RData. Updates self. Parameters: ext ( str ) \u2013 extension (\u2018.csv\u2019 or \u2018.tsv\u2019) Source code in src/dataverse_utils/collections.py def generic_metadata(self, ext)->None: ''' Make metadata for a [ct]sv file and RData. Updates self. Parameters ---------- ext : str extension ('.csv' or '.tsv') ''' #if ext == '.tsv': # data = pd.read_csv(self.__whichfile, sep='\\t') #else: # data = pd.read_csv(self.__whichfile) lookuptable ={'.tsv': {'func': pd.read_csv, 'kwargs' : {'sep':'\\t'}}, '.csv': {'func' : pd.read_csv}, '.rda': {'func' : pyreadr.read_r}, '.rdata':{'func' : pyreadr.read_r}} data = lookuptable[ext]['func'](self.__whichfile, **lookuptable[ext].get('kwargs', {})) if ext in ['.rda', '.rdata']: data = data[None] #why pyreadr why outmeta = {} outmeta['variables'] = {_:{} for _ in data.columns} for dt in data.columns: outmeta['variables'][dt]['Variable type'] = str(data[dt].dtype) # Make something from nothing desc = {k:str(v) for k, v in dict(data[dt].describe()).items()} outmeta['variables'][dt].update(desc) self.update(outmeta)","title":"generic_metadata"},{"location":"api_ref/#dataverse_utils.collections.FileAnalysis.stat_file_metadata","text":"Produces metadata from SAS, SPSS and Stata files. Parameters: ext ( str ) \u2013 File extension of statistical package file. Include the \u2018.\u2019. Eg. \u2018.sav\u2019 Source code in src/dataverse_utils/collections.py def stat_file_metadata(self, ext:str)->dict: ''' Produces metadata from SAS, SPSS and Stata files. Parameters ---------- ext : str File extension of statistical package file. Include the '.'. Eg. '.sav' ''' matcher = {'.sav': pyreadstat.read_sav, '.dta': pyreadstat.read_dta, '.sas7bdat': pyreadstat.read_sas7bdat} if not self.filename or ext not in matcher: return #whichfile = self.tempfile.name if self.tempfile else self.local statdata, meta = matcher[ext](self.__whichfile) outmeta = {} outmeta['variables'] = {_:{} for _ in meta.column_names_to_labels} for k, v in meta.column_names_to_labels.items(): outmeta['variables'][k]['Variable label'] = v for k, v in meta.original_variable_types.items(): outmeta['variables'][k]['Variable type'] = v for k, v in meta.variable_to_label.items(): outmeta['variables'][k]['Value labels'] = meta.value_labels.get(v, '') outmeta['encoding'] = meta.file_encoding for dt in statdata.columns: desc = {k:str(v) for k, v in dict(statdata[dt].describe()).items()} outmeta['variables'][dt].update(desc) self.update(outmeta) return","title":"stat_file_metadata"},{"location":"api_ref/#dataverse_utils.collections.MetadataError","text":"Bases: Exception MetadataError Source code in src/dataverse_utils/collections.py class MetadataError(Exception): ''' MetadataError '''","title":"MetadataError"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator","text":"Make formatted README documents out of a StudyMetadata object. Source code in src/dataverse_utils/collections.py class ReadmeCreator: ''' Make formatted README documents out of a StudyMetadata object. ''' def __init__(self, study_metadata_obj: StudyMetadata, **kwargs): ''' Send in StudyMetadata dict to create a nicely formatted README document Parameters ---------- study_metadata_obj : StudyMetadata A study metadata object **kwargs : dict Keyword arguments Other parameters ---------------- url : str The base URL for a Dataverse instance pid : typing.Union[str, int] The persistent identifier of a file or a file id key : str A valid API key for performing operations on Dataverse studies local : str Path to the top level directory which holds study files. If present, the Readme creator will try to create extended data from local files instead of downloading. Notes ----- Either `local` must be supplied, or `url`, `pid` and `key` must supplied ''' self.meta = study_metadata_obj self.kwargs = kwargs warnings.filterwarnings('ignore', category=bs4.MarkupResemblesLocatorWarning) #These values are the first part of the keys that need #concatenation to make them more legible. self.concat = ['author', 'datasetContact','otherId', 'keyword', 'topic', 'publication', 'producer', 'production', 'distributor', 'series', 'software', 'dsDescription', 'grant', 'contributor'] def __html_to_md(self, inval:str)->str: ''' Convert any HTML to markdown, or as much as possible. Parameters ---------- inval : str HTML string to convert ''' if isinstance(inval, str): #markdownify kwargs are here: #https://github.com/matthewwithanm/python-markdownify return markdownify.markdownify(inval) return str(inval) def make_md_heads(self, inkey:str)->str: ''' Make markdown H2 headings for selected sections, currently title, description, licence and terms of use. Parameters ---------- inkey : str Section heading ''' section_heads = {'Title':'## ', 'Description':'**Description**\\n\\n', 'Licence': '### Licence\\n\\n', 'Terms of Use': '### Terms of Use\\n\\n'} if inkey in section_heads: return section_heads[inkey] multi = [self.rename_field(_) for _ in self.concat] if inkey in multi: if inkey not in ['Series', 'Software', 'Production']: return f'{inkey}(s): \\n' return f'{inkey}: \\n' return f'{inkey}: ' @property def file_metadata_md(self)->str: ''' Produce pretty markdown for file metadata. Outputs markdown text string. ''' fmeta = [] for fil in self.meta.files: fileout = {} fileout['File'] = fil['filename'] for k, v in fil.items(): fileout[k.capitalize().replace('_',' ').replace('Pid', 'Persistent Identifier')] = v fileout['Message digest'] = f'{fileout[\"Chk type\"]}: {fileout[\"Chk digest\"]}' for rem in ['Chk type', 'Chk digest', 'Id', 'Has tab file', 'Study pid', 'File label', 'Filename']: del fileout[rem] #not everyone has a pid for the file if not fileout.get('Persistent Identifier'): del fileout['Persistent Identifier'] # Should I only have remote material here? What about # local files? if self.kwargs.get('local'): #TODO, if local fpath = pathlib.Path(self.kwargs['local']) #And from here you have to walk the tree to get the file in fil['filename'] #One day I will do this elif self.meta.kwargs.get('url'): # Should this be optional? ie, # and self.kwargs.get('download') or summat d_dict = FileAnalysis(url=self.meta.kwargs['url'], key=self.meta.kwargs.get('key'), **fil).md #I test here #d_dict = FileAnalysis(local='tmp/eics_2023_pumf_v1.sav').md if d_dict: fileout['Data Dictionary'] = d_dict fmeta.append(fileout) #----- original #outtmp = [] #for li in fmeta: # outtmp.append(' \\n'.join(f'{k}: {v}' for k, v in li.items())) #return '\\n\\n'.join(outtmp) #------- outtmp = [] for li in fmeta: o2 = [] for k, v in li.items(): if k == 'Data Dictionary': o2.append(f'### {k} for {li[\"File\"]} \\n{v}') else: o2.append(f'{k}: {v}') outtmp.append(' \\n'.join(o2)) outtmp = '\\n\\n'.join(outtmp) return outtmp @property def readme_md(self)->str: ''' Generate a Markdown text string (ie, the entire README) for entire an entire StudyMetadata object. ''' metatmp = self.meta.copy() neworder = self.reorder_fields(metatmp) addme = self.concatenator(metatmp) metatmp.update(addme) out = {_:None for _ in neworder} # A new dictionary with the correct order for k, v in metatmp.items(): out[k]=v #Now remove keys that should be gone for rem in self.concat: out = {k:v for k,v in out.items() if not (k.startswith(rem) and len(k) > len(rem))} fout = {self.rename_field(k): self.__fix_relation_type(self.__html_to_md(v)) for k, v in out.items()} #cludgy geometry hack is best hack if self.bbox(): fout.update(self.bbox()) delme = [_ for _ in fout if _.endswith('tude')] for _ in delme: del fout[_] outstr = '\\n\\n'.join(f'{self.make_md_heads(k)}{v}' for k, v in fout.items()) outstr += '\\n\\n## File information\\n\\n' outstr += self.file_metadata_md return outstr def bbox(self)->dict: ''' Produce sane bounding boxes from Dataverse metadata. Note that older versions of Dataverse used North and South *longitude*. Outputs a dict with bounding boxes contcatenated into a single line with each coordinate suffixed by its direction (eg: '42.97 E'), with coordinates separated by commas and multiple boxes separated by semi-colons. ''' #Yes, northLongitude, etc. Blame Harvard. bbox_order =['westLongitude', 'southLongitude', 'southLatitude', 'eastLongitude', 'northLongitude', 'northLatitude'] geog_me = {_: self.meta[_].split(';') for _ in bbox_order if self.meta.get(_)}# Checking for existence causes problems if not geog_me: #Sometimes there is no bounding box return {} bbox = {k: [f'{v} {k[0].capitalize()}'.strip() for v in geog_me[k]] for k in bbox_order if geog_me.get(k)} boxes = self.max_zip(*bbox.values()) boxes = [', '.join(_) for _ in boxes] boxes = [f'({_})' for _ in boxes] return {'Bounding box(es)': '; '.join(boxes)} def __fix_relation_type(self, badstr:str)->str: ''' For some reason, Dataverse puts camelCase values in the 'values' field for publication relation. This will make it more readable. Parameters ---------- badstr : str Input string; problematic values will be fixed, all others returned as-is. ''' fixthese = ['IsCitedBy', 'IsSupplementTo', 'IsSupplementedBy', 'IsReferencedBy'] for val in fixthese: badstr=badstr.replace(val, self.rename_field(val)) return badstr def reorder_fields(self, indict:dict)->list: ''' Create a list which contains a list of keys in the right (corrected) order. This ensures that concatenated fields are inserted into the right place and not at the end of the dictionary, keeping the structure of Dataverse metadata intact while concatenating values that need combining. Parameters ---------- indict : dict Metadata dictionary ''' fieldlist = list(indict) for val in self.concat: pts = [n for n, x in enumerate(fieldlist) if x.startswith(val)] if pts: ins_point = min(pts) fieldlist.insert(ins_point, val) #Geography fields are a special case yay. #westLongitude is the fist one if 'westLongitude' in fieldlist: ins_here = fieldlist.index('westLongitude') fieldlist.insert(ins_here, 'Bounding box(es)') return fieldlist def rename_field(self, instr:str)->str: ''' Split and capitalize camelCase fields as required. eg: keywordValue -> Keyword Value eg: termsOfUse -> Terms of Use Parameters ---------- instr : str Camel case tring to split into words and capitalize. ''' noncap = ['A', 'Of', 'The'] wordsp = ''.join(map(lambda x: x if x not in string.ascii_uppercase else f' {x}', list(instr))) wordsp = wordsp.split(' ') #wordsp[0] = wordsp[0].capitalize() #wordsp = ' '.join(map(lambda x: x if x not in noncap else x.lower(), wordsp)) wordsp = list(map(lambda x: x if x not in noncap else x.lower(), wordsp)) wordsp[0] = wordsp[0].capitalize() wordsp = ' '.join(wordsp) #because they can't even use camelCaseConsistently #Also pluralization of concatenated fields fixthese ={'U R L': 'URL', 'U R I': 'URI', 'I D': 'ID', 'Ds': '', 'Country':'Country(ies)', 'State':'State(s)', 'City':'City(ies)', 'Geographic Unit':'Geographic unit(s)'} for k, v in fixthese.items(): wordsp = wordsp.replace(k, v) return wordsp.strip() def concatenator(self, meta:dict)->dict: ''' Produce a concatenated dictionary with the key being just the prefix. For fields like author[whatever], etc, where there are multiple *components* of similar metadata held in completely separated fields. Parameters ---------- meta : dict Input metadata ''' #The keys are the first part of the fields that need concatenation concat = {_:[] for _ in self.concat} for k, v in meta.items(): for fk in concat: if k.startswith(fk): if v: if concat[fk]: concat[fk].append(v.split(';')) else: concat[fk] = [v.split(';')] outdict = {} for ke, va in concat.items(): if va: interim = self.max_zip(*va) interim = [' - '.join([y.strip() for y in _ if y]) for _ in interim ] #interim = '; '.join(interim) # Should it be newline? #interim = ' \\n'.join(interim) # Should it be newline? interim= '<br/>'.join(interim)# Markdownify strips internal spaces #if ke.startswith('keyw'): outdict[ke] = interim return outdict def max_zip(self, *args): ''' Like built-in zip, only uses the *maximum* length and appends None if not found instead of stopping at the shortest iterable. Parameters ---------- *args : iterable Any iterable ''' length = max(map(len, args)) outlist=[] for n in range(length): vals = [] for arg in args: try: vals.append(arg[n]) except IndexError: vals.append(None) outlist.append(vals) return outlist def write_pdf(self, dest:str)->None: ''' Make the PDF of a README and save it to a file. Parameters ---------- dest : str Destination of file, optionally including path. eg: /Users/foo/study/README.pdf or ~/tmp/README_I_AM_METADATA.pdf ''' dest = pathlib.Path(dest).expanduser().absolute() output = markdown_pdf.MarkdownPdf(toc_level=1) content = markdown_pdf.Section(self.readme_md, toc=False) output.add_section(content) output.save(dest) def write_md(self, dest:str)->None: ''' Write Markdown text of the complete documentation to a file. Parameters ---------- dest : str Destination of file, optionally including path. eg: /Users/foo/study/README.md or ~/tmp/README_I_AM_METADATA.md ''' dest = pathlib.Path(dest).expanduser().absolute() with open(file=dest, mode='w', encoding='utf=8') as f: f.write(self.readme_md)","title":"ReadmeCreator"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.file_metadata_md","text":"Produce pretty markdown for file metadata. Outputs markdown text string.","title":"file_metadata_md"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.readme_md","text":"Generate a Markdown text string (ie, the entire README) for entire an entire StudyMetadata object.","title":"readme_md"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.__fix_relation_type","text":"For some reason, Dataverse puts camelCase values in the \u2018values\u2019 field for publication relation. This will make it more readable. Parameters: badstr ( str ) \u2013 Input string; problematic values will be fixed, all others returned as-is. Source code in src/dataverse_utils/collections.py def __fix_relation_type(self, badstr:str)->str: ''' For some reason, Dataverse puts camelCase values in the 'values' field for publication relation. This will make it more readable. Parameters ---------- badstr : str Input string; problematic values will be fixed, all others returned as-is. ''' fixthese = ['IsCitedBy', 'IsSupplementTo', 'IsSupplementedBy', 'IsReferencedBy'] for val in fixthese: badstr=badstr.replace(val, self.rename_field(val)) return badstr","title":"__fix_relation_type"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.__html_to_md","text":"Convert any HTML to markdown, or as much as possible. Parameters: inval ( str ) \u2013 HTML string to convert Source code in src/dataverse_utils/collections.py def __html_to_md(self, inval:str)->str: ''' Convert any HTML to markdown, or as much as possible. Parameters ---------- inval : str HTML string to convert ''' if isinstance(inval, str): #markdownify kwargs are here: #https://github.com/matthewwithanm/python-markdownify return markdownify.markdownify(inval) return str(inval)","title":"__html_to_md"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.__init__","text":"Send in StudyMetadata dict to create a nicely formatted README document Parameters: study_metadata_obj ( StudyMetadata ) \u2013 A study metadata object **kwargs ( dict , default: {} ) \u2013 Keyword arguments url ( str ) \u2013 The base URL for a Dataverse instance pid ( Union [ str , int ] ) \u2013 The persistent identifier of a file or a file id key ( str ) \u2013 A valid API key for performing operations on Dataverse studies local ( str ) \u2013 Path to the top level directory which holds study files. If present, the Readme creator will try to create extended data from local files instead of downloading. Notes Either local must be supplied, or url , pid and key must supplied Source code in src/dataverse_utils/collections.py def __init__(self, study_metadata_obj: StudyMetadata, **kwargs): ''' Send in StudyMetadata dict to create a nicely formatted README document Parameters ---------- study_metadata_obj : StudyMetadata A study metadata object **kwargs : dict Keyword arguments Other parameters ---------------- url : str The base URL for a Dataverse instance pid : typing.Union[str, int] The persistent identifier of a file or a file id key : str A valid API key for performing operations on Dataverse studies local : str Path to the top level directory which holds study files. If present, the Readme creator will try to create extended data from local files instead of downloading. Notes ----- Either `local` must be supplied, or `url`, `pid` and `key` must supplied ''' self.meta = study_metadata_obj self.kwargs = kwargs warnings.filterwarnings('ignore', category=bs4.MarkupResemblesLocatorWarning) #These values are the first part of the keys that need #concatenation to make them more legible. self.concat = ['author', 'datasetContact','otherId', 'keyword', 'topic', 'publication', 'producer', 'production', 'distributor', 'series', 'software', 'dsDescription', 'grant', 'contributor']","title":"__init__"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.bbox","text":"Produce sane bounding boxes from Dataverse metadata. Note that older versions of Dataverse used North and South longitude . Outputs a dict with bounding boxes contcatenated into a single line with each coordinate suffixed by its direction (eg: \u201842.97 E\u2019), with coordinates separated by commas and multiple boxes separated by semi-colons. Source code in src/dataverse_utils/collections.py def bbox(self)->dict: ''' Produce sane bounding boxes from Dataverse metadata. Note that older versions of Dataverse used North and South *longitude*. Outputs a dict with bounding boxes contcatenated into a single line with each coordinate suffixed by its direction (eg: '42.97 E'), with coordinates separated by commas and multiple boxes separated by semi-colons. ''' #Yes, northLongitude, etc. Blame Harvard. bbox_order =['westLongitude', 'southLongitude', 'southLatitude', 'eastLongitude', 'northLongitude', 'northLatitude'] geog_me = {_: self.meta[_].split(';') for _ in bbox_order if self.meta.get(_)}# Checking for existence causes problems if not geog_me: #Sometimes there is no bounding box return {} bbox = {k: [f'{v} {k[0].capitalize()}'.strip() for v in geog_me[k]] for k in bbox_order if geog_me.get(k)} boxes = self.max_zip(*bbox.values()) boxes = [', '.join(_) for _ in boxes] boxes = [f'({_})' for _ in boxes] return {'Bounding box(es)': '; '.join(boxes)}","title":"bbox"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.concatenator","text":"Produce a concatenated dictionary with the key being just the prefix. For fields like author[whatever], etc, where there are multiple components of similar metadata held in completely separated fields. Parameters: meta ( dict ) \u2013 Input metadata Source code in src/dataverse_utils/collections.py def concatenator(self, meta:dict)->dict: ''' Produce a concatenated dictionary with the key being just the prefix. For fields like author[whatever], etc, where there are multiple *components* of similar metadata held in completely separated fields. Parameters ---------- meta : dict Input metadata ''' #The keys are the first part of the fields that need concatenation concat = {_:[] for _ in self.concat} for k, v in meta.items(): for fk in concat: if k.startswith(fk): if v: if concat[fk]: concat[fk].append(v.split(';')) else: concat[fk] = [v.split(';')] outdict = {} for ke, va in concat.items(): if va: interim = self.max_zip(*va) interim = [' - '.join([y.strip() for y in _ if y]) for _ in interim ] #interim = '; '.join(interim) # Should it be newline? #interim = ' \\n'.join(interim) # Should it be newline? interim= '<br/>'.join(interim)# Markdownify strips internal spaces #if ke.startswith('keyw'): outdict[ke] = interim return outdict","title":"concatenator"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.make_md_heads","text":"Make markdown H2 headings for selected sections, currently title, description, licence and terms of use. Parameters: inkey ( str ) \u2013 Section heading Source code in src/dataverse_utils/collections.py def make_md_heads(self, inkey:str)->str: ''' Make markdown H2 headings for selected sections, currently title, description, licence and terms of use. Parameters ---------- inkey : str Section heading ''' section_heads = {'Title':'## ', 'Description':'**Description**\\n\\n', 'Licence': '### Licence\\n\\n', 'Terms of Use': '### Terms of Use\\n\\n'} if inkey in section_heads: return section_heads[inkey] multi = [self.rename_field(_) for _ in self.concat] if inkey in multi: if inkey not in ['Series', 'Software', 'Production']: return f'{inkey}(s): \\n' return f'{inkey}: \\n' return f'{inkey}: '","title":"make_md_heads"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.max_zip","text":"Like built-in zip, only uses the maximum length and appends None if not found instead of stopping at the shortest iterable. Parameters: *args ( iterable , default: () ) \u2013 Any iterable Source code in src/dataverse_utils/collections.py def max_zip(self, *args): ''' Like built-in zip, only uses the *maximum* length and appends None if not found instead of stopping at the shortest iterable. Parameters ---------- *args : iterable Any iterable ''' length = max(map(len, args)) outlist=[] for n in range(length): vals = [] for arg in args: try: vals.append(arg[n]) except IndexError: vals.append(None) outlist.append(vals) return outlist","title":"max_zip"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.rename_field","text":"Split and capitalize camelCase fields as required. eg: keywordValue -> Keyword Value eg: termsOfUse -> Terms of Use Parameters: instr ( str ) \u2013 Camel case tring to split into words and capitalize. Source code in src/dataverse_utils/collections.py def rename_field(self, instr:str)->str: ''' Split and capitalize camelCase fields as required. eg: keywordValue -> Keyword Value eg: termsOfUse -> Terms of Use Parameters ---------- instr : str Camel case tring to split into words and capitalize. ''' noncap = ['A', 'Of', 'The'] wordsp = ''.join(map(lambda x: x if x not in string.ascii_uppercase else f' {x}', list(instr))) wordsp = wordsp.split(' ') #wordsp[0] = wordsp[0].capitalize() #wordsp = ' '.join(map(lambda x: x if x not in noncap else x.lower(), wordsp)) wordsp = list(map(lambda x: x if x not in noncap else x.lower(), wordsp)) wordsp[0] = wordsp[0].capitalize() wordsp = ' '.join(wordsp) #because they can't even use camelCaseConsistently #Also pluralization of concatenated fields fixthese ={'U R L': 'URL', 'U R I': 'URI', 'I D': 'ID', 'Ds': '', 'Country':'Country(ies)', 'State':'State(s)', 'City':'City(ies)', 'Geographic Unit':'Geographic unit(s)'} for k, v in fixthese.items(): wordsp = wordsp.replace(k, v) return wordsp.strip()","title":"rename_field"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.reorder_fields","text":"Create a list which contains a list of keys in the right (corrected) order. This ensures that concatenated fields are inserted into the right place and not at the end of the dictionary, keeping the structure of Dataverse metadata intact while concatenating values that need combining. Parameters: indict ( dict ) \u2013 Metadata dictionary Source code in src/dataverse_utils/collections.py def reorder_fields(self, indict:dict)->list: ''' Create a list which contains a list of keys in the right (corrected) order. This ensures that concatenated fields are inserted into the right place and not at the end of the dictionary, keeping the structure of Dataverse metadata intact while concatenating values that need combining. Parameters ---------- indict : dict Metadata dictionary ''' fieldlist = list(indict) for val in self.concat: pts = [n for n, x in enumerate(fieldlist) if x.startswith(val)] if pts: ins_point = min(pts) fieldlist.insert(ins_point, val) #Geography fields are a special case yay. #westLongitude is the fist one if 'westLongitude' in fieldlist: ins_here = fieldlist.index('westLongitude') fieldlist.insert(ins_here, 'Bounding box(es)') return fieldlist","title":"reorder_fields"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.write_md","text":"Write Markdown text of the complete documentation to a file. Parameters: dest ( str ) \u2013 Destination of file, optionally including path. eg: /Users/foo/study/README.md or ~/tmp/README_I_AM_METADATA.md Source code in src/dataverse_utils/collections.py def write_md(self, dest:str)->None: ''' Write Markdown text of the complete documentation to a file. Parameters ---------- dest : str Destination of file, optionally including path. eg: /Users/foo/study/README.md or ~/tmp/README_I_AM_METADATA.md ''' dest = pathlib.Path(dest).expanduser().absolute() with open(file=dest, mode='w', encoding='utf=8') as f: f.write(self.readme_md)","title":"write_md"},{"location":"api_ref/#dataverse_utils.collections.ReadmeCreator.write_pdf","text":"Make the PDF of a README and save it to a file. Parameters: dest ( str ) \u2013 Destination of file, optionally including path. eg: /Users/foo/study/README.pdf or ~/tmp/README_I_AM_METADATA.pdf Source code in src/dataverse_utils/collections.py def write_pdf(self, dest:str)->None: ''' Make the PDF of a README and save it to a file. Parameters ---------- dest : str Destination of file, optionally including path. eg: /Users/foo/study/README.pdf or ~/tmp/README_I_AM_METADATA.pdf ''' dest = pathlib.Path(dest).expanduser().absolute() output = markdown_pdf.MarkdownPdf(toc_level=1) content = markdown_pdf.Section(self.readme_md, toc=False) output.add_section(content) output.save(dest)","title":"write_pdf"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata","text":"Bases: dict The metadata container for a single study. Source code in src/dataverse_utils/collections.py class StudyMetadata(dict): ''' The metadata container for a single study. ''' def __init__(self, **kwargs): ''' Intializize a StudyMetadata object. Parameters ---------- **kwargs: dict At least some of the following Other parameters ---------------- study_meta : dict, optional The dataverse study metadata JSON url : str, optional Base URL to dataverse instance pid : str, optional Persistent ID of a study key : str Dataverse instance API key (needed for unpublished studies) Notes ----- Either `study_meta` is required OR `pid` and `url`. `key` _may_ be required if either a draft study is being accessed or the Dataverse installation requires API keys for all requests. ''' self.kwargs = kwargs self.study_meta = kwargs.get('study_meta') self.url = kwargs.get('url') self.pid = kwargs.get('pid') self.headers = UAHEADER.copy() if not (('study_meta' in kwargs) or ('url' in kwargs and 'pid' in kwargs)): raise TypeError('At least one of a URL/pid combo (url, pid) (and possibly key) or ' 'study metadata json (study_meta) is required.') if not self.study_meta: self.study_meta = self.__obtain_metadata() try: self.extract_metadata() except KeyError as e: raise MetadataError(f'Unable to parse study metadata. Do you need an API key?\\n' f'{e} key not found.\\n' f'Offending JSON: {self.study_meta}') from e self.__files = None def __obtain_metadata(self): ''' Obtain study metadata as required. ''' if self.kwargs.get('key'): self.headers.update({'X-Dataverse-key':self.kwargs['key']}) params = {'persistentId': self.pid} self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=RETRY)) self.url = self.url.strip('/') if not self.url.startswith('https://'): self.url = f'https://{self.url}' data = self.session.get(f'{self.url}/api/datasets/:persistentId', headers=self.headers, params=params) return data.json() def __has_metadata(self)->bool: ''' Returns a boolean to ensure if there *is* study metadata. Deacessioned items are notable for their lack of any indication that they are deacessioned. However, they lack the \"latestVersion\" key, which serves as a proxy. Ideally. ''' #try: # t = self.study_meta['data'] # del t #OMG This is so dumb #except KeyError as e: # raise e if not self.study_meta.get('data'): raise KeyError('data') testfields = ['id', 'identifier', 'authority', 'latestVersion'] if all(self.study_meta['data'].get(_) for _ in testfields): return True return False def extract_metadata(self): ''' Convenience function for parsing the study metadata of the latest version. Results are written to self, accessible as a dictionary. ''' if not self.__has_metadata(): return for v in self.study_meta['data']['latestVersion']['metadataBlocks'].values(): for field in v['fields']: self.extract_field_metadata(field) self.__extract_licence_info() self.__version() #['data']['latestVersion']['versionNumber'] #['data']['latestVersion']['versionMinorNumber'] def extract_field_metadata(self, field): ''' Extract the metadata from a single field and make it into a human-readable dict. Output updates self. ''' #pylint: disable=too-many-branches, too-many-nested-blocks #typeClass: compound = dict, primitive = string #multiple: false= one thing, true=list # so typeClass:compound AND multiple:true = a list of dicts. # also, typeClass can be \"controlledVocabulary\" because reasons. #is this crap recursive or is one level enough? #[[x['typeName'], x['typeClass'], x['multiple']] for x in citation['fields']] # {('primitive', False), ('compound', True), ('compound', False), # ('primitive', True), ('controlledVocabulary', True)} if not field['multiple']: if field['typeClass']=='primitive': self.update({field['typeName']: field['value']}) if field['typeClass'] == 'compound': for v2 in field['value']: self.extract_field_metadata(field['value'][v2]) if field['multiple']: if field['typeClass'] == 'compound': #produce a list of similar values concatenated for v3 in field['value']: interim = {} for insane_dict in field['value']: for v3 in insane_dict.values(): if interim.get(v3['typeName']): interim.update({v3['typeName']: interim[v3['typeName']]+ [v3['value']]}) else: #sometimes value is None because reasons. interim[v3['typeName']] = [v3.get('value', [] )] LOGGER.debug(interim) for k9, v9 in interim.items(): self.update({k9: '; '.join(v9)}) if field['typeClass'] == 'primitive': self.update({field['typeName'] : '; '.join(field['value'])}) if field['typeClass'] == 'controlledVocabulary': if isinstance(field['value'], list): self.update({field['typeName'] : '; '.join(field['value'])}) else: self.update({field['typeName'] : field['value']}) # And that should cover every option! @property def files(self)->list: ''' Return a list of of dicts with file metadata. ''' if not self.__files: self.__extract_files() return self.__files def __extract_files(self): ''' Extract file level metadata, and write to self.__files. ''' #Note: ALL other dict values for this object are single values, #but files would (usually) be an arbitrary number of files. #That bothers me on an intellectual level. Therefore, it will be attribute. #Iterate over StudyMetadata.files if you want to know the contents if not self.__files: outie = [] for v in self.study_meta['data']['latestVersion']['files']: innie = {} fpath = v.get('directoryLabel', '').strip('/') innie['filename'] = v['dataFile'].get('originalFileName', v['dataFile']['filename']) #innie['full_path'] = '/'.join([fpath, innie['filename']]) #In case it's pathless, drop any leading slash, because #'' is not the same as None, and None can't be joined. innie['filename'] = '/'.join([fpath, innie['filename']]).strip('/') innie['file_label'] = v.get('label') innie['description'] = v.get('description') innie['filesize_bytes'] = v['dataFile'].get('originalFileSize', v['dataFile']['filesize']) innie['chk_type'] = v['dataFile']['checksum']['type'] innie['chk_digest'] =v['dataFile']['checksum']['value'] innie['id'] = v['dataFile']['id'] innie['pid'] = v['dataFile'].get('persistentId') innie['has_tab_file'] = v['dataFile'].get('tabularData', False) innie['study_pid'] = (f\"{self.study_meta['data']['protocol']}:\" f\"{self.study_meta['data']['authority']}/\" f\"{self.study_meta['data']['identifier']}\") innie['tags'] = ', '.join(v.get('categories', [])) if not innie['tags']: del innie['tags']#tagless #innie['path'] = v.get('directoryLabel', '') outie.append(innie) self.__files = outie def __extract_licence_info(self): ''' Extract all the licence information fields and add them to self['licence'] *if present*. ''' lic_fields = ('termsOfUse', 'confidentialityDeclaration', 'specialPermissions', 'restrictions', 'citationRequirements', 'depositorRequirements', 'conditions', 'disclaimer', 'dataAccessPlace', 'originalArchive', 'availabilityStatus', 'contactForAccess', 'sizeOfCollection', 'studyCompletion', 'fileAccessRequest') for field in self.study_meta['data']['latestVersion']: if field in lic_fields: self[field] = self.study_meta['data']['latestVersion'][field] common_lic = self.study_meta['data']['latestVersion'].get('license') if isinstance(common_lic, str) and common_lic != 'NONE': self['licence'] = common_lic elif isinstance(common_lic, dict): self['licence'] = self.study_meta['data']['latestVersion']['license'].get('name') link = self.study_meta['data']['latestVersion']['license'].get('uri') if link: self['licenceLink'] = link def __version(self): ''' Obtain the current version and add it to self['studyVersion']. ''' if self.study_meta['data']['latestVersion']['versionState'] == 'RELEASED': self['studyVersion'] = (f\"{self.study_meta['data']['latestVersion']['versionNumber']}.\" f\"{self.study_meta['data']['latestVersion']['versionMinorNumber']}\") return self['studyVersion'] = self.study_meta['data']['latestVersion']['versionState'] return","title":"StudyMetadata"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata.files","text":"Return a list of of dicts with file metadata.","title":"files"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata.__extract_files","text":"Extract file level metadata, and write to self.__files. Source code in src/dataverse_utils/collections.py def __extract_files(self): ''' Extract file level metadata, and write to self.__files. ''' #Note: ALL other dict values for this object are single values, #but files would (usually) be an arbitrary number of files. #That bothers me on an intellectual level. Therefore, it will be attribute. #Iterate over StudyMetadata.files if you want to know the contents if not self.__files: outie = [] for v in self.study_meta['data']['latestVersion']['files']: innie = {} fpath = v.get('directoryLabel', '').strip('/') innie['filename'] = v['dataFile'].get('originalFileName', v['dataFile']['filename']) #innie['full_path'] = '/'.join([fpath, innie['filename']]) #In case it's pathless, drop any leading slash, because #'' is not the same as None, and None can't be joined. innie['filename'] = '/'.join([fpath, innie['filename']]).strip('/') innie['file_label'] = v.get('label') innie['description'] = v.get('description') innie['filesize_bytes'] = v['dataFile'].get('originalFileSize', v['dataFile']['filesize']) innie['chk_type'] = v['dataFile']['checksum']['type'] innie['chk_digest'] =v['dataFile']['checksum']['value'] innie['id'] = v['dataFile']['id'] innie['pid'] = v['dataFile'].get('persistentId') innie['has_tab_file'] = v['dataFile'].get('tabularData', False) innie['study_pid'] = (f\"{self.study_meta['data']['protocol']}:\" f\"{self.study_meta['data']['authority']}/\" f\"{self.study_meta['data']['identifier']}\") innie['tags'] = ', '.join(v.get('categories', [])) if not innie['tags']: del innie['tags']#tagless #innie['path'] = v.get('directoryLabel', '') outie.append(innie) self.__files = outie","title":"__extract_files"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata.__extract_licence_info","text":"Extract all the licence information fields and add them to self[\u2018licence\u2019] if present . Source code in src/dataverse_utils/collections.py def __extract_licence_info(self): ''' Extract all the licence information fields and add them to self['licence'] *if present*. ''' lic_fields = ('termsOfUse', 'confidentialityDeclaration', 'specialPermissions', 'restrictions', 'citationRequirements', 'depositorRequirements', 'conditions', 'disclaimer', 'dataAccessPlace', 'originalArchive', 'availabilityStatus', 'contactForAccess', 'sizeOfCollection', 'studyCompletion', 'fileAccessRequest') for field in self.study_meta['data']['latestVersion']: if field in lic_fields: self[field] = self.study_meta['data']['latestVersion'][field] common_lic = self.study_meta['data']['latestVersion'].get('license') if isinstance(common_lic, str) and common_lic != 'NONE': self['licence'] = common_lic elif isinstance(common_lic, dict): self['licence'] = self.study_meta['data']['latestVersion']['license'].get('name') link = self.study_meta['data']['latestVersion']['license'].get('uri') if link: self['licenceLink'] = link","title":"__extract_licence_info"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata.__has_metadata","text":"Returns a boolean to ensure if there is study metadata. Deacessioned items are notable for their lack of any indication that they are deacessioned. However, they lack the \u201clatestVersion\u201d key, which serves as a proxy. Ideally. Source code in src/dataverse_utils/collections.py def __has_metadata(self)->bool: ''' Returns a boolean to ensure if there *is* study metadata. Deacessioned items are notable for their lack of any indication that they are deacessioned. However, they lack the \"latestVersion\" key, which serves as a proxy. Ideally. ''' #try: # t = self.study_meta['data'] # del t #OMG This is so dumb #except KeyError as e: # raise e if not self.study_meta.get('data'): raise KeyError('data') testfields = ['id', 'identifier', 'authority', 'latestVersion'] if all(self.study_meta['data'].get(_) for _ in testfields): return True return False","title":"__has_metadata"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata.__init__","text":"Intializize a StudyMetadata object. Parameters: **kwargs \u2013 At least some of the following study_meta ( dict ) \u2013 The dataverse study metadata JSON url ( str ) \u2013 Base URL to dataverse instance pid ( str ) \u2013 Persistent ID of a study key ( str ) \u2013 Dataverse instance API key (needed for unpublished studies) Notes Either study_meta is required OR pid and url . key may be required if either a draft study is being accessed or the Dataverse installation requires API keys for all requests. Source code in src/dataverse_utils/collections.py def __init__(self, **kwargs): ''' Intializize a StudyMetadata object. Parameters ---------- **kwargs: dict At least some of the following Other parameters ---------------- study_meta : dict, optional The dataverse study metadata JSON url : str, optional Base URL to dataverse instance pid : str, optional Persistent ID of a study key : str Dataverse instance API key (needed for unpublished studies) Notes ----- Either `study_meta` is required OR `pid` and `url`. `key` _may_ be required if either a draft study is being accessed or the Dataverse installation requires API keys for all requests. ''' self.kwargs = kwargs self.study_meta = kwargs.get('study_meta') self.url = kwargs.get('url') self.pid = kwargs.get('pid') self.headers = UAHEADER.copy() if not (('study_meta' in kwargs) or ('url' in kwargs and 'pid' in kwargs)): raise TypeError('At least one of a URL/pid combo (url, pid) (and possibly key) or ' 'study metadata json (study_meta) is required.') if not self.study_meta: self.study_meta = self.__obtain_metadata() try: self.extract_metadata() except KeyError as e: raise MetadataError(f'Unable to parse study metadata. Do you need an API key?\\n' f'{e} key not found.\\n' f'Offending JSON: {self.study_meta}') from e self.__files = None","title":"__init__"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata.__obtain_metadata","text":"Obtain study metadata as required. Source code in src/dataverse_utils/collections.py def __obtain_metadata(self): ''' Obtain study metadata as required. ''' if self.kwargs.get('key'): self.headers.update({'X-Dataverse-key':self.kwargs['key']}) params = {'persistentId': self.pid} self.session = requests.Session() self.session.mount('https://', requests.adapters.HTTPAdapter(max_retries=RETRY)) self.url = self.url.strip('/') if not self.url.startswith('https://'): self.url = f'https://{self.url}' data = self.session.get(f'{self.url}/api/datasets/:persistentId', headers=self.headers, params=params) return data.json()","title":"__obtain_metadata"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata.__version","text":"Obtain the current version and add it to self[\u2018studyVersion\u2019]. Source code in src/dataverse_utils/collections.py def __version(self): ''' Obtain the current version and add it to self['studyVersion']. ''' if self.study_meta['data']['latestVersion']['versionState'] == 'RELEASED': self['studyVersion'] = (f\"{self.study_meta['data']['latestVersion']['versionNumber']}.\" f\"{self.study_meta['data']['latestVersion']['versionMinorNumber']}\") return self['studyVersion'] = self.study_meta['data']['latestVersion']['versionState'] return","title":"__version"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata.extract_field_metadata","text":"Extract the metadata from a single field and make it into a human-readable dict. Output updates self. Source code in src/dataverse_utils/collections.py def extract_field_metadata(self, field): ''' Extract the metadata from a single field and make it into a human-readable dict. Output updates self. ''' #pylint: disable=too-many-branches, too-many-nested-blocks #typeClass: compound = dict, primitive = string #multiple: false= one thing, true=list # so typeClass:compound AND multiple:true = a list of dicts. # also, typeClass can be \"controlledVocabulary\" because reasons. #is this crap recursive or is one level enough? #[[x['typeName'], x['typeClass'], x['multiple']] for x in citation['fields']] # {('primitive', False), ('compound', True), ('compound', False), # ('primitive', True), ('controlledVocabulary', True)} if not field['multiple']: if field['typeClass']=='primitive': self.update({field['typeName']: field['value']}) if field['typeClass'] == 'compound': for v2 in field['value']: self.extract_field_metadata(field['value'][v2]) if field['multiple']: if field['typeClass'] == 'compound': #produce a list of similar values concatenated for v3 in field['value']: interim = {} for insane_dict in field['value']: for v3 in insane_dict.values(): if interim.get(v3['typeName']): interim.update({v3['typeName']: interim[v3['typeName']]+ [v3['value']]}) else: #sometimes value is None because reasons. interim[v3['typeName']] = [v3.get('value', [] )] LOGGER.debug(interim) for k9, v9 in interim.items(): self.update({k9: '; '.join(v9)}) if field['typeClass'] == 'primitive': self.update({field['typeName'] : '; '.join(field['value'])}) if field['typeClass'] == 'controlledVocabulary': if isinstance(field['value'], list): self.update({field['typeName'] : '; '.join(field['value'])}) else: self.update({field['typeName'] : field['value']})","title":"extract_field_metadata"},{"location":"api_ref/#dataverse_utils.collections.StudyMetadata.extract_metadata","text":"Convenience function for parsing the study metadata of the latest version. Results are written to self, accessible as a dictionary. Source code in src/dataverse_utils/collections.py def extract_metadata(self): ''' Convenience function for parsing the study metadata of the latest version. Results are written to self, accessible as a dictionary. ''' if not self.__has_metadata(): return for v in self.study_meta['data']['latestVersion']['metadataBlocks'].values(): for field in v['fields']: self.extract_field_metadata(field) self.__extract_licence_info() self.__version()","title":"extract_metadata"},{"location":"common_workflows/","text":"Common workflows \u00b6 Given the pile of various utilities, what are they for? Here are some very common use cases. Studies with many files \u00b6 While it\u2019s possible to add multiple files at once to a Dataverse record by uploading a zip file, there is no metadata included in a zip file, which means laboriously adding file descriptions after the fact using the Dataverse GUI. Imagine that you have a study with a complex file tree and a large number of files. For example: . \u251c\u2500\u2500 Command Files \u2502 \u251c\u2500\u2500 bootstrap \u2502 \u2502 \u251c\u2500\u2500 sas \u2502 \u2502 \u2502 \u2514\u2500\u2500 bsw_i.sas \u2502 \u2502 \u2514\u2500\u2500 spss \u2502 \u2502 \u2514\u2500\u2500 bsw_i.sps \u2502 \u251c\u2500\u2500 sas \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_SAS_sas.zip \u2502 \u251c\u2500\u2500 spss \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_SPSS_sps.zip \u2502 \u2514\u2500\u2500 stata \u2502 \u2514\u2500\u2500 CCHS_2022_Stata_do.zip \u251c\u2500\u2500 Data \u2502 \u251c\u2500\u2500 ascii \u2502 \u2502 \u251c\u2500\u2500 CCHS_2022_ASCII_csv.zip \u2502 \u2502 \u251c\u2500\u2500 CCHS_2022_ASCII_txt.zip \u2502 \u2502 \u251c\u2500\u2500 pumf_cchs.csv \u2502 \u2502 \u2514\u2500\u2500 PUMF_MASTER_CCHS.txt \u2502 \u251c\u2500\u2500 bootstrap \u2502 \u2502 \u251c\u2500\u2500 ascii \u2502 \u2502 \u2502 \u251c\u2500\u2500 bsw.txt \u2502 \u2502 \u2502 \u251c\u2500\u2500 CCHS_2022_BSW_ASCII_csv.zip \u2502 \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_BSW_ASCII_txt.zip \u2502 \u2502 \u251c\u2500\u2500 sas \u2502 \u2502 \u2502 \u251c\u2500\u2500 bsw.sas7bdat \u2502 \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_BSW_SAS_sas7bdat.zip \u2502 \u2502 \u251c\u2500\u2500 spss \u2502 \u2502 \u2502 \u251c\u2500\u2500 bsw.sav \u2502 \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_BSW_SPSS_sav.zip \u2502 \u2502 \u2514\u2500\u2500 stata \u2502 \u2502 \u251c\u2500\u2500 bsw.dta \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_BSW_Stata_dta.zip \u2502 \u251c\u2500\u2500 sas \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_SAS_sas7bdat.zip \u2502 \u251c\u2500\u2500 spss \u2502 \u2502 \u251c\u2500\u2500 cchs_2022_pumf_v1.sav \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_SPSS_sav.zip \u2502 \u2514\u2500\u2500 stata \u2502 \u2514\u2500\u2500 CCHS_2022_Stata_dta.zip \u2514\u2500\u2500 Documentation \u251c\u2500\u2500 CCHS 2022 PUMF Complement User Guide.pdf \u251c\u2500\u2500 CCHS_2022_CV_Tables_PUMF.pdf \u251c\u2500\u2500 CCHS_2022_DataDictionary_Freqs.pdf \u251c\u2500\u2500 CCHS_2022_Income_Master File.pdf \u251c\u2500\u2500 CCHS_2022_PUMF_Grouped_Variables.pdf \u2514\u2500\u2500 CCHS_2022_User_Guide.pdf The procedure for this would be as follows: Create a study record manually Using dv_manifest_gen , create a file manifest. If the tree exists (ie, the files are arranged as shown above), use dv_manifest_gen -r * [plus any other options] to create a tsv manifest. If the tree does not yet exist, use the -p switch to add the field which allows virtual paths Edit the resultant tsv with file metadata. For example, the first three lines could be something like: file description tags Command Files/bootstrap/sas/bsw_i.sas SAS program for bootstrap weights file Command Files, SAS, Bootstrap weights Command Files/bootstrap/spss/bsw_i.sps SPSS syntax for bootstrap weights file Command Files, SPSS, Bootstrap weights Once the file metadata is complete, upload the whole works in one shot: dv_upload_tsv -u [url] -k [your API key] -p [your study id] [your manifest tsv file] Once it\u2019s uploaded, verify it\u2019s correct and you\u2019ve saved a great deal of time. Your definition of \u201cmany\u201d may vary, but generally \u201cmany\u201d is >=1. Using Linguistic Data Consortium metadata \u00b6 If your instituion uses a Dataverse installation to host Linguistic Data Consortium materials, you can automate record creation. Create a file manifest as outlined above for your LDC data. Use dv_ldc_uploader -u [dataverse_url] -k [API key] -t [your file manifest] -d [collection name] [LDC Study number] and your record will be created and the files uploaded and restricted. The only thing left to do is to review the record and hit the publish button. Creating README templates for institutional users \u00b6 Researcher documentation is notoriously incomplete, often neglecting important information such as rights information or basic data dictionaries. Even if the fields are required to create a record, they can still be missing from researcher-supplied documentation. Data dictionaries, message digests, etc are also often missing. The README creator will harvest this information, much of which is required to create a record in the first place, and concatenate it into a nicely structured text (or PDF) document. This can be forwarded to the researcher for editing and uploading along with a data set. This is commonly done by the administrator of a collection, as they have access to the study contents and metadata. To do this: dv_readme_creator -u [Dataverse url] -p [persistent ID] --k [API key] README.md Attach the resultant README.md file to an email along with suggestions for improvement. Collection analysis \u00b6 While Dataverse installations have a reasonably robust set of metrics available, detailed collection analysis may be required for any number of reasons. Collection recursion is not supported in the API, but is required for institutions analyzing their collections. A spreadsheet of a collection with selected metadata fields can be made easily with dv_collection_info For example, you wish to recursively analyze your collection called foo , and you need the authors, titles, keywords and distribution date. dv_collection_info -u [Dataverse instance] -k [key, may or may not be required] -c foo -f authorName title keywordValue distributionDate -o [wherever your file is .tsv] Field names are those defined in the Dataverse JSON specification. Fields can be added by custom metadata blocks, but a sample can be found on the IQSS github page here [2025-10-23], where the field names are found using the \u2018typeName\u2019 key. Recurring series \u00b6 Some records are broadly similar, requiring only minor changes, such as a year change, etc. Instead of copy/paste: dv_record_copy -u [Dataverse installation] -k [key] -c [collection where it should live] doi:xxx/xxx Your record is instantly copied and requires minor edits instead of a full manual entry. Helper utilities \u00b6 All of these things take place in a command line environment. To avoid having to leave it to use a spreadsheet application, consider using Visidata instead of a spreadsheet so that you can rapidly edit any tabular data. Dataverse_utils has no affiliation with Visidata beyond recommending it as a useful tool.","title":"Common workflows"},{"location":"common_workflows/#common-workflows","text":"Given the pile of various utilities, what are they for? Here are some very common use cases.","title":"Common workflows"},{"location":"common_workflows/#studies-with-many-files","text":"While it\u2019s possible to add multiple files at once to a Dataverse record by uploading a zip file, there is no metadata included in a zip file, which means laboriously adding file descriptions after the fact using the Dataverse GUI. Imagine that you have a study with a complex file tree and a large number of files. For example: . \u251c\u2500\u2500 Command Files \u2502 \u251c\u2500\u2500 bootstrap \u2502 \u2502 \u251c\u2500\u2500 sas \u2502 \u2502 \u2502 \u2514\u2500\u2500 bsw_i.sas \u2502 \u2502 \u2514\u2500\u2500 spss \u2502 \u2502 \u2514\u2500\u2500 bsw_i.sps \u2502 \u251c\u2500\u2500 sas \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_SAS_sas.zip \u2502 \u251c\u2500\u2500 spss \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_SPSS_sps.zip \u2502 \u2514\u2500\u2500 stata \u2502 \u2514\u2500\u2500 CCHS_2022_Stata_do.zip \u251c\u2500\u2500 Data \u2502 \u251c\u2500\u2500 ascii \u2502 \u2502 \u251c\u2500\u2500 CCHS_2022_ASCII_csv.zip \u2502 \u2502 \u251c\u2500\u2500 CCHS_2022_ASCII_txt.zip \u2502 \u2502 \u251c\u2500\u2500 pumf_cchs.csv \u2502 \u2502 \u2514\u2500\u2500 PUMF_MASTER_CCHS.txt \u2502 \u251c\u2500\u2500 bootstrap \u2502 \u2502 \u251c\u2500\u2500 ascii \u2502 \u2502 \u2502 \u251c\u2500\u2500 bsw.txt \u2502 \u2502 \u2502 \u251c\u2500\u2500 CCHS_2022_BSW_ASCII_csv.zip \u2502 \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_BSW_ASCII_txt.zip \u2502 \u2502 \u251c\u2500\u2500 sas \u2502 \u2502 \u2502 \u251c\u2500\u2500 bsw.sas7bdat \u2502 \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_BSW_SAS_sas7bdat.zip \u2502 \u2502 \u251c\u2500\u2500 spss \u2502 \u2502 \u2502 \u251c\u2500\u2500 bsw.sav \u2502 \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_BSW_SPSS_sav.zip \u2502 \u2502 \u2514\u2500\u2500 stata \u2502 \u2502 \u251c\u2500\u2500 bsw.dta \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_BSW_Stata_dta.zip \u2502 \u251c\u2500\u2500 sas \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_SAS_sas7bdat.zip \u2502 \u251c\u2500\u2500 spss \u2502 \u2502 \u251c\u2500\u2500 cchs_2022_pumf_v1.sav \u2502 \u2502 \u2514\u2500\u2500 CCHS_2022_SPSS_sav.zip \u2502 \u2514\u2500\u2500 stata \u2502 \u2514\u2500\u2500 CCHS_2022_Stata_dta.zip \u2514\u2500\u2500 Documentation \u251c\u2500\u2500 CCHS 2022 PUMF Complement User Guide.pdf \u251c\u2500\u2500 CCHS_2022_CV_Tables_PUMF.pdf \u251c\u2500\u2500 CCHS_2022_DataDictionary_Freqs.pdf \u251c\u2500\u2500 CCHS_2022_Income_Master File.pdf \u251c\u2500\u2500 CCHS_2022_PUMF_Grouped_Variables.pdf \u2514\u2500\u2500 CCHS_2022_User_Guide.pdf The procedure for this would be as follows: Create a study record manually Using dv_manifest_gen , create a file manifest. If the tree exists (ie, the files are arranged as shown above), use dv_manifest_gen -r * [plus any other options] to create a tsv manifest. If the tree does not yet exist, use the -p switch to add the field which allows virtual paths Edit the resultant tsv with file metadata. For example, the first three lines could be something like: file description tags Command Files/bootstrap/sas/bsw_i.sas SAS program for bootstrap weights file Command Files, SAS, Bootstrap weights Command Files/bootstrap/spss/bsw_i.sps SPSS syntax for bootstrap weights file Command Files, SPSS, Bootstrap weights Once the file metadata is complete, upload the whole works in one shot: dv_upload_tsv -u [url] -k [your API key] -p [your study id] [your manifest tsv file] Once it\u2019s uploaded, verify it\u2019s correct and you\u2019ve saved a great deal of time. Your definition of \u201cmany\u201d may vary, but generally \u201cmany\u201d is >=1.","title":"Studies with many files"},{"location":"common_workflows/#using-linguistic-data-consortium-metadata","text":"If your instituion uses a Dataverse installation to host Linguistic Data Consortium materials, you can automate record creation. Create a file manifest as outlined above for your LDC data. Use dv_ldc_uploader -u [dataverse_url] -k [API key] -t [your file manifest] -d [collection name] [LDC Study number] and your record will be created and the files uploaded and restricted. The only thing left to do is to review the record and hit the publish button.","title":"Using Linguistic Data Consortium metadata"},{"location":"common_workflows/#creating-readme-templates-for-institutional-users","text":"Researcher documentation is notoriously incomplete, often neglecting important information such as rights information or basic data dictionaries. Even if the fields are required to create a record, they can still be missing from researcher-supplied documentation. Data dictionaries, message digests, etc are also often missing. The README creator will harvest this information, much of which is required to create a record in the first place, and concatenate it into a nicely structured text (or PDF) document. This can be forwarded to the researcher for editing and uploading along with a data set. This is commonly done by the administrator of a collection, as they have access to the study contents and metadata. To do this: dv_readme_creator -u [Dataverse url] -p [persistent ID] --k [API key] README.md Attach the resultant README.md file to an email along with suggestions for improvement.","title":"Creating README templates for institutional users"},{"location":"common_workflows/#collection-analysis","text":"While Dataverse installations have a reasonably robust set of metrics available, detailed collection analysis may be required for any number of reasons. Collection recursion is not supported in the API, but is required for institutions analyzing their collections. A spreadsheet of a collection with selected metadata fields can be made easily with dv_collection_info For example, you wish to recursively analyze your collection called foo , and you need the authors, titles, keywords and distribution date. dv_collection_info -u [Dataverse instance] -k [key, may or may not be required] -c foo -f authorName title keywordValue distributionDate -o [wherever your file is .tsv] Field names are those defined in the Dataverse JSON specification. Fields can be added by custom metadata blocks, but a sample can be found on the IQSS github page here [2025-10-23], where the field names are found using the \u2018typeName\u2019 key.","title":"Collection analysis"},{"location":"common_workflows/#recurring-series","text":"Some records are broadly similar, requiring only minor changes, such as a year change, etc. Instead of copy/paste: dv_record_copy -u [Dataverse installation] -k [key] -c [collection where it should live] doi:xxx/xxx Your record is instantly copied and requires minor edits instead of a full manual entry.","title":"Recurring series"},{"location":"common_workflows/#helper-utilities","text":"All of these things take place in a command line environment. To avoid having to leave it to use a spreadsheet application, consider using Visidata instead of a spreadsheet so that you can rapidly edit any tabular data. Dataverse_utils has no affiliation with Visidata beyond recommending it as a useful tool.","title":"Helper utilities"},{"location":"credits/","text":"Credits \u00b6 Dataverse_utils and their associated command line programs were written by Paul Lesack . Testing and UI suggestions from: * Jeremy Buhler . * Eugene Barsky . This is a product of the University of British Columbia Library Research Commons .","title":"Credits"},{"location":"credits/#credits","text":"Dataverse_utils and their associated command line programs were written by Paul Lesack . Testing and UI suggestions from: * Jeremy Buhler . * Eugene Barsky . This is a product of the University of British Columbia Library Research Commons .","title":"Credits"},{"location":"faq/","text":"Frequently asked questions \u00b6 \u201cFrequently\u201d may be relative. 1. I\u2019m using Windows and the scripts don\u2019t seem to be working/recognized by [choice of command line interface here] \u00b6 There are (at least) four different ways to get some sort of command line access (actually a lot more than four but these are common). The traditional command line, PowerShell, via SSH and Git Bash. That\u2019s not even including the linux subsystem. Each of these may not be share the same environment as any other. The document on Windows script troubles gives common solutions. 2. I am using Windows [7-10]. I\u2019ve installed via pip using a virtual environment, but they don\u2019t use my virtual environment\u2019s Python. \u00b6 What seems like a simple problem is surprisingly complex, as outlined here: https://matthew-brett.github.io/pydagogue/installing_scripts.html . This is further complicated by some other factors. If you have only one Python installation on your machine, you are probably OK. Mind you, if you\u2019re looking at this and that\u2019s the case, maybe I\u2019m wrong. If you have multiple Pythons, you can try changing your environment variables to point to the correct Python. If you are using ArcGIS products, ArcGIS may write items into your Windows registry which will associate any .py file with it\u2019s grossly outdated Python 2.7, or just the wrong Python. More specifically: Computer\\HKEY_CLASSES_ROOT\\Python.File\\shell\\open\\command in the Windows registry. Unfortunately, the Windows computer that I have available for testing (as I normally use Linux or Mac) does not have an administrator account, so I can\u2019t confirm that changing this key will work (although there\u2019s no reason to believe it won\u2019t). This is a pain. Is there something less irritating that would work? \u00b6 Yes, there is. You can still run the scripts manually. There are two options for this. Download the repository via git to a convenient place and use the files in the scripts/ directory Point your %PATH% to where the scripts are installed. To find out where they are installed: TLDR: version: Point your %PATH% (and use first part of the path) to point to your [venv]\\Scripts directory, because they\u2019re probably there. Long winded instructions/explanation: Start a Python session: import sys sys.path [x for x in sys.path if x.endswith(\u2018site-packages\u2019)] The location of the scripts will be written in [whatever the output of sys.path]/dataverse_utils[somestuff]egg-info/installed-files.txt , usually three levels up in the scripts directory. Ironically, this is also the location of the activate portion of the comand required to start a virtual environment (if you are using a virtual environment). For some background on this, venv in Linux and Mac versions of Python uses bin , and reserves scripts for the scripts directory. Windows, however, uses Scripts for the venv module, and to make it worse it\u2019s not usually case sensitive, so anything in scripts gets put into Scripts .","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"\u201cFrequently\u201d may be relative.","title":"Frequently asked questions"},{"location":"faq/#1-im-using-windows-and-the-scripts-dont-seem-to-be-workingrecognized-by-choice-of-command-line-interface-here","text":"There are (at least) four different ways to get some sort of command line access (actually a lot more than four but these are common). The traditional command line, PowerShell, via SSH and Git Bash. That\u2019s not even including the linux subsystem. Each of these may not be share the same environment as any other. The document on Windows script troubles gives common solutions.","title":"1. I&rsquo;m using Windows and the scripts don&rsquo;t seem to be working/recognized by [choice of command line interface here]"},{"location":"faq/#2-i-am-using-windows-7-10-ive-installed-via-pip-using-a-virtual-environment-but-they-dont-use-my-virtual-environments-python","text":"What seems like a simple problem is surprisingly complex, as outlined here: https://matthew-brett.github.io/pydagogue/installing_scripts.html . This is further complicated by some other factors. If you have only one Python installation on your machine, you are probably OK. Mind you, if you\u2019re looking at this and that\u2019s the case, maybe I\u2019m wrong. If you have multiple Pythons, you can try changing your environment variables to point to the correct Python. If you are using ArcGIS products, ArcGIS may write items into your Windows registry which will associate any .py file with it\u2019s grossly outdated Python 2.7, or just the wrong Python. More specifically: Computer\\HKEY_CLASSES_ROOT\\Python.File\\shell\\open\\command in the Windows registry. Unfortunately, the Windows computer that I have available for testing (as I normally use Linux or Mac) does not have an administrator account, so I can\u2019t confirm that changing this key will work (although there\u2019s no reason to believe it won\u2019t).","title":"2. I am using Windows [7-10]. I&rsquo;ve installed via pip using a virtual environment, but they don&rsquo;t use my virtual environment&rsquo;s Python."},{"location":"faq/#this-is-a-pain-is-there-something-less-irritating-that-would-work","text":"Yes, there is. You can still run the scripts manually. There are two options for this. Download the repository via git to a convenient place and use the files in the scripts/ directory Point your %PATH% to where the scripts are installed. To find out where they are installed: TLDR: version: Point your %PATH% (and use first part of the path) to point to your [venv]\\Scripts directory, because they\u2019re probably there. Long winded instructions/explanation: Start a Python session: import sys sys.path [x for x in sys.path if x.endswith(\u2018site-packages\u2019)] The location of the scripts will be written in [whatever the output of sys.path]/dataverse_utils[somestuff]egg-info/installed-files.txt , usually three levels up in the scripts directory. Ironically, this is also the location of the activate portion of the comand required to start a virtual environment (if you are using a virtual environment). For some background on this, venv in Linux and Mac versions of Python uses bin , and reserves scripts for the scripts directory. Windows, however, uses Scripts for the venv module, and to make it worse it\u2019s not usually case sensitive, so anything in scripts gets put into Scripts .","title":"This is a pain. Is there something less irritating that would work?"},{"location":"scripts/","text":"Console utilities \u00b6 code { white-space : pre-wrap !important; } These utilities are available at the command line/command prompt and don\u2019t require any Python knowledge except how to install a Python library via pip, as outlined in the overview document. Once installed via pip, the scripts will be available via the command line and will not require calling Python explicitly. That is, they can be called from the command line directly. For example: dv_manifest_gen [parameters] is all you will need to type. Note that these programs have been primarily tested on Linux and MacOS, with Windows a tertiary priority . Windows is notable for its unusual file handling, so, as the MIT licence stipulates, there is no warranty as to the suitability for a particular purpose. In alphabetical order: dv_collection_info \u00b6 A recursive file metadata utility. You can specify the head of a tree and the harvester will harvest the [latest] study metadata and output it as a spreadsheet. An API key is not required for publicly accessible data. usage: dv_collection_info [-h] [-u URL] -c COLLECTION [-k KEY] [-d DELIMITER] [-f [FIELDS ...]] [-o OUTPUT] [--verbose] [-v] Recursively parses a dataverse collection and outputs study metadata for the latest version. If analyzing publicly available collections, a dataverse API key for the target system is not required. options: -h, --help show this help message and exit -u, --url URL Dataverse installation base url. defaults to \"https://abacus.library.ubc.ca\" -c, --collection COLLECTION Dataverse collection shortname or id at the top of the tree -k, --key KEY API key -d, --delimiter DELIMITER Delimiter for output spreadsheet. Default: tab (\\t) -f, --fields [FIELDS ...] Record metadata fields to output. For all fields, use \"all\". Default: title, author. -o, --output OUTPUT Output file name. --verbose Verbose output. See what's happening. -v, --version Show version number and exit dv_del \u00b6 This is bulk deletion utility for unpublished studies (or even single studies). It\u2019s useful when your automated procedures have gone wrong, or if you don\u2019t feel like navigating through many menus. Note the -i switch which can ask for manual confirmation of deletions. Usage usage: dv_del [-h] -k KEY [-d DATAVERSE | -p PID] [-i] [-u DVURL] [--version] Delete draft studies from a Dataverse collection options: -h, --help show this help message and exit -k KEY, --key KEY Dataverse user API key -d DATAVERSE, --dataverse DATAVERSE Dataverse collection short name from which to delete all draft records. eg. \"ldc\" -p PID, --persistentId PID Handle or DOI to delete in format hdl:11272.1/FK2/12345 -i, --interactive Confirm each study deletion -u DVURL, --url DVURL URL to base Dataverse installation --version Show version number and exit dv_ldc_uploader \u00b6 This is a very specialized utility which will scrape metadata from the Linguistic Data Consortium (LDC) and create a metadata record in a Dataverse. The LDC does not have an API, so the metadata is scraped from their web site. This means that the metadata may not be quite as controlled as that which comes from an API. Data from the LDC website is converted to Dryad -style JSON via dataverse_utils.ldc via the use of the dryad2dataverse library. There are two main methods of use for this utility: Multiple metadata uploads. Multiple LDC record numbers can be supplied and a study without files will be created for each one. If a TSV file with file information is upplied via the -t or --tsv switch, the utility will upload a single LDC study and upload the contents of the tsv file to the created record. Certificate chain issues \u00b6 Update of the certificate chain issue: 2024-09 The problem listed below seems to have resolved itself by September 2024. It\u2019s not clear whether this was a certifi issue or an issue with LDC\u2019s certificates. In any case, if you are having problems with LDC website, use the -c switch and follow the procedure below. As of early 2023, the LDC website is not supported by certifi . You will need to manually supply a certificate chain to use the utility. To obtain the certificate chain (in Firefox) perform the following steps: Select Tools/Page Info In the Security tab, select View Certificate Scroll to \u201cPEM (chain)\u201d Right click and \u201cSave link as\u201d Use this file for the -c/\u2013certchain option below. Searching for \u201cdownload pem certificate chain [browser]\u201d in a search engine will undoubtedly bring up results for whatever browser you like. Usage usage: dv_ldc_uploader [-h] [-u URL] -k KEY [-d DVS] [-t TSV] [-r] [-n CNAME] [-c CERTCHAIN] [-e EMAIL] [-v] [--version] studies [studies ...] Linguistic Data Consortium metadata uploader for Dataverse. This utility will scrape the metadata from the LDC website (https://catalog.ldc.upenn.edu) and upload data based on a TSV manifest. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: studies LDC Catalogue numbers to process, separated by spaces. eg. \"LDC2012T19 LDC2011T07\". Case is ignored, so \"ldc2012T19\" will also work. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -d DVS, --dvs DVS Short name of target Dataverse collection (eg: ldc). Defaults to \"ldc\" -t TSV, --tsv TSV Manifest tsv file for uploading and metadata. If not supplied, only metadata will be uploaded. Using this option requires only one positional *studies* argument -r, --no-restrict Don't restrict files after upload. -n CNAME, --cname CNAME Study contact name. Default: \"Abacus support\" -c CERTCHAIN, --certchain CERTCHAIN Certificate chain PEM: use if SSL issues are present. The PEM chain must be downloaded with a browser. Default: None -e EMAIL, --email EMAIL Dataverse study contact email address. Default: abacus-support@lists.ubc.ca -v, --verbose Verbose output --version Show version number and exit dv_list_files \u00b6 This utility will produce a csv, tsv or JSON output showing the path/name of a file description the file download page URL the API URL to download the file the version of the file (ie, the study version) the state of the version. Unless specified, the most current version is shown. If an API key is supplied and a draft version exists, then the most current version is considered the draft version. usage: dv_list_files [-h] [-u URL] [-o {csv,tsv,json}] [-a] [-k KEY] [-f FILE] [-v] pid This will parse a Dataverse record and show the path, filename, descriptions and download information for a Dataverse record. An API key is required for DRAFT versions. positional arguments: pid Dataverse study persistent identifier (DOI/handle) options: -h, --help show this help message and exit -u, --url URL Dataverse installation base url. Defaults to \"https://abacus.library.ubc.ca\" -o, --output {csv,tsv,json} Output format. One of csv, tsv, or json. Default tsv because descriptions often contain commas -a, --all Show info for *all* versions, not just most current -k, --key KEY API key; required for restricted or draft data sets -f, --file FILE Dump output to FILE -v, --version Show version number and exit dv_manifest_gen \u00b6 Not technically a Dataverse-specific script, this utility will generate a tab-separated value output. The file consists of 3 columns: file, description and tags , and optionally a mimetype column. Editing the result and using the upload utility to parse the tsv will add descriptive metadata, tags and file paths to an upload instead of laboriously using the Dataverse GUI. Tags may be separated by commas, eg: \u201cData, SAS, June 2021\u201d. Using stdout and a redirect will also save time. First dump a file as normal. Add other files to the end with different information using the exclude header switch -x and different tags along with output redirection >> . Usage usage: dv_manifest_gen [-h] [-f FILENAME] [-t TAG] [-x] [-r] [-q QUOTE] [-a] [-m] [-p] [--version] [files ...] Creates a file manifest in tab separated value format which can then be edited and used for file uploads to a Dataverse collection. Files can be edited to add file descriptions and comma-separated tags that will be automatically attached to metadata using products using the dataverse_utils library. Will dump to stdout unless -f or --filename is used. Using the command and a dash (ie, \"dv_manifest_gen.py -\" produces full paths for some reason. positional arguments: files Files to add to manifest. Leaving it blank will add all files in the current directory. If using -r will recursively show all. options: -h, --help show this help message and exit -f FILENAME, --filename FILENAME Save to file instead of outputting to stdout -t TAG, --tag TAG Default tag(s). Separate with comma and use quotes if there are spaces. eg. \"Data, June 2021\". Defaults to \"Data\" -x, --no-header Don't include header in output. Useful if creating a complex tsv using redirects (ie, \">>\"). -r, --recursive Recursive listing. -q QUOTE, --quote QUOTE Quote type. Cell value quoting parameters. Options: none (no quotes), min (minimal, ie. special characters only )nonum (non-numeric), all (all cells). Default: min -a, --show-hidden Include hidden files. -m, --mime Include autodetected mimetypes -p, --path Include an optional path column for custom file paths --version Show version number and exit dv_pg_facet_date \u00b6 This specialized tool is designed to be run on the server on which the Dataverse installation exists. When material is published in a Dataverse installation, the \u201cPublication Year\u201d facet in the Dataverse GUI is automatically populated with a date, which is the publication date in that Dataverse installation . This makes sense from the point of view of research data which is first deposited into a Dataverse installation, but fails as a finding aid for either; older data sets that have been migrated and reingested licensed data sets which may have been published years before they were purchased and ingested. For example, if you have a dataset that was published in 1971 but you only added it to your Dataverse installation in 2021, it is not necessarily intuitive to the end user that the \u201cpublication date\u201d in this instance would be 2021. Ideally, you might like it to be 1971. Unfortunately, there is no API-based tool to manage this date. The only way to change it, as of late 2021, is to modify the underlying PostgreSQL database directly with the desired date. Subsequently, the study must be reindexed so that the revised publication date appears as an option in the facet. This tool will perform those operations. However, the tool must be run on the server on which the Dataverse installation exists, as reindexing API calls must be from localhost and database access is necessarily restricted. There are a few other prerequisites for using this tool which differ from the rest of the scripts included in this package. The user must have shell access to the server hosting the Dataverse installation Python 3.6 or higher must be installed The user must possess a valid Dataverse API key The user must know the PostgreSQL password If the database name and user have been changed, the user must know this as well The script requires the manual installation of psycopg2-binary or have a successfully compiled psycopg2 package for Python. See https://www.psycopg.org/docs/ . This is not installed with the normal pip install of the dataverse_utils package as none of the other scripts require it and, in general, the odds of someone using this utility are low. If you forget to install it, the program will politely remind you. This cannot be stressed enough. This tool will directly change values within the PostgreSQL database which holds all of Dataverse\u2019s information . Use this at your own risk; no warranty is implied and no responsibility will be accepted for data loss, etc. If any of the options listed for the utility make no sense to you or sound like gibberish, do not use this tool. Because editing the underlying database may have a high pucker factor for some, there is both a dry-run option and an option to just dump out SQL instead of actually touching anything. These two options do not perform a study reindex and don\u2019t alter the contents of the database. Usage usage: dv_pg_facet_date [-h] [-d DBNAME] [-u USER] -p PASSWORD [-r | -o] [-s] -k KEY [-w URL] [--version] pids [pids ...] {distributionDate,productionDate,dateOfDeposit,dist,prod,dep} A utility to change the 'Production Date' web interface facet in a Dataverse installation to one of the three acceptable date types: 'distributionDate', 'productionDate', or 'dateOfDeposit'. This must be done in the PostgreSQL database directly, so this utility must be run on the *server* that hosts a Dataverse installation. Back up your database if you are unsure. positional arguments: pids persistentIdentifier {distributionDate,productionDate,dateOfDeposit,dist,prod,dep} date type which is to be shown in the facet. The short forms are aliases for the long forms. optional arguments: -h, --help show this help message and exit -d DBNAME, --dbname DBNAME Database name -u USER, --user USER PostgreSQL username -p PASSWORD, --password PASSWORD PostgreSQL password -r, --dry-run print proposed SQL to stdout -o, --sql-only dump sql to file called *pg_sql.sql* in current directory. Appends to file if it exists -s, --save-old Dump old values to tsv called *pg_changed.tsv* in current directory. Appends to file if it exists -k KEY, --key KEY API key for Dataverse installation. -w URL, --url URL URL for base Dataverse installation. Default https://abacus.library.ubc.ca --version Show version number and exit THIS WILL EDIT YOUR POSTGRESQL DATABASE DIRECTLY. USE AT YOUR OWN RISK. dv_readme_creator \u00b6 Creates a basic data documentation from the metadata already existing in a Dataverse study. This includes a rudimentary data dictionary for plain text .csv/.tsv, SAS .sas7bdat, SPSS .sav, Stata .dta and R .Rdata/.rda files). The readme will include all fields from a record, and will concatenate repeating fields and fields with multiple components such as author fields. Output is in either Markdown/plain text format (for subsequent editing) or PDF. Note that .md and .txt output are identical, just have different extensions. The README creator is intended to be a framework for enhancing the documentation, not an end in itself. But incomplete documentation is still better than no documentation. Usage usage: dv_readme_creator [-h] [-u URL] -p PID -k KEY [-v] outfile Creates a README file from a Dataverse study, in Markdown or PDF format. An API is *required* to use this utility, as it must work with draft studies. Example command: dv_readme_creator -p doi:12.2345/PRE/ZYX9876 -u test.invalid -k 00000000-0000-0000-0000-000000000000 test.md If using Borealis, -u switch is not required. positional arguments: outfile Output file. File extension must be one of .pdf, .md or .txt (case insensitive). options: -h, --help show this help message and exit -u, --url URL Dataverse installation base url. Defaults to \"borealisdata.ca\" -p, --pid PID Persistent ID of study (ie, doi or hdl). format: doi: doi:12.2345/PRE/ZYX9876 -k, --key KEY API key -v, --version Show version number and exit dv_record_copy \u00b6 Copies an existing Dataverse study metadata record to a target collection, or replaces a currently existing record. Files are not copied, only the study record. This utility is useful for mateial which is in a series, requiring only minor changes for each iteration. Usage usage: dv_record_copy [-h] [-u URL] -k KEY (-c COLLECTION | -r REPLACE) [-v] pid Record duplicator for Dataverse. This utility will download a Dataverse record And then upload the study level metadata into a new record in a user-specified collection. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: pid PID of original dataverse recordseparated by spaces. eg. \"hdl:11272.1/AB2/NOMATH hdl:11272.1/AB2/HANDLE\". Case is ignored, so \"hdl:11272.1/ab2/handle\" will also work. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -c COLLECTION, --collection COLLECTION Short name of target Dataverse collection (eg: ldc). Defaults to \"statcan-public\" -r REPLACE, --replace REPLACE Replace metadata data in record with this PID -v, --version Show version number and exit dv_release \u00b6 A bulk release utility for Dataverse. This utility will normally be used after a migration or large data transfer, such as a dryad2dataverse transfer from the Dryad data repository. It can release studies individually by persistent ID or just release all unreleased files in a Dataverse. Usage usage: dv_release [-h] [-u URL] -k KEY [-i] [--time STIME] [-v] [-r] [-d DV | -p PID [PID ...]] [--version] Bulk file releaser for unpublished Dataverse studies. Either releases individual studies or all unreleased studies in a single Dataverse collection. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Default: https://abacus.library.ubc.ca -k KEY, --key KEY API key -i, --interactive Manually confirm each release --time STIME, -t STIME Time between release attempts in seconds. Default 10 -v Verbose mode -r, --dry-run Only output a list of studies to be released -d DV, --dv DV Short name of Dataverse collection to process (eg: statcan) -p PID [PID ...], --pid PID [PID ...] Handles or DOIs to release in format hdl:11272.1/FK2/12345 or doi:10.80240/FK2/NWRABI. Multiple values OK --version Show version number and exit dv_replace_licen[cs]e \u00b6 This will replace the text in a record with the text Markdown file. Text is converted to HTML. Optionally, the record can be republished without incrementing the version (ie, with type=updatecurrent . Deprecation warning dv_replace_license will be removed in future releases to conform to Canadian English standards. usage: dv_replace_licence [-h] [-u URL] -l LIC -k KEY [-r] [--version] studies [studies ...] Replaces the licence text in a Dataverse study and [optionally] republishes it as the same version. Superuser privileges are required for republishing as the version is not incremented. This software requires the Dataverse installation to be running Dataverse software version >= 5.6. positional arguments: studies Persistent IDs of studies options: -h, --help show this help message and exit -u URL, --url URL Base URL of Dataverse installation. Defaults to \"https://abacus.library.ubc.ca\" -l LIC, --licence LIC Licence file in Markdown format -k KEY, --key KEY Dataverse API key -r, --republish Republish study without incrementing version --version Show version number and exit dv_study_migrator \u00b6 If for some reason you need to copy everything from a Dataverse record to a different Dataverse installation or a different collection, this utility will do it for you. Metadata, file names, paths, restrictions etc will all be copied. There are some limitations, though, as only the most recent version will be copied and date handling is done on the target server. The utility will either copy records specifice with a persistent identifer (PID) to a target collection on the same or another server, or replace records with an existing PID. usage: dv_study_migrator [-h] -s SOURCE_URL -a SOURCE_KEY -t TARGET_URL -b TARGET_KEY [-o TIMEOUT] (-c COLLECTION | -r REPLACE [REPLACE ...]) [-v] pids [pids ...] Record migrator for Dataverse. This utility will take the most recent version of a study from one Dataverse installation and copy the metadata and records to another, completely separate dataverse installation. You could also use it to copy records from one collection to another. positional arguments: pids PID(s) of original Dataverse record(s) in source Dataverse separated by spaces. eg. \"hdl:11272.1/AB2/JEG5RH doi:11272.1/AB2/JEG5RH\". Case is ignored. options: -h, --help show this help message and exit -s SOURCE_URL, --source_url SOURCE_URL Source Dataverse installation base URL. -a SOURCE_KEY, --source_key SOURCE_KEY API key for source Dataverse installation. -t TARGET_URL, --target_url TARGET_URL Source Dataverse installation base URL. -b TARGET_KEY, --target_key TARGET_KEY API key for target Dataverse installation. -o TIMEOUT, --timeout TIMEOUT Request timeout in seconds. Default 100. -c COLLECTION, --collection COLLECTION Short name of target Dataverse collection (eg: dli). -r REPLACE [REPLACE ...], --replace REPLACE [REPLACE ...] Replace data in these target PIDs with data from the source PIDS. Number of PIDs listed here must match the number of PID arguments to follow. That is, the number of records must be equal. Records will be matched on a 1-1 basis in order. For example: [rest of command] -r doi:123.34/etc hdl:12323/AB/SOMETHI will replace the record with identifier 'doi' with the data from 'hdl'. Make sure you don't use this as the penultimate switch, because then it's not possible to disambiguate PIDS from this argument and positional arguments. ie, something like dv_study_migrator -r blah blah -s http//test.invalid etc. -v, --version Show version number and exit dv_upload_tsv \u00b6 Now that you have a tsv full of nicely described data, you can easily upload it to an existing study if you know the persistent ID and have an API key. For the best metadata, you should probably edit it manually to add correct descriptive metadata, notably the \u201cDescription\u201d and \u201cTags\u201d. Tags are split separated by commas, so it\u2019s possible to have multiple tags for each data item, like \u201cData, SPSS, June 2021\u201d. File paths are automatically generated from the \u201cfile\u201d column. Because of this, you should probably use relative paths rather than absolute paths unless you want to have a lengthy path string in Dataverse. If uploading a tsv which includes mimetypes, be aware that mimetypes for zip files will be ignored to circumvent Dataverse\u2019s automatic unzipping feature. The rationale for manually specifiying mimetypes is to enable the use of previews which require a specific mimetype to function, but Dataverse does not correctly detect the type. For example, the GeoJSON file previewer requires a mimetype of application/geo+json , but the detection of this mimetype is not supported until Dataverse v5.9. By manually setting the mimetype, the previewer can be used by earlier Dataverse versions. Usage usage: dv_upload_tsv [-h] -p PID -k KEY [-u URL] [-r] [-n] [-t TRUNCATE] [-o] [-v] tsv Uploads data sets to an *existing* Dataverse study from the contents of a TSV (tab separated value) file. Metadata, file tags, paths, etc are all read from the TSV. JSON output from the Dataverse API is printed to stdout during the process. By default, files will be unrestricted but the utility will ask for confirmation before uploading. positional arguments: tsv TSV file to upload options: -h, --help show this help message and exit -p PID, --pid PID Dataverse study persistent identifier (DOI/handle) -k KEY, --key KEY API key -u URL, --url URL Dataverse installation base url. defaults to \"https://abacus.library.ubc.ca\" -r, --restrict Restrict files after upload. -n, --no-confirm Don't confirm non-restricted status -t TRUNCATE, --truncate TRUNCATE Left truncate file path. As Dataverse studies can retain directory structure, you can set an arbitrary starting point by removing the leftmost portion. Eg: if the TSV has a file path of /home/user/Data/file.txt, setting --truncate to \"/home/user\" would have file.txt in the Data directory in the Dataverse study. The file is still loaded from the path in the spreadsheet. Defaults to no truncation. -o, --override Disables replacement of mimetypes for Dataverse- processable files. That is, files such as Excel, SPSS, etc, will have their actual mimetypes sent instead of 'application/octet-stream'. Useful when mimetypes are specified in the TSV file and the upload mimetype is not the expected result. -v, --version Show version number and exit Notes for Windows users \u00b6 Command line scripts for Python may not necessarily behave the way they do in Linux/Mac, depending on how you access them. For detailed information on Windows systems, please see the Windows testing document","title":"Console utilities"},{"location":"scripts/#console-utilities","text":"code { white-space : pre-wrap !important; } These utilities are available at the command line/command prompt and don\u2019t require any Python knowledge except how to install a Python library via pip, as outlined in the overview document. Once installed via pip, the scripts will be available via the command line and will not require calling Python explicitly. That is, they can be called from the command line directly. For example: dv_manifest_gen [parameters] is all you will need to type. Note that these programs have been primarily tested on Linux and MacOS, with Windows a tertiary priority . Windows is notable for its unusual file handling, so, as the MIT licence stipulates, there is no warranty as to the suitability for a particular purpose. In alphabetical order:","title":"Console utilities"},{"location":"scripts/#dv_collection_info","text":"A recursive file metadata utility. You can specify the head of a tree and the harvester will harvest the [latest] study metadata and output it as a spreadsheet. An API key is not required for publicly accessible data. usage: dv_collection_info [-h] [-u URL] -c COLLECTION [-k KEY] [-d DELIMITER] [-f [FIELDS ...]] [-o OUTPUT] [--verbose] [-v] Recursively parses a dataverse collection and outputs study metadata for the latest version. If analyzing publicly available collections, a dataverse API key for the target system is not required. options: -h, --help show this help message and exit -u, --url URL Dataverse installation base url. defaults to \"https://abacus.library.ubc.ca\" -c, --collection COLLECTION Dataverse collection shortname or id at the top of the tree -k, --key KEY API key -d, --delimiter DELIMITER Delimiter for output spreadsheet. Default: tab (\\t) -f, --fields [FIELDS ...] Record metadata fields to output. For all fields, use \"all\". Default: title, author. -o, --output OUTPUT Output file name. --verbose Verbose output. See what's happening. -v, --version Show version number and exit","title":"dv_collection_info"},{"location":"scripts/#dv_del","text":"This is bulk deletion utility for unpublished studies (or even single studies). It\u2019s useful when your automated procedures have gone wrong, or if you don\u2019t feel like navigating through many menus. Note the -i switch which can ask for manual confirmation of deletions. Usage usage: dv_del [-h] -k KEY [-d DATAVERSE | -p PID] [-i] [-u DVURL] [--version] Delete draft studies from a Dataverse collection options: -h, --help show this help message and exit -k KEY, --key KEY Dataverse user API key -d DATAVERSE, --dataverse DATAVERSE Dataverse collection short name from which to delete all draft records. eg. \"ldc\" -p PID, --persistentId PID Handle or DOI to delete in format hdl:11272.1/FK2/12345 -i, --interactive Confirm each study deletion -u DVURL, --url DVURL URL to base Dataverse installation --version Show version number and exit","title":"dv_del"},{"location":"scripts/#dv_ldc_uploader","text":"This is a very specialized utility which will scrape metadata from the Linguistic Data Consortium (LDC) and create a metadata record in a Dataverse. The LDC does not have an API, so the metadata is scraped from their web site. This means that the metadata may not be quite as controlled as that which comes from an API. Data from the LDC website is converted to Dryad -style JSON via dataverse_utils.ldc via the use of the dryad2dataverse library. There are two main methods of use for this utility: Multiple metadata uploads. Multiple LDC record numbers can be supplied and a study without files will be created for each one. If a TSV file with file information is upplied via the -t or --tsv switch, the utility will upload a single LDC study and upload the contents of the tsv file to the created record.","title":"dv_ldc_uploader"},{"location":"scripts/#certificate-chain-issues","text":"Update of the certificate chain issue: 2024-09 The problem listed below seems to have resolved itself by September 2024. It\u2019s not clear whether this was a certifi issue or an issue with LDC\u2019s certificates. In any case, if you are having problems with LDC website, use the -c switch and follow the procedure below. As of early 2023, the LDC website is not supported by certifi . You will need to manually supply a certificate chain to use the utility. To obtain the certificate chain (in Firefox) perform the following steps: Select Tools/Page Info In the Security tab, select View Certificate Scroll to \u201cPEM (chain)\u201d Right click and \u201cSave link as\u201d Use this file for the -c/\u2013certchain option below. Searching for \u201cdownload pem certificate chain [browser]\u201d in a search engine will undoubtedly bring up results for whatever browser you like. Usage usage: dv_ldc_uploader [-h] [-u URL] -k KEY [-d DVS] [-t TSV] [-r] [-n CNAME] [-c CERTCHAIN] [-e EMAIL] [-v] [--version] studies [studies ...] Linguistic Data Consortium metadata uploader for Dataverse. This utility will scrape the metadata from the LDC website (https://catalog.ldc.upenn.edu) and upload data based on a TSV manifest. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: studies LDC Catalogue numbers to process, separated by spaces. eg. \"LDC2012T19 LDC2011T07\". Case is ignored, so \"ldc2012T19\" will also work. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -d DVS, --dvs DVS Short name of target Dataverse collection (eg: ldc). Defaults to \"ldc\" -t TSV, --tsv TSV Manifest tsv file for uploading and metadata. If not supplied, only metadata will be uploaded. Using this option requires only one positional *studies* argument -r, --no-restrict Don't restrict files after upload. -n CNAME, --cname CNAME Study contact name. Default: \"Abacus support\" -c CERTCHAIN, --certchain CERTCHAIN Certificate chain PEM: use if SSL issues are present. The PEM chain must be downloaded with a browser. Default: None -e EMAIL, --email EMAIL Dataverse study contact email address. Default: abacus-support@lists.ubc.ca -v, --verbose Verbose output --version Show version number and exit","title":"Certificate chain issues"},{"location":"scripts/#dv_list_files","text":"This utility will produce a csv, tsv or JSON output showing the path/name of a file description the file download page URL the API URL to download the file the version of the file (ie, the study version) the state of the version. Unless specified, the most current version is shown. If an API key is supplied and a draft version exists, then the most current version is considered the draft version. usage: dv_list_files [-h] [-u URL] [-o {csv,tsv,json}] [-a] [-k KEY] [-f FILE] [-v] pid This will parse a Dataverse record and show the path, filename, descriptions and download information for a Dataverse record. An API key is required for DRAFT versions. positional arguments: pid Dataverse study persistent identifier (DOI/handle) options: -h, --help show this help message and exit -u, --url URL Dataverse installation base url. Defaults to \"https://abacus.library.ubc.ca\" -o, --output {csv,tsv,json} Output format. One of csv, tsv, or json. Default tsv because descriptions often contain commas -a, --all Show info for *all* versions, not just most current -k, --key KEY API key; required for restricted or draft data sets -f, --file FILE Dump output to FILE -v, --version Show version number and exit","title":"dv_list_files"},{"location":"scripts/#dv_manifest_gen","text":"Not technically a Dataverse-specific script, this utility will generate a tab-separated value output. The file consists of 3 columns: file, description and tags , and optionally a mimetype column. Editing the result and using the upload utility to parse the tsv will add descriptive metadata, tags and file paths to an upload instead of laboriously using the Dataverse GUI. Tags may be separated by commas, eg: \u201cData, SAS, June 2021\u201d. Using stdout and a redirect will also save time. First dump a file as normal. Add other files to the end with different information using the exclude header switch -x and different tags along with output redirection >> . Usage usage: dv_manifest_gen [-h] [-f FILENAME] [-t TAG] [-x] [-r] [-q QUOTE] [-a] [-m] [-p] [--version] [files ...] Creates a file manifest in tab separated value format which can then be edited and used for file uploads to a Dataverse collection. Files can be edited to add file descriptions and comma-separated tags that will be automatically attached to metadata using products using the dataverse_utils library. Will dump to stdout unless -f or --filename is used. Using the command and a dash (ie, \"dv_manifest_gen.py -\" produces full paths for some reason. positional arguments: files Files to add to manifest. Leaving it blank will add all files in the current directory. If using -r will recursively show all. options: -h, --help show this help message and exit -f FILENAME, --filename FILENAME Save to file instead of outputting to stdout -t TAG, --tag TAG Default tag(s). Separate with comma and use quotes if there are spaces. eg. \"Data, June 2021\". Defaults to \"Data\" -x, --no-header Don't include header in output. Useful if creating a complex tsv using redirects (ie, \">>\"). -r, --recursive Recursive listing. -q QUOTE, --quote QUOTE Quote type. Cell value quoting parameters. Options: none (no quotes), min (minimal, ie. special characters only )nonum (non-numeric), all (all cells). Default: min -a, --show-hidden Include hidden files. -m, --mime Include autodetected mimetypes -p, --path Include an optional path column for custom file paths --version Show version number and exit","title":"dv_manifest_gen"},{"location":"scripts/#dv_pg_facet_date","text":"This specialized tool is designed to be run on the server on which the Dataverse installation exists. When material is published in a Dataverse installation, the \u201cPublication Year\u201d facet in the Dataverse GUI is automatically populated with a date, which is the publication date in that Dataverse installation . This makes sense from the point of view of research data which is first deposited into a Dataverse installation, but fails as a finding aid for either; older data sets that have been migrated and reingested licensed data sets which may have been published years before they were purchased and ingested. For example, if you have a dataset that was published in 1971 but you only added it to your Dataverse installation in 2021, it is not necessarily intuitive to the end user that the \u201cpublication date\u201d in this instance would be 2021. Ideally, you might like it to be 1971. Unfortunately, there is no API-based tool to manage this date. The only way to change it, as of late 2021, is to modify the underlying PostgreSQL database directly with the desired date. Subsequently, the study must be reindexed so that the revised publication date appears as an option in the facet. This tool will perform those operations. However, the tool must be run on the server on which the Dataverse installation exists, as reindexing API calls must be from localhost and database access is necessarily restricted. There are a few other prerequisites for using this tool which differ from the rest of the scripts included in this package. The user must have shell access to the server hosting the Dataverse installation Python 3.6 or higher must be installed The user must possess a valid Dataverse API key The user must know the PostgreSQL password If the database name and user have been changed, the user must know this as well The script requires the manual installation of psycopg2-binary or have a successfully compiled psycopg2 package for Python. See https://www.psycopg.org/docs/ . This is not installed with the normal pip install of the dataverse_utils package as none of the other scripts require it and, in general, the odds of someone using this utility are low. If you forget to install it, the program will politely remind you. This cannot be stressed enough. This tool will directly change values within the PostgreSQL database which holds all of Dataverse\u2019s information . Use this at your own risk; no warranty is implied and no responsibility will be accepted for data loss, etc. If any of the options listed for the utility make no sense to you or sound like gibberish, do not use this tool. Because editing the underlying database may have a high pucker factor for some, there is both a dry-run option and an option to just dump out SQL instead of actually touching anything. These two options do not perform a study reindex and don\u2019t alter the contents of the database. Usage usage: dv_pg_facet_date [-h] [-d DBNAME] [-u USER] -p PASSWORD [-r | -o] [-s] -k KEY [-w URL] [--version] pids [pids ...] {distributionDate,productionDate,dateOfDeposit,dist,prod,dep} A utility to change the 'Production Date' web interface facet in a Dataverse installation to one of the three acceptable date types: 'distributionDate', 'productionDate', or 'dateOfDeposit'. This must be done in the PostgreSQL database directly, so this utility must be run on the *server* that hosts a Dataverse installation. Back up your database if you are unsure. positional arguments: pids persistentIdentifier {distributionDate,productionDate,dateOfDeposit,dist,prod,dep} date type which is to be shown in the facet. The short forms are aliases for the long forms. optional arguments: -h, --help show this help message and exit -d DBNAME, --dbname DBNAME Database name -u USER, --user USER PostgreSQL username -p PASSWORD, --password PASSWORD PostgreSQL password -r, --dry-run print proposed SQL to stdout -o, --sql-only dump sql to file called *pg_sql.sql* in current directory. Appends to file if it exists -s, --save-old Dump old values to tsv called *pg_changed.tsv* in current directory. Appends to file if it exists -k KEY, --key KEY API key for Dataverse installation. -w URL, --url URL URL for base Dataverse installation. Default https://abacus.library.ubc.ca --version Show version number and exit THIS WILL EDIT YOUR POSTGRESQL DATABASE DIRECTLY. USE AT YOUR OWN RISK.","title":"dv_pg_facet_date"},{"location":"scripts/#dv_readme_creator","text":"Creates a basic data documentation from the metadata already existing in a Dataverse study. This includes a rudimentary data dictionary for plain text .csv/.tsv, SAS .sas7bdat, SPSS .sav, Stata .dta and R .Rdata/.rda files). The readme will include all fields from a record, and will concatenate repeating fields and fields with multiple components such as author fields. Output is in either Markdown/plain text format (for subsequent editing) or PDF. Note that .md and .txt output are identical, just have different extensions. The README creator is intended to be a framework for enhancing the documentation, not an end in itself. But incomplete documentation is still better than no documentation. Usage usage: dv_readme_creator [-h] [-u URL] -p PID -k KEY [-v] outfile Creates a README file from a Dataverse study, in Markdown or PDF format. An API is *required* to use this utility, as it must work with draft studies. Example command: dv_readme_creator -p doi:12.2345/PRE/ZYX9876 -u test.invalid -k 00000000-0000-0000-0000-000000000000 test.md If using Borealis, -u switch is not required. positional arguments: outfile Output file. File extension must be one of .pdf, .md or .txt (case insensitive). options: -h, --help show this help message and exit -u, --url URL Dataverse installation base url. Defaults to \"borealisdata.ca\" -p, --pid PID Persistent ID of study (ie, doi or hdl). format: doi: doi:12.2345/PRE/ZYX9876 -k, --key KEY API key -v, --version Show version number and exit","title":"dv_readme_creator"},{"location":"scripts/#dv_record_copy","text":"Copies an existing Dataverse study metadata record to a target collection, or replaces a currently existing record. Files are not copied, only the study record. This utility is useful for mateial which is in a series, requiring only minor changes for each iteration. Usage usage: dv_record_copy [-h] [-u URL] -k KEY (-c COLLECTION | -r REPLACE) [-v] pid Record duplicator for Dataverse. This utility will download a Dataverse record And then upload the study level metadata into a new record in a user-specified collection. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: pid PID of original dataverse recordseparated by spaces. eg. \"hdl:11272.1/AB2/NOMATH hdl:11272.1/AB2/HANDLE\". Case is ignored, so \"hdl:11272.1/ab2/handle\" will also work. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -c COLLECTION, --collection COLLECTION Short name of target Dataverse collection (eg: ldc). Defaults to \"statcan-public\" -r REPLACE, --replace REPLACE Replace metadata data in record with this PID -v, --version Show version number and exit","title":"dv_record_copy"},{"location":"scripts/#dv_release","text":"A bulk release utility for Dataverse. This utility will normally be used after a migration or large data transfer, such as a dryad2dataverse transfer from the Dryad data repository. It can release studies individually by persistent ID or just release all unreleased files in a Dataverse. Usage usage: dv_release [-h] [-u URL] -k KEY [-i] [--time STIME] [-v] [-r] [-d DV | -p PID [PID ...]] [--version] Bulk file releaser for unpublished Dataverse studies. Either releases individual studies or all unreleased studies in a single Dataverse collection. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Default: https://abacus.library.ubc.ca -k KEY, --key KEY API key -i, --interactive Manually confirm each release --time STIME, -t STIME Time between release attempts in seconds. Default 10 -v Verbose mode -r, --dry-run Only output a list of studies to be released -d DV, --dv DV Short name of Dataverse collection to process (eg: statcan) -p PID [PID ...], --pid PID [PID ...] Handles or DOIs to release in format hdl:11272.1/FK2/12345 or doi:10.80240/FK2/NWRABI. Multiple values OK --version Show version number and exit","title":"dv_release"},{"location":"scripts/#dv_replace_licencse","text":"This will replace the text in a record with the text Markdown file. Text is converted to HTML. Optionally, the record can be republished without incrementing the version (ie, with type=updatecurrent . Deprecation warning dv_replace_license will be removed in future releases to conform to Canadian English standards. usage: dv_replace_licence [-h] [-u URL] -l LIC -k KEY [-r] [--version] studies [studies ...] Replaces the licence text in a Dataverse study and [optionally] republishes it as the same version. Superuser privileges are required for republishing as the version is not incremented. This software requires the Dataverse installation to be running Dataverse software version >= 5.6. positional arguments: studies Persistent IDs of studies options: -h, --help show this help message and exit -u URL, --url URL Base URL of Dataverse installation. Defaults to \"https://abacus.library.ubc.ca\" -l LIC, --licence LIC Licence file in Markdown format -k KEY, --key KEY Dataverse API key -r, --republish Republish study without incrementing version --version Show version number and exit","title":"dv_replace_licen[cs]e"},{"location":"scripts/#dv_study_migrator","text":"If for some reason you need to copy everything from a Dataverse record to a different Dataverse installation or a different collection, this utility will do it for you. Metadata, file names, paths, restrictions etc will all be copied. There are some limitations, though, as only the most recent version will be copied and date handling is done on the target server. The utility will either copy records specifice with a persistent identifer (PID) to a target collection on the same or another server, or replace records with an existing PID. usage: dv_study_migrator [-h] -s SOURCE_URL -a SOURCE_KEY -t TARGET_URL -b TARGET_KEY [-o TIMEOUT] (-c COLLECTION | -r REPLACE [REPLACE ...]) [-v] pids [pids ...] Record migrator for Dataverse. This utility will take the most recent version of a study from one Dataverse installation and copy the metadata and records to another, completely separate dataverse installation. You could also use it to copy records from one collection to another. positional arguments: pids PID(s) of original Dataverse record(s) in source Dataverse separated by spaces. eg. \"hdl:11272.1/AB2/JEG5RH doi:11272.1/AB2/JEG5RH\". Case is ignored. options: -h, --help show this help message and exit -s SOURCE_URL, --source_url SOURCE_URL Source Dataverse installation base URL. -a SOURCE_KEY, --source_key SOURCE_KEY API key for source Dataverse installation. -t TARGET_URL, --target_url TARGET_URL Source Dataverse installation base URL. -b TARGET_KEY, --target_key TARGET_KEY API key for target Dataverse installation. -o TIMEOUT, --timeout TIMEOUT Request timeout in seconds. Default 100. -c COLLECTION, --collection COLLECTION Short name of target Dataverse collection (eg: dli). -r REPLACE [REPLACE ...], --replace REPLACE [REPLACE ...] Replace data in these target PIDs with data from the source PIDS. Number of PIDs listed here must match the number of PID arguments to follow. That is, the number of records must be equal. Records will be matched on a 1-1 basis in order. For example: [rest of command] -r doi:123.34/etc hdl:12323/AB/SOMETHI will replace the record with identifier 'doi' with the data from 'hdl'. Make sure you don't use this as the penultimate switch, because then it's not possible to disambiguate PIDS from this argument and positional arguments. ie, something like dv_study_migrator -r blah blah -s http//test.invalid etc. -v, --version Show version number and exit","title":"dv_study_migrator"},{"location":"scripts/#dv_upload_tsv","text":"Now that you have a tsv full of nicely described data, you can easily upload it to an existing study if you know the persistent ID and have an API key. For the best metadata, you should probably edit it manually to add correct descriptive metadata, notably the \u201cDescription\u201d and \u201cTags\u201d. Tags are split separated by commas, so it\u2019s possible to have multiple tags for each data item, like \u201cData, SPSS, June 2021\u201d. File paths are automatically generated from the \u201cfile\u201d column. Because of this, you should probably use relative paths rather than absolute paths unless you want to have a lengthy path string in Dataverse. If uploading a tsv which includes mimetypes, be aware that mimetypes for zip files will be ignored to circumvent Dataverse\u2019s automatic unzipping feature. The rationale for manually specifiying mimetypes is to enable the use of previews which require a specific mimetype to function, but Dataverse does not correctly detect the type. For example, the GeoJSON file previewer requires a mimetype of application/geo+json , but the detection of this mimetype is not supported until Dataverse v5.9. By manually setting the mimetype, the previewer can be used by earlier Dataverse versions. Usage usage: dv_upload_tsv [-h] -p PID -k KEY [-u URL] [-r] [-n] [-t TRUNCATE] [-o] [-v] tsv Uploads data sets to an *existing* Dataverse study from the contents of a TSV (tab separated value) file. Metadata, file tags, paths, etc are all read from the TSV. JSON output from the Dataverse API is printed to stdout during the process. By default, files will be unrestricted but the utility will ask for confirmation before uploading. positional arguments: tsv TSV file to upload options: -h, --help show this help message and exit -p PID, --pid PID Dataverse study persistent identifier (DOI/handle) -k KEY, --key KEY API key -u URL, --url URL Dataverse installation base url. defaults to \"https://abacus.library.ubc.ca\" -r, --restrict Restrict files after upload. -n, --no-confirm Don't confirm non-restricted status -t TRUNCATE, --truncate TRUNCATE Left truncate file path. As Dataverse studies can retain directory structure, you can set an arbitrary starting point by removing the leftmost portion. Eg: if the TSV has a file path of /home/user/Data/file.txt, setting --truncate to \"/home/user\" would have file.txt in the Data directory in the Dataverse study. The file is still loaded from the path in the spreadsheet. Defaults to no truncation. -o, --override Disables replacement of mimetypes for Dataverse- processable files. That is, files such as Excel, SPSS, etc, will have their actual mimetypes sent instead of 'application/octet-stream'. Useful when mimetypes are specified in the TSV file and the upload mimetype is not the expected result. -v, --version Show version number and exit","title":"dv_upload_tsv"},{"location":"scripts/#notes-for-windows-users","text":"Command line scripts for Python may not necessarily behave the way they do in Linux/Mac, depending on how you access them. For detailed information on Windows systems, please see the Windows testing document","title":"Notes for Windows users"},{"location":"windows/","text":"Running the dataverse_utils console utilities under Windows \u00b6 Using pipx \u00b6 If you are only interested in the command line utilities and don\u2019t care at all about the rest of the product (which is probably most people), install everything using pipx This will make your life much simpler. pipx install dataverse_utils You may (or may not) want or need to install for whatever shell you use (bash, command prompt, etc). Depending on what you\u2019re using one shell may be isolated from another. See the lengthy explanation below if necessary. Possibly, but not necessarily, outdated explanations \u00b6 On Mac and Linux, running the scripts supplied with dataverse_utils is straightforward. They\u2019re available at the command line, which means that you can simply run them by (for example): $ dv_manifest_gen followed by switches and variables as normal. Doing this results in output to the terminal window. This is not necessarily the case in Windows . Whether or not the utility appears on the Windows command line will likely depend on the shell you installed it with: Commonly used shells on Windows include, but are not limited to: Command prompt (cmd.exe) PowerShell Git bash Cygwin If the utilities are not functioning the way you are expecting, make sure that the pipx install location is in your PATH for whatever shell you are using. Troubleshooting \u00b6 These are (deprecated) notes from testing on a variety of Windows installations. If you\u2019re lucky, a solution can be found below. This test case uses a new installation of https://python.org Python, v.3.9.6, installed in Windows using the GUI, so it\u2019s as basic a Windows installation as you can get. In this instance, dataverse_utils were installed using pip from the PowerShell, ie pip install git+https://github.com/ubc-library-rc/dataverse_utils . However, pip should be pip regardless of which console you use to install. Here\u2019s a handy guide to show the results of how/if the command line scripts run. This example uses dv_manifest.gen.py , but appears to be generally applicable to any scripts configured with setuptools.setup() (ie, setuptools.setup(scripts=[whatever] ). If this means nothing to you because you\u2019re not distributing your own Python packages,, just skip to the table below. Windows Terminal type \u00b6 PowerShell \u00b6 Note that on these tests the user is not an administrator . Administrator results, in all likelihood, will be completely different, ideally better. Problem Attempting to run the script results in a window popping up for a nanosecond and no output. Solution This may not occur if the PowerShell is run as an administrator. What is happening here is that the script is running, but it\u2019s showing up in a separate window. Output can be created as normal, if you use the correct switches. Unfortunately, you won\u2019t be able to see what they are, because the popup window disappears, which is not helpful. There are three potential fixes. If you can run PowerShell as an administrator, that may solve the problem. Edit the PATHEXT environment variable to include the .py extension. Note that if the user PATHEXT is edited, the system will ignore the system PATHEXT, meaning that things like .exe files won\u2019t run unless the full name is typed (eg. \u2018notepad.exe\u2019). So, if editing the user PATHEXT, make sure to include the system PATHEXT and append ;.PY to the string. Edit the PATHEXT for PowerShell itself, rather than on a system wide level. Editing $PROFILE to include the .py extension should allow the Python script to run within PowerShell. For instructions, see https://docs.microsoft.com/en-ca/powershell/module/microsoft.powershell.core/about/about_profiles?view=powershell-7.1 . Create a profile as per How to create a profile Within that profile, $env:PATHEXT += \";.py\" Depending on the nature of your Windows installation, this may be disabled by the security policy, in which case you can also try the method above. Command prompt \u00b6 This is the traditional Windows command prompt (ie, cmd.exe ). The scripts should just work after being installed with pip, as this installation is the default. For example, run with: C:\\>dv_manifest_gen.py [arguments] Obviously it don\u2019t type the C:\\> part. SSH session \u00b6 If using the built-in Windows SSH server, scripts should simply run as per the command prompt above. Windows SSH sessions default to using the Windows command prompt, not bash. Bash sessions under SSH should function if Bash is configured as below. Git Bash \u00b6 Git Bash is part of the git suite available here: https://git-scm.com/downloads . There are a few notable wrinkles with for use with Python. During installation of Git After v2.32.0.2 (and possibly earlier), you will have the option during the installation to \u201cEnable experimental support for pseudo consoles\u201d. Doing this will allow you run Python directly from the bash shell like you would normally, and the scripts should function as per the command prompt above. As a bonus, enabling this feature seems to fix errors with pipes which formerly resulted in the stdout is not a tty error when piping shell output (for instance, to grep ). If you have not checked this box, you will need to add an alias to your .bashrc and/or .bash_profile : alias python='winpty python' alias pip='winpty pip' Either that, or you will need to start Python with winpty python , which is annoying. Similarly winpty pip . Even if you have not enabled pseudo-console support and didn\u2019t complete use option 2, the scripts should still function normally though. Having scripts work but Python not work is not optimal and confusing, so a solution is there even though it technically isn\u2019t required. There are many options for Git Bash installation; testing has not covered all possible permutations of installation options.","title":"Windows notes"},{"location":"windows/#running-the-dataverse_utils-console-utilities-under-windows","text":"","title":"Running the dataverse_utils console utilities under Windows"},{"location":"windows/#using-pipx","text":"If you are only interested in the command line utilities and don\u2019t care at all about the rest of the product (which is probably most people), install everything using pipx This will make your life much simpler. pipx install dataverse_utils You may (or may not) want or need to install for whatever shell you use (bash, command prompt, etc). Depending on what you\u2019re using one shell may be isolated from another. See the lengthy explanation below if necessary.","title":"Using pipx"},{"location":"windows/#possibly-but-not-necessarily-outdated-explanations","text":"On Mac and Linux, running the scripts supplied with dataverse_utils is straightforward. They\u2019re available at the command line, which means that you can simply run them by (for example): $ dv_manifest_gen followed by switches and variables as normal. Doing this results in output to the terminal window. This is not necessarily the case in Windows . Whether or not the utility appears on the Windows command line will likely depend on the shell you installed it with: Commonly used shells on Windows include, but are not limited to: Command prompt (cmd.exe) PowerShell Git bash Cygwin If the utilities are not functioning the way you are expecting, make sure that the pipx install location is in your PATH for whatever shell you are using.","title":"Possibly, but not necessarily, outdated explanations"},{"location":"windows/#troubleshooting","text":"These are (deprecated) notes from testing on a variety of Windows installations. If you\u2019re lucky, a solution can be found below. This test case uses a new installation of https://python.org Python, v.3.9.6, installed in Windows using the GUI, so it\u2019s as basic a Windows installation as you can get. In this instance, dataverse_utils were installed using pip from the PowerShell, ie pip install git+https://github.com/ubc-library-rc/dataverse_utils . However, pip should be pip regardless of which console you use to install. Here\u2019s a handy guide to show the results of how/if the command line scripts run. This example uses dv_manifest.gen.py , but appears to be generally applicable to any scripts configured with setuptools.setup() (ie, setuptools.setup(scripts=[whatever] ). If this means nothing to you because you\u2019re not distributing your own Python packages,, just skip to the table below.","title":"Troubleshooting"},{"location":"windows/#windows-terminal-type","text":"","title":"Windows Terminal type"},{"location":"windows/#powershell","text":"Note that on these tests the user is not an administrator . Administrator results, in all likelihood, will be completely different, ideally better. Problem Attempting to run the script results in a window popping up for a nanosecond and no output. Solution This may not occur if the PowerShell is run as an administrator. What is happening here is that the script is running, but it\u2019s showing up in a separate window. Output can be created as normal, if you use the correct switches. Unfortunately, you won\u2019t be able to see what they are, because the popup window disappears, which is not helpful. There are three potential fixes. If you can run PowerShell as an administrator, that may solve the problem. Edit the PATHEXT environment variable to include the .py extension. Note that if the user PATHEXT is edited, the system will ignore the system PATHEXT, meaning that things like .exe files won\u2019t run unless the full name is typed (eg. \u2018notepad.exe\u2019). So, if editing the user PATHEXT, make sure to include the system PATHEXT and append ;.PY to the string. Edit the PATHEXT for PowerShell itself, rather than on a system wide level. Editing $PROFILE to include the .py extension should allow the Python script to run within PowerShell. For instructions, see https://docs.microsoft.com/en-ca/powershell/module/microsoft.powershell.core/about/about_profiles?view=powershell-7.1 . Create a profile as per How to create a profile Within that profile, $env:PATHEXT += \";.py\" Depending on the nature of your Windows installation, this may be disabled by the security policy, in which case you can also try the method above.","title":"PowerShell"},{"location":"windows/#command-prompt","text":"This is the traditional Windows command prompt (ie, cmd.exe ). The scripts should just work after being installed with pip, as this installation is the default. For example, run with: C:\\>dv_manifest_gen.py [arguments] Obviously it don\u2019t type the C:\\> part.","title":"Command prompt"},{"location":"windows/#ssh-session","text":"If using the built-in Windows SSH server, scripts should simply run as per the command prompt above. Windows SSH sessions default to using the Windows command prompt, not bash. Bash sessions under SSH should function if Bash is configured as below.","title":"SSH session"},{"location":"windows/#git-bash","text":"Git Bash is part of the git suite available here: https://git-scm.com/downloads . There are a few notable wrinkles with for use with Python. During installation of Git After v2.32.0.2 (and possibly earlier), you will have the option during the installation to \u201cEnable experimental support for pseudo consoles\u201d. Doing this will allow you run Python directly from the bash shell like you would normally, and the scripts should function as per the command prompt above. As a bonus, enabling this feature seems to fix errors with pipes which formerly resulted in the stdout is not a tty error when piping shell output (for instance, to grep ). If you have not checked this box, you will need to add an alias to your .bashrc and/or .bash_profile : alias python='winpty python' alias pip='winpty pip' Either that, or you will need to start Python with winpty python , which is annoying. Similarly winpty pip . Even if you have not enabled pseudo-console support and didn\u2019t complete use option 2, the scripts should still function normally though. Having scripts work but Python not work is not optimal and confusing, so a solution is there even though it technically isn\u2019t required. There are many options for Git Bash installation; testing has not covered all possible permutations of installation options.","title":"Git Bash"}]}