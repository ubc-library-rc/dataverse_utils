{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dataverse utilities \u00b6 This is a generalized set of utilities which help with managing Dataverse repositories. These utilities are ad-hoc , written as needed and subject to change. That being said, the effort is being made to make this a useful library. Source code (and this documentation) is available at the Github repository https://github.com/ubc-library-rc/dataverse_utils , and the user-friendly version of the documentation is at https://ubc-library-rc.github.io/dataverse_utils . Installation \u00b6 Any installation will require the use of the command line/command prompt. The easiest installation is with pip : pip install git+https://github.com/ubc-library-rc/dataverse_utils Other methods of installing Python packages can be found at https://packaging.python.org/tutorials/installing-packages/ . If you have mkdocs installed, you can view the documentation in a web browser by running mkdocs from the top level directory of the library by running mkdocs serve . The components \u00b6 Scripts \u00b6 There are five (5) scripts currently available. dv_del.py : Bulk (unpublished) file deletion utility dv_ldc_uploader.py : A utility which scrapes Linguistic Data Consortium metadata from their website, converts it to Dataverse JSON and uploads it, with the possibility of including local files. dv_manifest_gen.py : Creates a simple tab-separated value format file which can be edited and then used to upload files as well as file-level metadata. Normally files will be edited after creation. dv_release.py : A bulk release utility. Either releases all the unreleased studies in a Dataverse or individually if persistent identifiers are available. dv_upload_tsv.py : Takes a tsv file in the format from dv_manifest_gen.py and does all the uploading and metadata entry. More information about these can be found on the scripts page . Python library: dataverse_utils \u00b6 The default feature set from import dataverse_utils (or, more easily, import dataverse_utils as du is designed to work with data already present locally. The idea of this portion is to create a tsv file manifest for files which are to be uploaded to a Dataverse instance. Once the manifest is created, it\u2019s edited to add descriptive metadata, file paths and tags. Then, the manifest is used to upload those files to a an existing Dataverse study. import dataverse_utils as du du.dump_tsv('.', '/Users/you/tmp/testme.tsv') [Edit the .tsv at this stage here] du.upload_from_tsv(fil='/Users/you/tmp/testme.tsv', hdl='hdl:PERSIST/ID', dv='https://dataverse.invalid' apikey='IAM-YOUR-DVERSE-APIKEY') The tsv should be edited to have a description. Tags should be separated by commas in the \u201cTags\u201d column. If you are using relative paths, make sure that the script you are using is reading from the correct location. ldc \u00b6 The ldc component represents the Linguistic Data Consortium or LDC. The ldc module is designed to harvest LDC metadata from its catalogue, convert it to Dataverse JSON, then upload it to a Dataverse installation. Once the study has been created, the general dataverse_utils module can handle the file uploading. The ldc module requires the dryad2dataverse package. Because of this, it requires a tiny bit more effort, because LDC material doesn\u2019t have the required metadata. Here\u2019s snippet that shows how it works. import dataverse_utils.ldc as ldc ldc.ds.constants.DV_CONTACT_EMAIL='iamcontact@test.invalid' ldc.ds.constants.DV_CONTACT_NAME='Generic Support Email' KEY = 'IAM-YOUR-DVERSE-APIKEY' stud = 'LDC2021T02' #LDC study number a = ldc.Ldc(stud) a.fetch_record() #Data goes into the 'ldc' dataverse info = a.upload_metadata(url='https://dataverse.invalid', key=KEY, dv='ldc') hdl = info['data']['persistentId'] with open('/Users/you/tmp/testme.tsv') as fil: du.upload_from_tsv(fil, hdl=hdl,dv='https://dataverse.invalid', apikey=KEY) Note that one method uses key and the other apikey . This is what is known as ad hoc . More information is available at the API reference . Samples \u00b6 The sample directory contains Python scripts which demonstrate the usage of the dataverse_utils library. They\u2019re not necessarily complete examples or optimized. Or even present, intially. You know, ad_hoc .","title":"Overview"},{"location":"#dataverse-utilities","text":"This is a generalized set of utilities which help with managing Dataverse repositories. These utilities are ad-hoc , written as needed and subject to change. That being said, the effort is being made to make this a useful library. Source code (and this documentation) is available at the Github repository https://github.com/ubc-library-rc/dataverse_utils , and the user-friendly version of the documentation is at https://ubc-library-rc.github.io/dataverse_utils .","title":"Dataverse utilities"},{"location":"#installation","text":"Any installation will require the use of the command line/command prompt. The easiest installation is with pip : pip install git+https://github.com/ubc-library-rc/dataverse_utils Other methods of installing Python packages can be found at https://packaging.python.org/tutorials/installing-packages/ . If you have mkdocs installed, you can view the documentation in a web browser by running mkdocs from the top level directory of the library by running mkdocs serve .","title":"Installation"},{"location":"#the-components","text":"","title":"The components"},{"location":"#scripts","text":"There are five (5) scripts currently available. dv_del.py : Bulk (unpublished) file deletion utility dv_ldc_uploader.py : A utility which scrapes Linguistic Data Consortium metadata from their website, converts it to Dataverse JSON and uploads it, with the possibility of including local files. dv_manifest_gen.py : Creates a simple tab-separated value format file which can be edited and then used to upload files as well as file-level metadata. Normally files will be edited after creation. dv_release.py : A bulk release utility. Either releases all the unreleased studies in a Dataverse or individually if persistent identifiers are available. dv_upload_tsv.py : Takes a tsv file in the format from dv_manifest_gen.py and does all the uploading and metadata entry. More information about these can be found on the scripts page .","title":"Scripts"},{"location":"#python-library-dataverse_utils","text":"The default feature set from import dataverse_utils (or, more easily, import dataverse_utils as du is designed to work with data already present locally. The idea of this portion is to create a tsv file manifest for files which are to be uploaded to a Dataverse instance. Once the manifest is created, it\u2019s edited to add descriptive metadata, file paths and tags. Then, the manifest is used to upload those files to a an existing Dataverse study. import dataverse_utils as du du.dump_tsv('.', '/Users/you/tmp/testme.tsv') [Edit the .tsv at this stage here] du.upload_from_tsv(fil='/Users/you/tmp/testme.tsv', hdl='hdl:PERSIST/ID', dv='https://dataverse.invalid' apikey='IAM-YOUR-DVERSE-APIKEY') The tsv should be edited to have a description. Tags should be separated by commas in the \u201cTags\u201d column. If you are using relative paths, make sure that the script you are using is reading from the correct location.","title":"Python library: dataverse_utils"},{"location":"#ldc","text":"The ldc component represents the Linguistic Data Consortium or LDC. The ldc module is designed to harvest LDC metadata from its catalogue, convert it to Dataverse JSON, then upload it to a Dataverse installation. Once the study has been created, the general dataverse_utils module can handle the file uploading. The ldc module requires the dryad2dataverse package. Because of this, it requires a tiny bit more effort, because LDC material doesn\u2019t have the required metadata. Here\u2019s snippet that shows how it works. import dataverse_utils.ldc as ldc ldc.ds.constants.DV_CONTACT_EMAIL='iamcontact@test.invalid' ldc.ds.constants.DV_CONTACT_NAME='Generic Support Email' KEY = 'IAM-YOUR-DVERSE-APIKEY' stud = 'LDC2021T02' #LDC study number a = ldc.Ldc(stud) a.fetch_record() #Data goes into the 'ldc' dataverse info = a.upload_metadata(url='https://dataverse.invalid', key=KEY, dv='ldc') hdl = info['data']['persistentId'] with open('/Users/you/tmp/testme.tsv') as fil: du.upload_from_tsv(fil, hdl=hdl,dv='https://dataverse.invalid', apikey=KEY) Note that one method uses key and the other apikey . This is what is known as ad hoc . More information is available at the API reference .","title":"ldc"},{"location":"#samples","text":"The sample directory contains Python scripts which demonstrate the usage of the dataverse_utils library. They\u2019re not necessarily complete examples or optimized. Or even present, intially. You know, ad_hoc .","title":"Samples"},{"location":"api_ref/","text":"API Reference \u00b6 dataverse_utils \u00b6 Generalized dataverse utilities dataverse_utils.dataverse_utils \u00b6 A collection of Dataverse utilities for file and metadata manipulation DvGeneralUploadError Objects \u00b6 class DvGeneralUploadError(Exception) Raised on non-200 URL response Md5Error Objects \u00b6 class Md5Error(Exception) Raised on md5 mismatch make_tsv \u00b6 make_tsv(start_dir, in_list=None, def_tag='Data', inc_header=True) Recurses the tree for files and produces tsv output with with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. The \u2018description\u2019 is the filename without an extension. Returns tsv as string Arguments : start_dir : str Path to start directory in_list : list Input file list. Defaults to recursive walk of current directory. def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with an easily splitable character: eg. (\u2018Data, 2016\u2019) inc_header : bool Include header row dump_tsv \u00b6 dump_tsv(start_dir, filename, in_list=None, def_tag='Data', inc_header=True) Dumps output of make_tsv manifest to a file. Arguments : start_dir : str Path to start directory in_list : list List of files for which to create manifest entries. Will default to recursive directory crawl def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with an easily splitable character: eg. (\u2018Data, 2016\u2019) inc_header : bool Include header for tsv. file_path \u00b6 file_path(fpath, trunc) Create relative file path from full path string file_path(\u2018/tmp/Data/2011/excelfile.xlsx\u2019, \u2018/tmp/\u2019) \u2018Data/2011\u2019 file_path(\u2018/tmp/Data/2011/excelfile.xlsx\u2019, \u2018/tmp\u2019) \u2018Data/2011\u2019 Arguments : fpath : str file location (ie, complete path) trunc : str rightmost portion of path to remove check_lock \u00b6 check_lock(dv_url, study, apikey) -> bool Checks study lock status; returns True if locked. Arguments : dvurl : str URL of Dataverse installation study - str Persistent ID of study apikey : str API key for user force_notab_unlock \u00b6 force_notab_unlock(study, dv_url, fid, apikey, try_uningest=True) -> int Forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Returns 0 if unlocked, file id if locked (and then unlocked). Arguments : study : str Persistent indentifer of study dv_url : str URL to base Dataverse installation fid : str File ID for file object apikey : str API key for user try_uningest : bool Try to uningest the file that was locked. - Default - True uningest_file \u00b6 uningest_file(dv_url, fid, apikey, study='n/a') Tries to uningest a file that has been ingested. Requires superuser API key. Arguments : dv_url : str URL to base Dataverse installation fid : int or str File ID of file to uningest apikey : str API key for superuser study : str Optional handle parameter for log messages upload_file \u00b6 upload_file(fpath, hdl, **kwargs) Uploads file to Dataverse study and sets file metdata and tags. Arguments : fpath : str file location (ie, complete path) hdl : str Dataverse persistent ID for study (handle or DOI) kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user descr : str OPTIONAL file description md5 : str OPTIONAL md5sum for file checking tags : list OPTIONAL list of text file tags. Eg [\u2018Data\u2019, \u2018June 2020\u2019] dirlabel : str OPTIONAL Unix style relative pathname for Dataverse file path: eg: path/to/file/ nowait : bool OPTIONAL Force a file unlock and uningest instead of waiting for processing to finish upload_from_tsv \u00b6 upload_from_tsv(fil, hdl, **kwargs) Utility for bulk uploading. Assumes fil is formatted as tsv with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. \u2018tags\u2019 field will be split on commas. Arguments : fil : filelike object Open file object or io.IOStream() hdl : str Dataverse persistent ID for study (handle or DOI) kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user dataverse_utils.ldc \u00b6 Creates dataverse JSON from Linguistic Data Consortium website page. Ldc Objects \u00b6 class Ldc(ds.Serializer) An LDC item (eg, LDC2021T01) ldcJson \u00b6 | @property | ldcJson() Returns a JSON based on the LDC web page scraping make_ldc_json \u00b6 | make_ldc_json() Returns a dict with keys created from an LDC catalogue web page. intext : str HTML page source from LDC catalogue page. name_parser \u00b6 | @staticmethod | name_parser(name) name : str A name make_dryad_json \u00b6 | make_dryad_json(ldc=None) Creates a Dryad-style dict from an LDC dictionary ldc : dict Dictionary containing LDC data find_block_index \u00b6 | @staticmethod | find_block_index(dvjson, key) Finds the index number of an item in Dataverse\u2019s idiotic JSON make_dv_json \u00b6 | make_dv_json(ldc=None) ldc : dict LDC dictionary. Defaults to self.ldcJson upload_metadata \u00b6 | upload_metadata(**kwargs) -> dict uploads metadata to dataverse kwargs: url : base url to Dataverse key : api key dv : dataverse to which it is being uploaded Returns json from connection attempt.","title":"API reference"},{"location":"api_ref/#api-reference","text":"","title":"API Reference"},{"location":"api_ref/#dataverse_utils","text":"Generalized dataverse utilities","title":"dataverse_utils"},{"location":"api_ref/#dataverse_utilsdataverse_utils","text":"A collection of Dataverse utilities for file and metadata manipulation","title":"dataverse_utils.dataverse_utils"},{"location":"api_ref/#dvgeneraluploaderror-objects","text":"class DvGeneralUploadError(Exception) Raised on non-200 URL response","title":"DvGeneralUploadError Objects"},{"location":"api_ref/#md5error-objects","text":"class Md5Error(Exception) Raised on md5 mismatch","title":"Md5Error Objects"},{"location":"api_ref/#make_tsv","text":"make_tsv(start_dir, in_list=None, def_tag='Data', inc_header=True) Recurses the tree for files and produces tsv output with with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. The \u2018description\u2019 is the filename without an extension. Returns tsv as string Arguments : start_dir : str Path to start directory in_list : list Input file list. Defaults to recursive walk of current directory. def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with an easily splitable character: eg. (\u2018Data, 2016\u2019) inc_header : bool Include header row","title":"make_tsv"},{"location":"api_ref/#dump_tsv","text":"dump_tsv(start_dir, filename, in_list=None, def_tag='Data', inc_header=True) Dumps output of make_tsv manifest to a file. Arguments : start_dir : str Path to start directory in_list : list List of files for which to create manifest entries. Will default to recursive directory crawl def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with an easily splitable character: eg. (\u2018Data, 2016\u2019) inc_header : bool Include header for tsv.","title":"dump_tsv"},{"location":"api_ref/#file_path","text":"file_path(fpath, trunc) Create relative file path from full path string file_path(\u2018/tmp/Data/2011/excelfile.xlsx\u2019, \u2018/tmp/\u2019) \u2018Data/2011\u2019 file_path(\u2018/tmp/Data/2011/excelfile.xlsx\u2019, \u2018/tmp\u2019) \u2018Data/2011\u2019 Arguments : fpath : str file location (ie, complete path) trunc : str rightmost portion of path to remove","title":"file_path"},{"location":"api_ref/#check_lock","text":"check_lock(dv_url, study, apikey) -> bool Checks study lock status; returns True if locked. Arguments : dvurl : str URL of Dataverse installation study - str Persistent ID of study apikey : str API key for user","title":"check_lock"},{"location":"api_ref/#force_notab_unlock","text":"force_notab_unlock(study, dv_url, fid, apikey, try_uningest=True) -> int Forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Returns 0 if unlocked, file id if locked (and then unlocked). Arguments : study : str Persistent indentifer of study dv_url : str URL to base Dataverse installation fid : str File ID for file object apikey : str API key for user try_uningest : bool Try to uningest the file that was locked. - Default - True","title":"force_notab_unlock"},{"location":"api_ref/#uningest_file","text":"uningest_file(dv_url, fid, apikey, study='n/a') Tries to uningest a file that has been ingested. Requires superuser API key. Arguments : dv_url : str URL to base Dataverse installation fid : int or str File ID of file to uningest apikey : str API key for superuser study : str Optional handle parameter for log messages","title":"uningest_file"},{"location":"api_ref/#upload_file","text":"upload_file(fpath, hdl, **kwargs) Uploads file to Dataverse study and sets file metdata and tags. Arguments : fpath : str file location (ie, complete path) hdl : str Dataverse persistent ID for study (handle or DOI) kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user descr : str OPTIONAL file description md5 : str OPTIONAL md5sum for file checking tags : list OPTIONAL list of text file tags. Eg [\u2018Data\u2019, \u2018June 2020\u2019] dirlabel : str OPTIONAL Unix style relative pathname for Dataverse file path: eg: path/to/file/ nowait : bool OPTIONAL Force a file unlock and uningest instead of waiting for processing to finish","title":"upload_file"},{"location":"api_ref/#upload_from_tsv","text":"upload_from_tsv(fil, hdl, **kwargs) Utility for bulk uploading. Assumes fil is formatted as tsv with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. \u2018tags\u2019 field will be split on commas. Arguments : fil : filelike object Open file object or io.IOStream() hdl : str Dataverse persistent ID for study (handle or DOI) kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user","title":"upload_from_tsv"},{"location":"api_ref/#dataverse_utilsldc","text":"Creates dataverse JSON from Linguistic Data Consortium website page.","title":"dataverse_utils.ldc"},{"location":"api_ref/#ldc-objects","text":"class Ldc(ds.Serializer) An LDC item (eg, LDC2021T01)","title":"Ldc Objects"},{"location":"api_ref/#ldcjson","text":"| @property | ldcJson() Returns a JSON based on the LDC web page scraping","title":"ldcJson"},{"location":"api_ref/#make_ldc_json","text":"| make_ldc_json() Returns a dict with keys created from an LDC catalogue web page. intext : str HTML page source from LDC catalogue page.","title":"make_ldc_json"},{"location":"api_ref/#name_parser","text":"| @staticmethod | name_parser(name) name : str A name","title":"name_parser"},{"location":"api_ref/#make_dryad_json","text":"| make_dryad_json(ldc=None) Creates a Dryad-style dict from an LDC dictionary ldc : dict Dictionary containing LDC data","title":"make_dryad_json"},{"location":"api_ref/#find_block_index","text":"| @staticmethod | find_block_index(dvjson, key) Finds the index number of an item in Dataverse\u2019s idiotic JSON","title":"find_block_index"},{"location":"api_ref/#make_dv_json","text":"| make_dv_json(ldc=None) ldc : dict LDC dictionary. Defaults to self.ldcJson","title":"make_dv_json"},{"location":"api_ref/#upload_metadata","text":"| upload_metadata(**kwargs) -> dict uploads metadata to dataverse kwargs: url : base url to Dataverse key : api key dv : dataverse to which it is being uploaded Returns json from connection attempt.","title":"upload_metadata"},{"location":"credits/","text":"Credits \u00b6 Dataverse_utils and their associated command line programs were written by Paul Lesack . Testing and UI suggestions from Jeremy Buhler . This is a product of the University of British Columbia Library Research Commons .","title":"Credits"},{"location":"credits/#credits","text":"Dataverse_utils and their associated command line programs were written by Paul Lesack . Testing and UI suggestions from Jeremy Buhler . This is a product of the University of British Columbia Library Research Commons .","title":"Credits"},{"location":"faq/","text":"Frequently asked questions \u00b6 \u201cFrequently\u201d may be relative. 1. I am using Windows [7-10]. I\u2019ve installed via pip using a virtual environment, but they don\u2019t use my virtual environment\u2019s Python. \u00b6 What seems like a simple problem is surprisingly complex, as outlined here: https://matthew-brett.github.io/pydagogue/installing_scripts.html . This is further complicated by some other factors. If you have only one Python installation on your machine, you are probably OK. Mind you, if you\u2019re looking at this and that\u2019s the case, maybe I\u2019m wrong. If you have multiple Pythons, you can try changing your environment variables to point to the correct Python. If you are using ArcGIS products, ArcGIS may write items into your Windows registry which will associate any .py file with it\u2019s grossly outdated Python 2.7, or just the wrong Python. More specifically: Computer\\HKEY_CLASSES_ROOT\\Python.File\\shell\\open\\command in the Windows registry. Unfortunately, the Windows computer that I have available for testing (as I normally use Linux or Mac) does not have an administrator account, so I can\u2019t confirm that changing this key will work (although there\u2019s no reason to believe it won\u2019t). This is a pain. Is there something less irritating that would work? \u00b6 Yes, there is. You can still run the scripts manually. There are two options for this. Download the repository via git to a convenient place and use the files in the scripts/ directory Point your %PATH% to where the scripts are installed. To find out where they are installed: TLDR: version: Point your %PATH% (and use first part of the path) to point to your [venv]\\Scripts directory, because they\u2019re probably there. Long winded instructions/explanation: Start a Python session: import sys sys.path [x for x in sys.path if x.endswith(\u2018site-packages\u2019)] The location of the scripts will be written in [whatever the output of sys.path]/dataverse_utils[somestuff]egg-info/installed-files.txt , usually three levels up in the scripts directory. Ironically, this is also the location of the activate portion of the comand required to start a virtual environment (if you are using a virtual environment). For some background on this, venv in Linux and Mac versions of Python uses bin , and reserves scripts for the scripts directory. Windows, however, uses Scripts for the venv module, and to make it worse it\u2019s not usually case sensitive, so anything in scripts gets put into Scripts .","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"\u201cFrequently\u201d may be relative.","title":"Frequently asked questions"},{"location":"faq/#1-i-am-using-windows-7-10-ive-installed-via-pip-using-a-virtual-environment-but-they-dont-use-my-virtual-environments-python","text":"What seems like a simple problem is surprisingly complex, as outlined here: https://matthew-brett.github.io/pydagogue/installing_scripts.html . This is further complicated by some other factors. If you have only one Python installation on your machine, you are probably OK. Mind you, if you\u2019re looking at this and that\u2019s the case, maybe I\u2019m wrong. If you have multiple Pythons, you can try changing your environment variables to point to the correct Python. If you are using ArcGIS products, ArcGIS may write items into your Windows registry which will associate any .py file with it\u2019s grossly outdated Python 2.7, or just the wrong Python. More specifically: Computer\\HKEY_CLASSES_ROOT\\Python.File\\shell\\open\\command in the Windows registry. Unfortunately, the Windows computer that I have available for testing (as I normally use Linux or Mac) does not have an administrator account, so I can\u2019t confirm that changing this key will work (although there\u2019s no reason to believe it won\u2019t).","title":"1. I am using Windows [7-10]. I've installed via pip using a virtual environment, but they don't use my virtual environment's Python."},{"location":"faq/#this-is-a-pain-is-there-something-less-irritating-that-would-work","text":"Yes, there is. You can still run the scripts manually. There are two options for this. Download the repository via git to a convenient place and use the files in the scripts/ directory Point your %PATH% to where the scripts are installed. To find out where they are installed: TLDR: version: Point your %PATH% (and use first part of the path) to point to your [venv]\\Scripts directory, because they\u2019re probably there. Long winded instructions/explanation: Start a Python session: import sys sys.path [x for x in sys.path if x.endswith(\u2018site-packages\u2019)] The location of the scripts will be written in [whatever the output of sys.path]/dataverse_utils[somestuff]egg-info/installed-files.txt , usually three levels up in the scripts directory. Ironically, this is also the location of the activate portion of the comand required to start a virtual environment (if you are using a virtual environment). For some background on this, venv in Linux and Mac versions of Python uses bin , and reserves scripts for the scripts directory. Windows, however, uses Scripts for the venv module, and to make it worse it\u2019s not usually case sensitive, so anything in scripts gets put into Scripts .","title":"This is a pain. Is there something less irritating that would work?"},{"location":"scripts/","text":"Utility scripts \u00b6 code { white-space : pre-wrap !important; } These scripts are available at the command line/command prompt and don\u2019t require any Python knowledge except how to install a Python library via pip, as outlined in the overview document. Once installed via pip, the scripts should be available via the command line and will not require calling Python explicitly. That is, they can be called from the command line directly. For example: dv_tsv_manifest.py is all you will need to type. Note that these programs have been primarily tested on Linux and MacOS, with Windows a distant third. Windows is notable for its unusual file handling, so, as the MIT licenses stipulates, there is no warranty as to the suitability for a particular purpose. Of course, they should work. In no particular order: dv_del.py \u00b6 This is bulk deletion utility for unpublished studies (or even single studies). It\u2019s useful when your automated procedures have gone wrong, or if you don\u2019t feel like navigating through many menus. Note the -i switch which can ask for manual confirmation of deletions. Usage usage: dv_del.py [-h] -k KEY [-d DATAVERSE | -p PID] [-i] [-u DVURL] Delete draft studies from a Dataverse collection optional arguments: -h, --help show this help message and exit -k KEY, --key KEY Dataverse user API key -d DATAVERSE, --dataverse DATAVERSE Dataverse collection short name from which to delete all draft records. eg. \"ldc\" -p PID, --persistentId PID Handle or DOI to delete in format hdl:11272.1/FK2/12345 -i, --interactive Confirm each study deletion -u DVURL, --url DVURL URL to base Dataverse installation dv_manifest_gen.py \u00b6 Not technically a Dataverse-specific script, this utility will generate a tab-separated value output. The file consists of 3 columns: file, description and tags . Editing the result and using the upload utility to parse the tsv will add descriptive metadata, tags and file paths to an upload instead of laboriously using the Dataverse GUI. Tags may be separated by commas, eg: \u201cData, SAS, June 2021\u201d. Using stdout and a redirect will also save time. First dump a file as normal. Add other files to the end with different information using the exclude header switch -x and different tags along with output redirection >> . Usage usage: dv_manifest_gen.py [-h] [-f FILENAME] [-t TAG] [-x] [-r] [--version] [files [files ...]] Creates a file manifest in tab separated value format which can then be edited and used for file uploads to a Dataverse collection. Files can be edited to add file descriptions and comma-separated tags that will be automatically attached to metadata using products using the dataverse_utils library. Will dump to stdout unless -f or --filename is used. Using the command and a dash (ie, \"dv_manifest_gen.py -\" produces full paths for some reason. positional arguments: files Files to add to manifest optional arguments: -h, --help show this help message and exit -f FILENAME, --filename FILENAME Save to file instead of outputting to stdout -t TAG, --tag TAG Default tag(s). Separate with comma and use quotes if there are spaces. eg. \"Data, June 2021\". Defaults to \"Data\" -x, --no-header Don't include header in output. Useful if creating a complex tsv using redirects (ie, \">>\"). -r, --recursive Recursive listing. --version Show version number and exit dv_upload_tsv.py \u00b6 Now that you have a tsv full of nicely described data, you can easily upload it to an existing study if you know the persistent ID and have an API key. For the best metadata, you should probably edit it manually to add correct descriptive metadata, notably the \u201cDescription\u201d and \u201cTags\u201d. Tags are split separated by commas, so it\u2019s possible to have multiple tags for each data item, like \u201cData, SPSS, June 2021\u201d. File paths are automatically generated from the \u201cfile\u201d column. Because of this, you should probably use relative paths rather than absolute paths unless you want to have a lengthy path string in Dataverse. Usage usage: dv_upload_tsv.py [-h] -p PID -k KEY [-u URL] [--version] [tsv] Uploads data sets to an *existing* Dataverse study from the contents of a TSV (tab separated value) file. Metadata, file tags, paths, etc are all read from the TSV. JSON output from the Dataverse API is printed to stdout during the process. positional arguments: tsv TSV file to upload optional arguments: -h, --help show this help message and exit -p PID, --pid PID Dataverse study persistent identifier (DOI/handle) -k KEY, --key KEY API key -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" --version Show version number and exit dv_release.py \u00b6 A bulk release utility for Dataverse. This utility will normally be used after a migration or large data transfer, such as a dryad2dataverse transfer from the Dryad data repository. It can release studies individually by persistent ID or just release all unreleased files in a Dataverse. Usage usage: dv_release.py [-h] [-u URL] -k KEY [-i] [--time STIME] [-v] [-r] [-d DV | -p PID [PID ...]] [--version] Bulk file releaser for unpublished Dataverse studies. Either releases individual studies or all unreleased studies in a single Dataverse collection. optional arguments: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Default: https://abacus.library.ubc.ca -k KEY, --key KEY API key -i, --interactive Manually confirm each release --time STIME, -t STIME Time between release attempts in seconds. Default 10 -v Verbose mode -r, --dry-run Only output a list of studies to be released -d DV, --dv DV Short name of Dataverse collection to process (eg: statcan) -p PID [PID ...], --pid PID [PID ...] Handles or DOIs to release in format hdl:11272.1/FK2/12345 or doi:10.80240/FK2/NWRABI. Multiple values OK --version Show version number and exit dv_ldc_uploader.py \u00b6 This is a very specialized utility which will scrape metadata from the Linguistic Data Consortium (LDC) and create a metadata record in a Dataverse. The LDC does not have an API, so the metadata is scraped from their web site. This means that the metadata may not be quite as controlled as that which comes from an API. Data from the LDC website is converted to Dryad -style JSON via dataverse_utils.ldc via the use of the dryad2dataverse library. There are two main methods of use for this utility: Multiple metadata uploads. Multiple LDC record numbers can be supplied and a study without files will be created for each one. If a TSV file with file information is upplied via the -t or --tsv switch, the utility will upload a single LDC study and upload the contents of the tsv file to the created record. Usage usage: dv_ldc_uploader.py [-h] [-u URL] -k KEY [-d DVS] [-t TSV] [-n CNAME] [-e EMAIL] [-v] [--version] studies [studies ...] Linguistic Data Consortium metadata uploader for Dataverse. This utility will scrape the metadata from the LDC website (https://catalog.ldc.upenn.edu) and upload data based on a TSV manifest. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: studies LDC Catalogue numbers to process, separated by spaces. eg. \"LDC2012T19 LDC2011T07\". Case is ignored, so \"ldc2012T19\" will also work. optional arguments: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -d DVS, --dvs DVS Short name of target Dataverse collection (eg: ldc). Defaults to \"ldc\" -t TSV, --tsv TSV Manifest tsv file for uploading and metadata. If not supplied, only metadata will be uploaded. Using this option requires only one positional *studies* argument -n CNAME, --cname CNAME Study contact name. Default: \"Abacus support\" -e EMAIL, --email EMAIL Dataverse study contact email address. Default: abacus-support@lists.ubc.ca -v, --verbose Verbose output --version Show version number and exit","title":"Scripts"},{"location":"scripts/#utility-scripts","text":"code { white-space : pre-wrap !important; } These scripts are available at the command line/command prompt and don\u2019t require any Python knowledge except how to install a Python library via pip, as outlined in the overview document. Once installed via pip, the scripts should be available via the command line and will not require calling Python explicitly. That is, they can be called from the command line directly. For example: dv_tsv_manifest.py is all you will need to type. Note that these programs have been primarily tested on Linux and MacOS, with Windows a distant third. Windows is notable for its unusual file handling, so, as the MIT licenses stipulates, there is no warranty as to the suitability for a particular purpose. Of course, they should work. In no particular order:","title":"Utility scripts"},{"location":"scripts/#dv_delpy","text":"This is bulk deletion utility for unpublished studies (or even single studies). It\u2019s useful when your automated procedures have gone wrong, or if you don\u2019t feel like navigating through many menus. Note the -i switch which can ask for manual confirmation of deletions. Usage usage: dv_del.py [-h] -k KEY [-d DATAVERSE | -p PID] [-i] [-u DVURL] Delete draft studies from a Dataverse collection optional arguments: -h, --help show this help message and exit -k KEY, --key KEY Dataverse user API key -d DATAVERSE, --dataverse DATAVERSE Dataverse collection short name from which to delete all draft records. eg. \"ldc\" -p PID, --persistentId PID Handle or DOI to delete in format hdl:11272.1/FK2/12345 -i, --interactive Confirm each study deletion -u DVURL, --url DVURL URL to base Dataverse installation","title":"dv_del.py"},{"location":"scripts/#dv_manifest_genpy","text":"Not technically a Dataverse-specific script, this utility will generate a tab-separated value output. The file consists of 3 columns: file, description and tags . Editing the result and using the upload utility to parse the tsv will add descriptive metadata, tags and file paths to an upload instead of laboriously using the Dataverse GUI. Tags may be separated by commas, eg: \u201cData, SAS, June 2021\u201d. Using stdout and a redirect will also save time. First dump a file as normal. Add other files to the end with different information using the exclude header switch -x and different tags along with output redirection >> . Usage usage: dv_manifest_gen.py [-h] [-f FILENAME] [-t TAG] [-x] [-r] [--version] [files [files ...]] Creates a file manifest in tab separated value format which can then be edited and used for file uploads to a Dataverse collection. Files can be edited to add file descriptions and comma-separated tags that will be automatically attached to metadata using products using the dataverse_utils library. Will dump to stdout unless -f or --filename is used. Using the command and a dash (ie, \"dv_manifest_gen.py -\" produces full paths for some reason. positional arguments: files Files to add to manifest optional arguments: -h, --help show this help message and exit -f FILENAME, --filename FILENAME Save to file instead of outputting to stdout -t TAG, --tag TAG Default tag(s). Separate with comma and use quotes if there are spaces. eg. \"Data, June 2021\". Defaults to \"Data\" -x, --no-header Don't include header in output. Useful if creating a complex tsv using redirects (ie, \">>\"). -r, --recursive Recursive listing. --version Show version number and exit","title":"dv_manifest_gen.py"},{"location":"scripts/#dv_upload_tsvpy","text":"Now that you have a tsv full of nicely described data, you can easily upload it to an existing study if you know the persistent ID and have an API key. For the best metadata, you should probably edit it manually to add correct descriptive metadata, notably the \u201cDescription\u201d and \u201cTags\u201d. Tags are split separated by commas, so it\u2019s possible to have multiple tags for each data item, like \u201cData, SPSS, June 2021\u201d. File paths are automatically generated from the \u201cfile\u201d column. Because of this, you should probably use relative paths rather than absolute paths unless you want to have a lengthy path string in Dataverse. Usage usage: dv_upload_tsv.py [-h] -p PID -k KEY [-u URL] [--version] [tsv] Uploads data sets to an *existing* Dataverse study from the contents of a TSV (tab separated value) file. Metadata, file tags, paths, etc are all read from the TSV. JSON output from the Dataverse API is printed to stdout during the process. positional arguments: tsv TSV file to upload optional arguments: -h, --help show this help message and exit -p PID, --pid PID Dataverse study persistent identifier (DOI/handle) -k KEY, --key KEY API key -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" --version Show version number and exit","title":"dv_upload_tsv.py"},{"location":"scripts/#dv_releasepy","text":"A bulk release utility for Dataverse. This utility will normally be used after a migration or large data transfer, such as a dryad2dataverse transfer from the Dryad data repository. It can release studies individually by persistent ID or just release all unreleased files in a Dataverse. Usage usage: dv_release.py [-h] [-u URL] -k KEY [-i] [--time STIME] [-v] [-r] [-d DV | -p PID [PID ...]] [--version] Bulk file releaser for unpublished Dataverse studies. Either releases individual studies or all unreleased studies in a single Dataverse collection. optional arguments: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Default: https://abacus.library.ubc.ca -k KEY, --key KEY API key -i, --interactive Manually confirm each release --time STIME, -t STIME Time between release attempts in seconds. Default 10 -v Verbose mode -r, --dry-run Only output a list of studies to be released -d DV, --dv DV Short name of Dataverse collection to process (eg: statcan) -p PID [PID ...], --pid PID [PID ...] Handles or DOIs to release in format hdl:11272.1/FK2/12345 or doi:10.80240/FK2/NWRABI. Multiple values OK --version Show version number and exit","title":"dv_release.py"},{"location":"scripts/#dv_ldc_uploaderpy","text":"This is a very specialized utility which will scrape metadata from the Linguistic Data Consortium (LDC) and create a metadata record in a Dataverse. The LDC does not have an API, so the metadata is scraped from their web site. This means that the metadata may not be quite as controlled as that which comes from an API. Data from the LDC website is converted to Dryad -style JSON via dataverse_utils.ldc via the use of the dryad2dataverse library. There are two main methods of use for this utility: Multiple metadata uploads. Multiple LDC record numbers can be supplied and a study without files will be created for each one. If a TSV file with file information is upplied via the -t or --tsv switch, the utility will upload a single LDC study and upload the contents of the tsv file to the created record. Usage usage: dv_ldc_uploader.py [-h] [-u URL] -k KEY [-d DVS] [-t TSV] [-n CNAME] [-e EMAIL] [-v] [--version] studies [studies ...] Linguistic Data Consortium metadata uploader for Dataverse. This utility will scrape the metadata from the LDC website (https://catalog.ldc.upenn.edu) and upload data based on a TSV manifest. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: studies LDC Catalogue numbers to process, separated by spaces. eg. \"LDC2012T19 LDC2011T07\". Case is ignored, so \"ldc2012T19\" will also work. optional arguments: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -d DVS, --dvs DVS Short name of target Dataverse collection (eg: ldc). Defaults to \"ldc\" -t TSV, --tsv TSV Manifest tsv file for uploading and metadata. If not supplied, only metadata will be uploaded. Using this option requires only one positional *studies* argument -n CNAME, --cname CNAME Study contact name. Default: \"Abacus support\" -e EMAIL, --email EMAIL Dataverse study contact email address. Default: abacus-support@lists.ubc.ca -v, --verbose Verbose output --version Show version number and exit","title":"dv_ldc_uploader.py"}]}