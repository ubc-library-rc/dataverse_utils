{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dataverse utilities \u00b6 This is a generalized set of utilities which help with managing Dataverse repositories. This has nothing to do with the Microsoft product of the same name. Despite being written as they were required, that doesn\u2019t mean they\u2019re not useful or user-friendly. With these utilities you can: Upload your data sets from a tab-separated-value spreadsheet Bulk release multiple data sets Bulk delete (unpublished) assets Quickly duplicate records Replace licences and more! Get your copy today! Source code (and this documentation) is available at the Github repository https://github.com/ubc-library-rc/dataverse_utils , and the user-friendly version of the documentation is at https://ubc-library-rc.github.io/dataverse_utils . Presumably you know this already otherwise you wouldn\u2019t be reading this. Installation \u00b6 Any installation will require the use of the command line/command prompt. The easiest installation is with pip : pip install dataverse_utils There is also a server specific version if you need to use the dv_facet_date utility. This can only be run on a server hosting a Dataverse instance, so for the vast majority of users it will be unusable. This can also be installed with pip : pip install 'dataverse_utils[server]' Note the extra quotes. You can install the server version if you want to, but it\u2019s useless without server access. Upgrading \u00b6 Just as easy as installation: pip install --upgrade dataverse_utils Other methods of installing Python packages can be found at https://packaging.python.org/tutorials/installing-packages/ . Downloading the source code \u00b6 Source code is available at https://github.com/ubc-library-rc/dataverse_utils . Working on the assumption that git is installed, you can download the whole works with: git clone https://github.com/ubc-library-rc/dataverse_utils If you have mkdocs installed, you can view the documentation in a web browser by running mkdocs from the top level directory of the downloaded source files by running mkdocs serve . The components \u00b6 Console utilities \u00b6 There are eight (8) console utilities currently available. dv_del.py : Bulk (unpublished) file deletion utility dv_ldc_uploader.py : A utility which scrapes Linguistic Data Consortium metadata from their website, converts it to Dataverse JSON and uploads it, with the possibility of including local files. As of early 2023, there is an issue which requires attaching a manually downloaded certificate chain . Don\u2019t worry, that\u2019s not as hard as it sounds. dv_manifest_gen.py : Creates a simple tab-separated value format file which can be edited and then used to upload files as well as file-level metadata. Normally files will be edited after creation, usually in a spreadsheet like Excel. dv_pg_facet_date.py : A server-based tool which updates the publication date facet and performs a study reindex. dv_record_copy.py : Copies an existing Dataverse study metadata record to a target collection, or replace a currently existing record. dv_release.py : A bulk release utility. Either releases all the unreleased studies in a Dataverse or individually if persistent identifiers are available. dv_replace_licences : Replaces the licence associated with a PID with text from a Markdown file. Also available as dv_replace_licenses for those using American English. dv_upload_tsv.py : Takes a tsv file in the format from dv_manifest_gen.py and does all the uploading and metadata entry. More information about these can be found on the console utilities page . Python package: dataverse_utils \u00b6 This package contains a variety of utility functions which, for the most part, allow uploads of files and associated metadata without having to touch the Dataverse GUI or to have complex JSON attached. For example, the upload_file requires no JSON attachments: dataverse_utils.upload_file('/path/to/file.ext', dv='https://targetdataverse.invalid' descr='A file description', tags=['Data', 'Example', 'Spam'], dirlabel=['path/to/spam'], mimetype='application/geo+json') Consult the API reference for full details. ldc \u00b6 The ldc component represents the Linguistic Data Consortium or LDC. The ldc module is designed to harvest LDC metadata from its catalogue, convert it to Dataverse JSON, then upload it to a Dataverse installation. Once the study has been created, the general dataverse_utils module can handle the file uploading. The ldc module requires the dryad2dataverse package. Because of this, it requires a tiny bit more effort, because LDC material doesn\u2019t have the required metadata. Here\u2019s snippet that shows how it works. import dataverse_utils.ldc as ldc ldc.ds.constants.DV_CONTACT_EMAIL='iamcontact@test.invalid' ldc.ds.constants.DV_CONTACT_NAME='Generic Support Email' KEY = 'IAM-YOUR-DVERSE-APIKEY' stud = 'LDC2021T02' #LDC study number a = ldc.Ldc(stud) a.fetch_record() #Data goes into the 'ldc' dataverse info = a.upload_metadata(url='https://dataverse.invalid', key=KEY, dv='ldc') hdl = info['data']['persistentId'] with open('/Users/you/tmp/testme.tsv') as fil: du.upload_from_tsv(fil, hdl=hdl,dv='https://dataverse.invalid', apikey=KEY) Note that one method uses key and the other apikey . This is what is known as ad hoc . More information is available at the API reference .","title":"Overview"},{"location":"#dataverse-utilities","text":"This is a generalized set of utilities which help with managing Dataverse repositories. This has nothing to do with the Microsoft product of the same name. Despite being written as they were required, that doesn\u2019t mean they\u2019re not useful or user-friendly. With these utilities you can: Upload your data sets from a tab-separated-value spreadsheet Bulk release multiple data sets Bulk delete (unpublished) assets Quickly duplicate records Replace licences and more! Get your copy today! Source code (and this documentation) is available at the Github repository https://github.com/ubc-library-rc/dataverse_utils , and the user-friendly version of the documentation is at https://ubc-library-rc.github.io/dataverse_utils . Presumably you know this already otherwise you wouldn\u2019t be reading this.","title":"Dataverse utilities"},{"location":"#installation","text":"Any installation will require the use of the command line/command prompt. The easiest installation is with pip : pip install dataverse_utils There is also a server specific version if you need to use the dv_facet_date utility. This can only be run on a server hosting a Dataverse instance, so for the vast majority of users it will be unusable. This can also be installed with pip : pip install 'dataverse_utils[server]' Note the extra quotes. You can install the server version if you want to, but it\u2019s useless without server access.","title":"Installation"},{"location":"#upgrading","text":"Just as easy as installation: pip install --upgrade dataverse_utils Other methods of installing Python packages can be found at https://packaging.python.org/tutorials/installing-packages/ .","title":"Upgrading"},{"location":"#downloading-the-source-code","text":"Source code is available at https://github.com/ubc-library-rc/dataverse_utils . Working on the assumption that git is installed, you can download the whole works with: git clone https://github.com/ubc-library-rc/dataverse_utils If you have mkdocs installed, you can view the documentation in a web browser by running mkdocs from the top level directory of the downloaded source files by running mkdocs serve .","title":"Downloading the source code"},{"location":"#the-components","text":"","title":"The components"},{"location":"#console-utilities","text":"There are eight (8) console utilities currently available. dv_del.py : Bulk (unpublished) file deletion utility dv_ldc_uploader.py : A utility which scrapes Linguistic Data Consortium metadata from their website, converts it to Dataverse JSON and uploads it, with the possibility of including local files. As of early 2023, there is an issue which requires attaching a manually downloaded certificate chain . Don\u2019t worry, that\u2019s not as hard as it sounds. dv_manifest_gen.py : Creates a simple tab-separated value format file which can be edited and then used to upload files as well as file-level metadata. Normally files will be edited after creation, usually in a spreadsheet like Excel. dv_pg_facet_date.py : A server-based tool which updates the publication date facet and performs a study reindex. dv_record_copy.py : Copies an existing Dataverse study metadata record to a target collection, or replace a currently existing record. dv_release.py : A bulk release utility. Either releases all the unreleased studies in a Dataverse or individually if persistent identifiers are available. dv_replace_licences : Replaces the licence associated with a PID with text from a Markdown file. Also available as dv_replace_licenses for those using American English. dv_upload_tsv.py : Takes a tsv file in the format from dv_manifest_gen.py and does all the uploading and metadata entry. More information about these can be found on the console utilities page .","title":"Console utilities"},{"location":"#python-package-dataverse_utils","text":"This package contains a variety of utility functions which, for the most part, allow uploads of files and associated metadata without having to touch the Dataverse GUI or to have complex JSON attached. For example, the upload_file requires no JSON attachments: dataverse_utils.upload_file('/path/to/file.ext', dv='https://targetdataverse.invalid' descr='A file description', tags=['Data', 'Example', 'Spam'], dirlabel=['path/to/spam'], mimetype='application/geo+json') Consult the API reference for full details.","title":"Python package: dataverse_utils"},{"location":"#ldc","text":"The ldc component represents the Linguistic Data Consortium or LDC. The ldc module is designed to harvest LDC metadata from its catalogue, convert it to Dataverse JSON, then upload it to a Dataverse installation. Once the study has been created, the general dataverse_utils module can handle the file uploading. The ldc module requires the dryad2dataverse package. Because of this, it requires a tiny bit more effort, because LDC material doesn\u2019t have the required metadata. Here\u2019s snippet that shows how it works. import dataverse_utils.ldc as ldc ldc.ds.constants.DV_CONTACT_EMAIL='iamcontact@test.invalid' ldc.ds.constants.DV_CONTACT_NAME='Generic Support Email' KEY = 'IAM-YOUR-DVERSE-APIKEY' stud = 'LDC2021T02' #LDC study number a = ldc.Ldc(stud) a.fetch_record() #Data goes into the 'ldc' dataverse info = a.upload_metadata(url='https://dataverse.invalid', key=KEY, dv='ldc') hdl = info['data']['persistentId'] with open('/Users/you/tmp/testme.tsv') as fil: du.upload_from_tsv(fil, hdl=hdl,dv='https://dataverse.invalid', apikey=KEY) Note that one method uses key and the other apikey . This is what is known as ad hoc . More information is available at the API reference .","title":"ldc"},{"location":"api_ref/","text":"API Reference \u00b6 dataverse_utils \u00b6 Generalized dataverse utilities. Note that import dataverse_utils is the equivalent of import dataverse_utils.dataverse_utils dataverse_utils.dvdata \u00b6 Dataverse studies and files Study Objects \u00b6 class Study(dict) Dataverse record. Dataverse study records are pure metadata so this is represented with a dictionary. __init__ \u00b6 def __init__(pid: str, url: str, key: str, **kwargs) pid : str Record persistent identifier: hdl or doi url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges get_version \u00b6 @classmethod def get_version(cls, url: str, timeout: int = 100) -> float Returns a float representing a Dataverse version number. Floating point value composed of: float(f\u2019{major_version}.{minor_verson:03d}{patch:03d}\u2019) ie, version 5.9.2 would be 5.009002 url : str URL of base Dataverse instance. eg: \u2018https://abacus.library.ubc.ca\u2019 timeout : int Request timeout in seconds set_version \u00b6 def set_version(url: str, timeout: int = 100) -> None Sets self[\u2018target_version\u2019] to appropriate integer value AND formats self[\u2018upload_json\u2019] to correct JSON format url : str URL of target Dataverse instance timeout : int request timeout in seconds fix_licence \u00b6 def fix_licence() -> None With Dataverse v5.10+, a licence type of \u2018NONE\u2019 is now forbidden. Now, as per https://guides.dataverse.org/en/5.14/api/sword.html ?highlight=invalid%20license , non-standard licences may be replaced with None. This function edits the same Study object in place , so returns nothing. production_location \u00b6 def production_location() -> None Changes \u201cmultiple\u201d to True where typeName == \u2018productionPlace\u2019 in Study[\u2018upload_json\u2019] Changes are done in place . This change came into effect with Dataverse v5.13 File Objects \u00b6 class File(dict) Class representing a file on a Dataverse instance __init__ \u00b6 def __init__(url: str, key: str, **kwargs) url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges id : int or str File identifier; can be a file ID or PID args : list kwargs : dict To initialize correctly, pass a value from Study[\u2018file_info\u2019]. Eg: File(\u2018https://test.invalid\u2019, \u2018ABC123\u2019, **Study_instance[\u2018file_info\u2019][0]) download_file \u00b6 def download_file() Downloads the file to a temporary location. Data will be in the ORIGINAL format, not Dataverse-processed TSVs del_tempfile \u00b6 def del_tempfile() Delete tempfile if it exists produce_digest \u00b6 def produce_digest(prot: str = 'md5', blocksize: int = 2**16) -> str Returns hex digest for object fname : str Path to a file object prot : str Hash type. Supported hashes: 'sha1', 'sha224', 'sha256', 'sha384', 'sha512', 'blake2b', 'blake2s', 'md5'. Default: 'md5' blocksize : int Read block size in bytes verify \u00b6 def verify() -> None Compares checksum with stated checksum dataverse_utils.scripts.dv_record_copy \u00b6 Copies a dataverse record to collection OR copies a record to an existing PID. That way all you have to do is edit a few fields in the GUI instead of painfully editing JSON or painfully using the Dataverse GUI. parsley \u00b6 def parsley() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser main \u00b6 def main() You know what this does dataverse_utils.scripts.dv_study_migrator \u00b6 Copies an entire record and migrates it including the data parsley \u00b6 def parsley() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser upload_file_to_target \u00b6 def upload_file_to_target(indict: dict, pid, source_url, source_key, target_url, target_key) Uploads a single file with metadata to a dataverse record remove_target_files \u00b6 def remove_target_files(record: dataverse_utils.dvdata.Study, timeout: int = 100) Removes all files from a dataverse record. record: dataverse_utils.dvdata.Study timeout: int Timeout in seconds main \u00b6 def main() Run this, obviously dataverse_utils.scripts.dv_ldc_uploader \u00b6 Auto download/upload LDC metadata and files. python3 uploadme.py LDC20201S01 . . . LDC2021T21 apikey parse \u00b6 def parse() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser upload_meta \u00b6 def upload_meta(ldccat: str, url: str, key: str, dvs: str, verbose: bool = False, certchain: str = None) -> str Uploads metadata to target dataverse collection. Returns persistentId. ldccat : str Linguistic Data Consortium catalogue number url : str URL to base instance of Dataverse installation key : str API key dvs : str Target Dataverse collection short name certchain : str Path to LDC .PEM certificate chain main \u00b6 def main() -> None Uploads metadata and data to Dataverse collection/study respectively dataverse_utils.scripts.dv_pg_facet_date \u00b6 Reads the date from a Dataverse study and forces the facet sidebar to use that date by manually updating the Dataverse Postgres database. This must be run on the server that hosts a Dataverse installation, and the user must supply, at a minimum, the database password and a persistent ID to be read, as well as a date type. Requires two non-standard python libraries: psycopg2 (use psycopg2-binary to avoid installing from source) and requests. Psycopg2 is not part of the requirements for dataverse_utils because it is only used for the server portion of these utilities, and hence useless for them. parsely \u00b6 def parsely() -> argparse.ArgumentParser Command line argument parser parse_dtype \u00b6 def parse_dtype(dtype) -> str Returns correctly formatted date type string for Dataverse API dtype : str One of the allowable values from the parser write_old \u00b6 def write_old(data) -> None Writes older data to a tsv file. Assumes 4 values per item: id, authority, identifier, publicationdate. publicationdate is assumed to be a datetime.datetime instance. \u00b6 Arguments : data : list Postqres query output list (ie, data = cursor.fetchall()) write_sql \u00b6 def write_sql(data) -> None Write SQL to file get_datetime \u00b6 def get_datetime(datestr) -> (datetime.datetime, str) Return datetime from poorly formatted Dataverse dates string datestr : str Dataverse date returned by API fetch_date_api \u00b6 def fetch_date_api(url, key, pid, dtype) -> str Returns the requested date string from the Dataverse study record url : str Base URL of Dataverse installation key :str API key for Dataverse user pid : str Persistent identifier for Dataverse study dtype : str Date type required reindex \u00b6 def reindex(pid) -> dict Reindexes study in place. Localhost access only. pid : str PersistentId for Dataverse study main \u00b6 def main() The heart of the application dataverse_utils.scripts.dv_release \u00b6 Bulk release script for Dataverse. This is almost identical to the dryad2dataverse bulk releaser except that the defaults are changed to https://abacus.library.ubc.ca argp \u00b6 def argp() Parses the arguments from the command line. Returns arparse.ArgumentParser Dverse Objects \u00b6 class Dverse() An object representing a Dataverse installation __init__ \u00b6 def __init__(dvurl, apikey, dvs) Intializes Dataverse installation object. Arguments : dvurl : str. URL to base Dataverse installation (eg. \u2018https://abacus.library.ubc.ca\u2019) apikey : str. API key for Dataverse user dv : str. Short name of target Dataverse collection (eg. \u2018statcan\u2019) study_list \u00b6 @property def study_list() -> list Returns a list of all studies (published or not) in the Dataverse collection unreleased \u00b6 @property def unreleased(all_stud: list = None) -> list Finds only unreleased studies from a list of studies Arguments : all_stud : list. List of Dataverse studies. Defaults to output of Dverse.get_study_list() Study Objects \u00b6 class Study() Instance representing a Dataverse study __init__ \u00b6 def __init__(**kwargs) :kwarg dvurl: str. Base URL for Dataverse instance :kwarg apikey: str. API key for Dataverse user :kwarg pid: str. Persistent identifier for study :kwarg stime: int. Time between file lock checks. Default 10 :kwarg verbose: Verbose output. Default False status_ok \u00b6 def status_ok() Checks to see if study has a lock. Returns True if OK to continue, else False. release_me \u00b6 def release_me(interactive=False) Releases study and waits until it\u2019s unlocked before returning to the function main \u00b6 def main() The primary function. Will release all unreleased studies in the the target Dataverse collection, or selected studies as required. dataverse_utils.scripts.dv_del \u00b6 Dataverse Bulk Deleter Deletes unpublished studies at the command line delstudy \u00b6 def delstudy(dvurl, key, pid) Deletes Dataverse study dvurl : str Dataverse installation base URL key : str Dataverse user API key pid : str Dataverse collection study persistent identifier conf \u00b6 def conf(tex) Confirmation dialogue checker. Returns true if \u201cY\u201d or \u201cy\u201d getsize \u00b6 def getsize(dvurl, pid, key) Returns size of Dataverse study. Mostly here for debugging. dvurl : str Dataverse installation base URL pid : str Dataverse collection study persistent identifier key : str Dataverse user API key parsley \u00b6 def parsley() -> argparse.ArgumentParser Argument parser as separate function main \u00b6 def main() Command line bulk deleter dataverse_utils.scripts.dv_replace_licence \u00b6 Replace all licence in a study with one read from an external markdown file. This requires using a different API, the \u201csemantic metadata api\u201d https://guides.dataverse.org/en/5.6/developers/ dataset-semantic-metadata-api.html parsley \u00b6 def parsley() -> argparse.ArgumentParser() parse the command line replace_licence \u00b6 def replace_licence(hdl, lic, key, url='https://abacus.library.ubc.ca') Replace the licence for a dataverse study with persistent ID hdl. hdl : str Dataverse persistent ID lic : str Licence text in Markdown format key : str Dataverse API key url : str Dataverse installation base URL republish \u00b6 def republish(hdl, key, url='https://abacus.library.ubc.ca') Republish study without updating version hdl : str Persistent Id key : str Dataverse API key url : str Dataverse installation base URL print_stat \u00b6 def print_stat(rjson) Prints error status to stdout main \u00b6 def main() Main script function dataverse_utils.scripts.dv_upload_tsv \u00b6 Uploads data sets to a dataverse installation from the contents of a TSV (tab separated value) file. Metadata, file tags, paths, etc are all read from the TSV. parse \u00b6 def parse() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser main \u00b6 def main() -> None Uploads data to an already existing Dataverse study dataverse_utils.scripts.dv_manifest_gen \u00b6 Creates a file manifest in tab separated value format which can be used with other dataverse_util library utilities and functions to upload files complete with metadata. parse \u00b6 def parse() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser quotype \u00b6 def quotype(quote: str) -> int Parse quotation type for csv parser. returns csv quote constant. main \u00b6 def main() -> None The main function call dataverse_utils.dataverse_utils \u00b6 A collection of Dataverse utilities for file and metadata manipulation DvGeneralUploadError Objects \u00b6 class DvGeneralUploadError(Exception) Raised on non-200 URL response Md5Error Objects \u00b6 class Md5Error(Exception) Raised on md5 mismatch make_tsv \u00b6 def make_tsv(start_dir, in_list=None, def_tag='Data', inc_header=True, mime=False, quotype=csv.QUOTE_MINIMAL) -> str Recurses the tree for files and produces tsv output with with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. The \u2018description\u2019 is the filename without an extension. Returns tsv as string. Arguments : start_dir : str Path to start directory in_list : list Input file list. Defaults to recursive walk of current directory. def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with a comma: eg. (\u2018Data, 2016\u2019) inc_header : bool Include header row mime : bool Include automatically determined mimetype quotype - int integer value or csv quote type. Default = csv.QUOTE_MINIMAL Acceptable values: csv.QUOTE_MINIMAL / 0 csv.QUOTE_ALL / 1 csv.QUOTE_NONNUMERIC / 2 csv.QUOTE_NONE / 3 dump_tsv \u00b6 def dump_tsv(start_dir, filename, in_list=None, **kwargs) Dumps output of make_tsv manifest to a file. Arguments : start_dir : str Path to start directory in_list : list List of files for which to create manifest entries. Will default to recursive directory crawl OPTIONAL KEYWORD ARGUMENTS def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with an easily splitable character: eg. (\u2018Data, 2016\u2019) - Default - \u2018Data\u2019 inc_header : bool Include header for tsv. Default : True quotype - int integer value or csv quote type. Default : csv.QUOTE_MINIMAL Acceptable values: csv.QUOTE_MINIMAL / 0 csv.QUOTE_ALL / 1 csv.QUOTE_NONNUMERIC / 2 csv.QUOTE_NONE / 3 file_path \u00b6 def file_path(fpath, trunc='') -> str Create relative file path from full path string file_path(\u2018/tmp/Data/2011/excelfile.xlsx\u2019, \u2018/tmp/\u2019) \u2018Data/2011\u2019 file_path(\u2018/tmp/Data/2011/excelfile.xlsx\u2019, \u2018/tmp\u2019) \u2018Data/2011\u2019 Arguments : fpath : str File location (ie, complete path) trunc : str Leftmost portion of path to remove check_lock \u00b6 def check_lock(dv_url, study, apikey) -> bool Checks study lock status; returns True if locked. Arguments : dvurl : str URL of Dataverse installation study - str Persistent ID of study apikey : str API key for user force_notab_unlock \u00b6 def force_notab_unlock(study, dv_url, fid, apikey, try_uningest=True) -> int Forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Returns 0 if unlocked, file id if locked (and then unlocked). Arguments : study : str Persistent indentifer of study dv_url : str URL to base Dataverse installation fid : str File ID for file object apikey : str API key for user try_uningest : bool Try to uningest the file that was locked. - Default - True uningest_file \u00b6 def uningest_file(dv_url, fid, apikey, study='n/a') Tries to uningest a file that has been ingested. Requires superuser API key. Arguments : dv_url : str URL to base Dataverse installation fid : int or str File ID of file to uningest apikey : str API key for superuser study : str Optional handle parameter for log messages upload_file \u00b6 def upload_file(fpath, hdl, **kwargs) Uploads file to Dataverse study and sets file metadata and tags. Arguments : fpath : str file location (ie, complete path) hdl : str Dataverse persistent ID for study (handle or DOI) kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user descr : str OPTIONAL file description md5 : str OPTIONAL md5sum for file checking tags : list OPTIONAL list of text file tags. Eg [\u2018Data\u2019, \u2018June 2020\u2019] dirlabel : str OPTIONAL Unix style relative pathname for Dataverse file path: eg: path/to/file/ nowait : bool OPTIONAL Force a file unlock and uningest instead of waiting for processing to finish trunc : str OPTIONAL Leftmost portion of path to remove rest : bool OPTIONAL Restrict file. Defaults to false unless True supplied mimetype : str OPTIONAL Mimetype of file. Useful if using File Previewers. Mimetype for zip files (application/zip) will be ignored to circumvent Dataverse\u2019s automatic unzipping function. label : str OPTIONAL If included in kwargs, this value will be used for the label timeout = int OPTIONAL Timeout in seconds restrict_file \u00b6 def restrict_file(**kwargs) Restrict file in Dataverse study. Arguments : kwargs : dict other parameters. Acceptable keywords and contents are: One of pid or fid is required pid : str file persistent ID fid : str file database ID dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user rest : bool On True, restrict. Default True upload_from_tsv \u00b6 def upload_from_tsv(fil, hdl, **kwargs) Utility for bulk uploading. Assumes fil is formatted as tsv with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. \u2018tags\u2019 field will be split on commas. Arguments : fil : filelike object Open file object or io.IOStream() hdl : str Dataverse persistent ID for study (handle or DOI) trunc : str Leftmost portion of Dataverse study file path to remove. - eg - trunc =\u2019/home/user/\u2019 if the tsv field is \u2018/home/user/Data/ASCII\u2019 would set the path for that line of the tsv to \u2018Data/ASCII\u2019. Defaults to None. kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user rest : bool On True, restrict access. Default False dataverse_utils.ldc \u00b6 Creates dataverse JSON from Linguistic Data Consortium website page. Ldc Objects \u00b6 class Ldc(ds.Serializer) An LDC item (eg, LDC2021T01) __init__ \u00b6 def __init__(ldc, cert=None) Returns a dict with keys created from an LDC catalogue web page. Arguments : ldc : str Linguistic Consortium Catalogue Number (eg. \u2018LDC2015T05\u2019. This is what forms the last part of the LDC catalogue URL. cert : str Path to certificate chain; LDC has had a problem with intermediate certificates, so you can download the chain with a browser and supply a path to the .pem with this parameter ldcJson \u00b6 @property def ldcJson() Returns a JSON based on the LDC web page scraping dryadJson \u00b6 @property def dryadJson() LDC metadata in Dryad JSON format dvJson \u00b6 @property def dvJson() LDC metadata in Dataverse JSON format embargo \u00b6 @property def embargo() Boolean indicating embargo status fileJson \u00b6 @property def fileJson(timeout=45) Returns False: No attached files possible at LDC files \u00b6 @property def files() Returns None. No files possible fetch_record \u00b6 def fetch_record(url=None, timeout=45) Downloads record from LDC website make_ldc_json \u00b6 def make_ldc_json() Returns a dict with keys created from an LDC catalogue web page. name_parser \u00b6 @staticmethod def name_parser(name) Returns lastName/firstName JSON snippet from name Arguments : name : str A name make_dryad_json \u00b6 def make_dryad_json(ldc=None) Creates a Dryad-style dict from an LDC dictionary Arguments : ldc : dict Dictionary containing LDC data. Defaults to self.ldcJson find_block_index \u00b6 @staticmethod def find_block_index(dvjson, key) Finds the index number of an item in Dataverse\u2019s idiotic JSON list Arguments : dvjson : dict Dataverse JSON key : str key for which to find list index make_dv_json \u00b6 def make_dv_json(ldc=None) Returns complete Dataverse JSON Arguments : ldc : dict LDC dictionary. Defaults to self.ldcJson upload_metadata \u00b6 def upload_metadata(**kwargs) -> dict Uploads metadata to dataverse Returns json from connection attempt. Arguments : kwargs: url : str base url to Dataverse key : str api key dv : str Dataverse to which it is being uploaded","title":"API reference"},{"location":"api_ref/#api-reference","text":"","title":"API Reference"},{"location":"api_ref/#dataverse_utils","text":"Generalized dataverse utilities. Note that import dataverse_utils is the equivalent of import dataverse_utils.dataverse_utils","title":"dataverse_utils"},{"location":"api_ref/#dataverse_utilsdvdata","text":"Dataverse studies and files","title":"dataverse_utils.dvdata"},{"location":"api_ref/#study-objects","text":"class Study(dict) Dataverse record. Dataverse study records are pure metadata so this is represented with a dictionary.","title":"Study Objects"},{"location":"api_ref/#__init__","text":"def __init__(pid: str, url: str, key: str, **kwargs) pid : str Record persistent identifier: hdl or doi url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges","title":"__init__"},{"location":"api_ref/#get_version","text":"@classmethod def get_version(cls, url: str, timeout: int = 100) -> float Returns a float representing a Dataverse version number. Floating point value composed of: float(f\u2019{major_version}.{minor_verson:03d}{patch:03d}\u2019) ie, version 5.9.2 would be 5.009002 url : str URL of base Dataverse instance. eg: \u2018https://abacus.library.ubc.ca\u2019 timeout : int Request timeout in seconds","title":"get_version"},{"location":"api_ref/#set_version","text":"def set_version(url: str, timeout: int = 100) -> None Sets self[\u2018target_version\u2019] to appropriate integer value AND formats self[\u2018upload_json\u2019] to correct JSON format url : str URL of target Dataverse instance timeout : int request timeout in seconds","title":"set_version"},{"location":"api_ref/#fix_licence","text":"def fix_licence() -> None With Dataverse v5.10+, a licence type of \u2018NONE\u2019 is now forbidden. Now, as per https://guides.dataverse.org/en/5.14/api/sword.html ?highlight=invalid%20license , non-standard licences may be replaced with None. This function edits the same Study object in place , so returns nothing.","title":"fix_licence"},{"location":"api_ref/#production_location","text":"def production_location() -> None Changes \u201cmultiple\u201d to True where typeName == \u2018productionPlace\u2019 in Study[\u2018upload_json\u2019] Changes are done in place . This change came into effect with Dataverse v5.13","title":"production_location"},{"location":"api_ref/#file-objects","text":"class File(dict) Class representing a file on a Dataverse instance","title":"File Objects"},{"location":"api_ref/#__init___1","text":"def __init__(url: str, key: str, **kwargs) url : str Base URL to host Dataverse instance key : str Dataverse API key with downloader privileges id : int or str File identifier; can be a file ID or PID args : list kwargs : dict To initialize correctly, pass a value from Study[\u2018file_info\u2019]. Eg: File(\u2018https://test.invalid\u2019, \u2018ABC123\u2019, **Study_instance[\u2018file_info\u2019][0])","title":"__init__"},{"location":"api_ref/#download_file","text":"def download_file() Downloads the file to a temporary location. Data will be in the ORIGINAL format, not Dataverse-processed TSVs","title":"download_file"},{"location":"api_ref/#del_tempfile","text":"def del_tempfile() Delete tempfile if it exists","title":"del_tempfile"},{"location":"api_ref/#produce_digest","text":"def produce_digest(prot: str = 'md5', blocksize: int = 2**16) -> str Returns hex digest for object fname : str Path to a file object prot : str Hash type. Supported hashes: 'sha1', 'sha224', 'sha256', 'sha384', 'sha512', 'blake2b', 'blake2s', 'md5'. Default: 'md5' blocksize : int Read block size in bytes","title":"produce_digest"},{"location":"api_ref/#verify","text":"def verify() -> None Compares checksum with stated checksum","title":"verify"},{"location":"api_ref/#dataverse_utilsscriptsdv_record_copy","text":"Copies a dataverse record to collection OR copies a record to an existing PID. That way all you have to do is edit a few fields in the GUI instead of painfully editing JSON or painfully using the Dataverse GUI.","title":"dataverse_utils.scripts.dv_record_copy"},{"location":"api_ref/#parsley","text":"def parsley() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser","title":"parsley"},{"location":"api_ref/#main","text":"def main() You know what this does","title":"main"},{"location":"api_ref/#dataverse_utilsscriptsdv_study_migrator","text":"Copies an entire record and migrates it including the data","title":"dataverse_utils.scripts.dv_study_migrator"},{"location":"api_ref/#parsley_1","text":"def parsley() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser","title":"parsley"},{"location":"api_ref/#upload_file_to_target","text":"def upload_file_to_target(indict: dict, pid, source_url, source_key, target_url, target_key) Uploads a single file with metadata to a dataverse record","title":"upload_file_to_target"},{"location":"api_ref/#remove_target_files","text":"def remove_target_files(record: dataverse_utils.dvdata.Study, timeout: int = 100) Removes all files from a dataverse record. record: dataverse_utils.dvdata.Study timeout: int Timeout in seconds","title":"remove_target_files"},{"location":"api_ref/#main_1","text":"def main() Run this, obviously","title":"main"},{"location":"api_ref/#dataverse_utilsscriptsdv_ldc_uploader","text":"Auto download/upload LDC metadata and files. python3 uploadme.py LDC20201S01 . . . LDC2021T21 apikey","title":"dataverse_utils.scripts.dv_ldc_uploader"},{"location":"api_ref/#parse","text":"def parse() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser","title":"parse"},{"location":"api_ref/#upload_meta","text":"def upload_meta(ldccat: str, url: str, key: str, dvs: str, verbose: bool = False, certchain: str = None) -> str Uploads metadata to target dataverse collection. Returns persistentId. ldccat : str Linguistic Data Consortium catalogue number url : str URL to base instance of Dataverse installation key : str API key dvs : str Target Dataverse collection short name certchain : str Path to LDC .PEM certificate chain","title":"upload_meta"},{"location":"api_ref/#main_2","text":"def main() -> None Uploads metadata and data to Dataverse collection/study respectively","title":"main"},{"location":"api_ref/#dataverse_utilsscriptsdv_pg_facet_date","text":"Reads the date from a Dataverse study and forces the facet sidebar to use that date by manually updating the Dataverse Postgres database. This must be run on the server that hosts a Dataverse installation, and the user must supply, at a minimum, the database password and a persistent ID to be read, as well as a date type. Requires two non-standard python libraries: psycopg2 (use psycopg2-binary to avoid installing from source) and requests. Psycopg2 is not part of the requirements for dataverse_utils because it is only used for the server portion of these utilities, and hence useless for them.","title":"dataverse_utils.scripts.dv_pg_facet_date"},{"location":"api_ref/#parsely","text":"def parsely() -> argparse.ArgumentParser Command line argument parser","title":"parsely"},{"location":"api_ref/#parse_dtype","text":"def parse_dtype(dtype) -> str Returns correctly formatted date type string for Dataverse API dtype : str One of the allowable values from the parser","title":"parse_dtype"},{"location":"api_ref/#write_old","text":"def write_old(data) -> None Writes older data to a tsv file. Assumes 4 values per item: id, authority, identifier, publicationdate.","title":"write_old"},{"location":"api_ref/#publicationdate-is-assumed-to-be-a-datetimedatetime-instance","text":"Arguments : data : list Postqres query output list (ie, data = cursor.fetchall())","title":"publicationdate is assumed to be a datetime.datetime instance."},{"location":"api_ref/#write_sql","text":"def write_sql(data) -> None Write SQL to file","title":"write_sql"},{"location":"api_ref/#get_datetime","text":"def get_datetime(datestr) -> (datetime.datetime, str) Return datetime from poorly formatted Dataverse dates string datestr : str Dataverse date returned by API","title":"get_datetime"},{"location":"api_ref/#fetch_date_api","text":"def fetch_date_api(url, key, pid, dtype) -> str Returns the requested date string from the Dataverse study record url : str Base URL of Dataverse installation key :str API key for Dataverse user pid : str Persistent identifier for Dataverse study dtype : str Date type required","title":"fetch_date_api"},{"location":"api_ref/#reindex","text":"def reindex(pid) -> dict Reindexes study in place. Localhost access only. pid : str PersistentId for Dataverse study","title":"reindex"},{"location":"api_ref/#main_3","text":"def main() The heart of the application","title":"main"},{"location":"api_ref/#dataverse_utilsscriptsdv_release","text":"Bulk release script for Dataverse. This is almost identical to the dryad2dataverse bulk releaser except that the defaults are changed to https://abacus.library.ubc.ca","title":"dataverse_utils.scripts.dv_release"},{"location":"api_ref/#argp","text":"def argp() Parses the arguments from the command line. Returns arparse.ArgumentParser","title":"argp"},{"location":"api_ref/#dverse-objects","text":"class Dverse() An object representing a Dataverse installation","title":"Dverse Objects"},{"location":"api_ref/#__init___2","text":"def __init__(dvurl, apikey, dvs) Intializes Dataverse installation object. Arguments : dvurl : str. URL to base Dataverse installation (eg. \u2018https://abacus.library.ubc.ca\u2019) apikey : str. API key for Dataverse user dv : str. Short name of target Dataverse collection (eg. \u2018statcan\u2019)","title":"__init__"},{"location":"api_ref/#study_list","text":"@property def study_list() -> list Returns a list of all studies (published or not) in the Dataverse collection","title":"study_list"},{"location":"api_ref/#unreleased","text":"@property def unreleased(all_stud: list = None) -> list Finds only unreleased studies from a list of studies Arguments : all_stud : list. List of Dataverse studies. Defaults to output of Dverse.get_study_list()","title":"unreleased"},{"location":"api_ref/#study-objects_1","text":"class Study() Instance representing a Dataverse study","title":"Study Objects"},{"location":"api_ref/#__init___3","text":"def __init__(**kwargs) :kwarg dvurl: str. Base URL for Dataverse instance :kwarg apikey: str. API key for Dataverse user :kwarg pid: str. Persistent identifier for study :kwarg stime: int. Time between file lock checks. Default 10 :kwarg verbose: Verbose output. Default False","title":"__init__"},{"location":"api_ref/#status_ok","text":"def status_ok() Checks to see if study has a lock. Returns True if OK to continue, else False.","title":"status_ok"},{"location":"api_ref/#release_me","text":"def release_me(interactive=False) Releases study and waits until it\u2019s unlocked before returning to the function","title":"release_me"},{"location":"api_ref/#main_4","text":"def main() The primary function. Will release all unreleased studies in the the target Dataverse collection, or selected studies as required.","title":"main"},{"location":"api_ref/#dataverse_utilsscriptsdv_del","text":"Dataverse Bulk Deleter Deletes unpublished studies at the command line","title":"dataverse_utils.scripts.dv_del"},{"location":"api_ref/#delstudy","text":"def delstudy(dvurl, key, pid) Deletes Dataverse study dvurl : str Dataverse installation base URL key : str Dataverse user API key pid : str Dataverse collection study persistent identifier","title":"delstudy"},{"location":"api_ref/#conf","text":"def conf(tex) Confirmation dialogue checker. Returns true if \u201cY\u201d or \u201cy\u201d","title":"conf"},{"location":"api_ref/#getsize","text":"def getsize(dvurl, pid, key) Returns size of Dataverse study. Mostly here for debugging. dvurl : str Dataverse installation base URL pid : str Dataverse collection study persistent identifier key : str Dataverse user API key","title":"getsize"},{"location":"api_ref/#parsley_2","text":"def parsley() -> argparse.ArgumentParser Argument parser as separate function","title":"parsley"},{"location":"api_ref/#main_5","text":"def main() Command line bulk deleter","title":"main"},{"location":"api_ref/#dataverse_utilsscriptsdv_replace_licence","text":"Replace all licence in a study with one read from an external markdown file. This requires using a different API, the \u201csemantic metadata api\u201d https://guides.dataverse.org/en/5.6/developers/ dataset-semantic-metadata-api.html","title":"dataverse_utils.scripts.dv_replace_licence"},{"location":"api_ref/#parsley_3","text":"def parsley() -> argparse.ArgumentParser() parse the command line","title":"parsley"},{"location":"api_ref/#replace_licence","text":"def replace_licence(hdl, lic, key, url='https://abacus.library.ubc.ca') Replace the licence for a dataverse study with persistent ID hdl. hdl : str Dataverse persistent ID lic : str Licence text in Markdown format key : str Dataverse API key url : str Dataverse installation base URL","title":"replace_licence"},{"location":"api_ref/#republish","text":"def republish(hdl, key, url='https://abacus.library.ubc.ca') Republish study without updating version hdl : str Persistent Id key : str Dataverse API key url : str Dataverse installation base URL","title":"republish"},{"location":"api_ref/#print_stat","text":"def print_stat(rjson) Prints error status to stdout","title":"print_stat"},{"location":"api_ref/#main_6","text":"def main() Main script function","title":"main"},{"location":"api_ref/#dataverse_utilsscriptsdv_upload_tsv","text":"Uploads data sets to a dataverse installation from the contents of a TSV (tab separated value) file. Metadata, file tags, paths, etc are all read from the TSV.","title":"dataverse_utils.scripts.dv_upload_tsv"},{"location":"api_ref/#parse_1","text":"def parse() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser","title":"parse"},{"location":"api_ref/#main_7","text":"def main() -> None Uploads data to an already existing Dataverse study","title":"main"},{"location":"api_ref/#dataverse_utilsscriptsdv_manifest_gen","text":"Creates a file manifest in tab separated value format which can be used with other dataverse_util library utilities and functions to upload files complete with metadata.","title":"dataverse_utils.scripts.dv_manifest_gen"},{"location":"api_ref/#parse_2","text":"def parse() -> argparse.ArgumentParser() Parses the arguments from the command line. Returns argparse.ArgumentParser","title":"parse"},{"location":"api_ref/#quotype","text":"def quotype(quote: str) -> int Parse quotation type for csv parser. returns csv quote constant.","title":"quotype"},{"location":"api_ref/#main_8","text":"def main() -> None The main function call","title":"main"},{"location":"api_ref/#dataverse_utilsdataverse_utils","text":"A collection of Dataverse utilities for file and metadata manipulation","title":"dataverse_utils.dataverse_utils"},{"location":"api_ref/#dvgeneraluploaderror-objects","text":"class DvGeneralUploadError(Exception) Raised on non-200 URL response","title":"DvGeneralUploadError Objects"},{"location":"api_ref/#md5error-objects","text":"class Md5Error(Exception) Raised on md5 mismatch","title":"Md5Error Objects"},{"location":"api_ref/#make_tsv","text":"def make_tsv(start_dir, in_list=None, def_tag='Data', inc_header=True, mime=False, quotype=csv.QUOTE_MINIMAL) -> str Recurses the tree for files and produces tsv output with with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. The \u2018description\u2019 is the filename without an extension. Returns tsv as string. Arguments : start_dir : str Path to start directory in_list : list Input file list. Defaults to recursive walk of current directory. def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with a comma: eg. (\u2018Data, 2016\u2019) inc_header : bool Include header row mime : bool Include automatically determined mimetype quotype - int integer value or csv quote type. Default = csv.QUOTE_MINIMAL Acceptable values: csv.QUOTE_MINIMAL / 0 csv.QUOTE_ALL / 1 csv.QUOTE_NONNUMERIC / 2 csv.QUOTE_NONE / 3","title":"make_tsv"},{"location":"api_ref/#dump_tsv","text":"def dump_tsv(start_dir, filename, in_list=None, **kwargs) Dumps output of make_tsv manifest to a file. Arguments : start_dir : str Path to start directory in_list : list List of files for which to create manifest entries. Will default to recursive directory crawl OPTIONAL KEYWORD ARGUMENTS def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with an easily splitable character: eg. (\u2018Data, 2016\u2019) - Default - \u2018Data\u2019 inc_header : bool Include header for tsv. Default : True quotype - int integer value or csv quote type. Default : csv.QUOTE_MINIMAL Acceptable values: csv.QUOTE_MINIMAL / 0 csv.QUOTE_ALL / 1 csv.QUOTE_NONNUMERIC / 2 csv.QUOTE_NONE / 3","title":"dump_tsv"},{"location":"api_ref/#file_path","text":"def file_path(fpath, trunc='') -> str Create relative file path from full path string file_path(\u2018/tmp/Data/2011/excelfile.xlsx\u2019, \u2018/tmp/\u2019) \u2018Data/2011\u2019 file_path(\u2018/tmp/Data/2011/excelfile.xlsx\u2019, \u2018/tmp\u2019) \u2018Data/2011\u2019 Arguments : fpath : str File location (ie, complete path) trunc : str Leftmost portion of path to remove","title":"file_path"},{"location":"api_ref/#check_lock","text":"def check_lock(dv_url, study, apikey) -> bool Checks study lock status; returns True if locked. Arguments : dvurl : str URL of Dataverse installation study - str Persistent ID of study apikey : str API key for user","title":"check_lock"},{"location":"api_ref/#force_notab_unlock","text":"def force_notab_unlock(study, dv_url, fid, apikey, try_uningest=True) -> int Forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Returns 0 if unlocked, file id if locked (and then unlocked). Arguments : study : str Persistent indentifer of study dv_url : str URL to base Dataverse installation fid : str File ID for file object apikey : str API key for user try_uningest : bool Try to uningest the file that was locked. - Default - True","title":"force_notab_unlock"},{"location":"api_ref/#uningest_file","text":"def uningest_file(dv_url, fid, apikey, study='n/a') Tries to uningest a file that has been ingested. Requires superuser API key. Arguments : dv_url : str URL to base Dataverse installation fid : int or str File ID of file to uningest apikey : str API key for superuser study : str Optional handle parameter for log messages","title":"uningest_file"},{"location":"api_ref/#upload_file","text":"def upload_file(fpath, hdl, **kwargs) Uploads file to Dataverse study and sets file metadata and tags. Arguments : fpath : str file location (ie, complete path) hdl : str Dataverse persistent ID for study (handle or DOI) kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user descr : str OPTIONAL file description md5 : str OPTIONAL md5sum for file checking tags : list OPTIONAL list of text file tags. Eg [\u2018Data\u2019, \u2018June 2020\u2019] dirlabel : str OPTIONAL Unix style relative pathname for Dataverse file path: eg: path/to/file/ nowait : bool OPTIONAL Force a file unlock and uningest instead of waiting for processing to finish trunc : str OPTIONAL Leftmost portion of path to remove rest : bool OPTIONAL Restrict file. Defaults to false unless True supplied mimetype : str OPTIONAL Mimetype of file. Useful if using File Previewers. Mimetype for zip files (application/zip) will be ignored to circumvent Dataverse\u2019s automatic unzipping function. label : str OPTIONAL If included in kwargs, this value will be used for the label timeout = int OPTIONAL Timeout in seconds","title":"upload_file"},{"location":"api_ref/#restrict_file","text":"def restrict_file(**kwargs) Restrict file in Dataverse study. Arguments : kwargs : dict other parameters. Acceptable keywords and contents are: One of pid or fid is required pid : str file persistent ID fid : str file database ID dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user rest : bool On True, restrict. Default True","title":"restrict_file"},{"location":"api_ref/#upload_from_tsv","text":"def upload_from_tsv(fil, hdl, **kwargs) Utility for bulk uploading. Assumes fil is formatted as tsv with headers \u2018file\u2019, \u2018description\u2019, \u2018tags\u2019. \u2018tags\u2019 field will be split on commas. Arguments : fil : filelike object Open file object or io.IOStream() hdl : str Dataverse persistent ID for study (handle or DOI) trunc : str Leftmost portion of Dataverse study file path to remove. - eg - trunc =\u2019/home/user/\u2019 if the tsv field is \u2018/home/user/Data/ASCII\u2019 would set the path for that line of the tsv to \u2018Data/ASCII\u2019. Defaults to None. kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - \u2018https://abacus.library.ubc.ca\u2019 apikey : str REQUIRED API key for user rest : bool On True, restrict access. Default False","title":"upload_from_tsv"},{"location":"api_ref/#dataverse_utilsldc","text":"Creates dataverse JSON from Linguistic Data Consortium website page.","title":"dataverse_utils.ldc"},{"location":"api_ref/#ldc-objects","text":"class Ldc(ds.Serializer) An LDC item (eg, LDC2021T01)","title":"Ldc Objects"},{"location":"api_ref/#__init___4","text":"def __init__(ldc, cert=None) Returns a dict with keys created from an LDC catalogue web page. Arguments : ldc : str Linguistic Consortium Catalogue Number (eg. \u2018LDC2015T05\u2019. This is what forms the last part of the LDC catalogue URL. cert : str Path to certificate chain; LDC has had a problem with intermediate certificates, so you can download the chain with a browser and supply a path to the .pem with this parameter","title":"__init__"},{"location":"api_ref/#ldcjson","text":"@property def ldcJson() Returns a JSON based on the LDC web page scraping","title":"ldcJson"},{"location":"api_ref/#dryadjson","text":"@property def dryadJson() LDC metadata in Dryad JSON format","title":"dryadJson"},{"location":"api_ref/#dvjson","text":"@property def dvJson() LDC metadata in Dataverse JSON format","title":"dvJson"},{"location":"api_ref/#embargo","text":"@property def embargo() Boolean indicating embargo status","title":"embargo"},{"location":"api_ref/#filejson","text":"@property def fileJson(timeout=45) Returns False: No attached files possible at LDC","title":"fileJson"},{"location":"api_ref/#files","text":"@property def files() Returns None. No files possible","title":"files"},{"location":"api_ref/#fetch_record","text":"def fetch_record(url=None, timeout=45) Downloads record from LDC website","title":"fetch_record"},{"location":"api_ref/#make_ldc_json","text":"def make_ldc_json() Returns a dict with keys created from an LDC catalogue web page.","title":"make_ldc_json"},{"location":"api_ref/#name_parser","text":"@staticmethod def name_parser(name) Returns lastName/firstName JSON snippet from name Arguments : name : str A name","title":"name_parser"},{"location":"api_ref/#make_dryad_json","text":"def make_dryad_json(ldc=None) Creates a Dryad-style dict from an LDC dictionary Arguments : ldc : dict Dictionary containing LDC data. Defaults to self.ldcJson","title":"make_dryad_json"},{"location":"api_ref/#find_block_index","text":"@staticmethod def find_block_index(dvjson, key) Finds the index number of an item in Dataverse\u2019s idiotic JSON list Arguments : dvjson : dict Dataverse JSON key : str key for which to find list index","title":"find_block_index"},{"location":"api_ref/#make_dv_json","text":"def make_dv_json(ldc=None) Returns complete Dataverse JSON Arguments : ldc : dict LDC dictionary. Defaults to self.ldcJson","title":"make_dv_json"},{"location":"api_ref/#upload_metadata","text":"def upload_metadata(**kwargs) -> dict Uploads metadata to dataverse Returns json from connection attempt. Arguments : kwargs: url : str base url to Dataverse key : str api key dv : str Dataverse to which it is being uploaded","title":"upload_metadata"},{"location":"credits/","text":"Credits \u00b6 Dataverse_utils and their associated command line programs were written by Paul Lesack . Testing and UI suggestions from Jeremy Buhler . This is a product of the University of British Columbia Library Research Commons .","title":"Credits"},{"location":"credits/#credits","text":"Dataverse_utils and their associated command line programs were written by Paul Lesack . Testing and UI suggestions from Jeremy Buhler . This is a product of the University of British Columbia Library Research Commons .","title":"Credits"},{"location":"faq/","text":"Frequently asked questions \u00b6 \u201cFrequently\u201d may be relative. 1. I\u2019m using Windows and the scripts don\u2019t seem to be working/recognized by [choice of command line interface here] \u00b6 There are (at least) four different ways to get some sort of command line access. The traditional command line, PowerShell, via SSH and Git bash. That\u2019s not even including the linux subsystem. The document on Windows script troubles gives common solutions. 2. I am using Windows [7-10]. I\u2019ve installed via pip using a virtual environment, but they don\u2019t use my virtual environment\u2019s Python. \u00b6 What seems like a simple problem is surprisingly complex, as outlined here: https://matthew-brett.github.io/pydagogue/installing_scripts.html . This is further complicated by some other factors. If you have only one Python installation on your machine, you are probably OK. Mind you, if you\u2019re looking at this and that\u2019s the case, maybe I\u2019m wrong. If you have multiple Pythons, you can try changing your environment variables to point to the correct Python. If you are using ArcGIS products, ArcGIS may write items into your Windows registry which will associate any .py file with it\u2019s grossly outdated Python 2.7, or just the wrong Python. More specifically: Computer\\HKEY_CLASSES_ROOT\\Python.File\\shell\\open\\command in the Windows registry. Unfortunately, the Windows computer that I have available for testing (as I normally use Linux or Mac) does not have an administrator account, so I can\u2019t confirm that changing this key will work (although there\u2019s no reason to believe it won\u2019t). This is a pain. Is there something less irritating that would work? \u00b6 Yes, there is. You can still run the scripts manually. There are two options for this. Download the repository via git to a convenient place and use the files in the scripts/ directory Point your %PATH% to where the scripts are installed. To find out where they are installed: TLDR: version: Point your %PATH% (and use first part of the path) to point to your [venv]\\Scripts directory, because they\u2019re probably there. Long winded instructions/explanation: Start a Python session: import sys sys.path [x for x in sys.path if x.endswith(\u2018site-packages\u2019)] The location of the scripts will be written in [whatever the output of sys.path]/dataverse_utils[somestuff]egg-info/installed-files.txt , usually three levels up in the scripts directory. Ironically, this is also the location of the activate portion of the comand required to start a virtual environment (if you are using a virtual environment). For some background on this, venv in Linux and Mac versions of Python uses bin , and reserves scripts for the scripts directory. Windows, however, uses Scripts for the venv module, and to make it worse it\u2019s not usually case sensitive, so anything in scripts gets put into Scripts .","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"\u201cFrequently\u201d may be relative.","title":"Frequently asked questions"},{"location":"faq/#1-im-using-windows-and-the-scripts-dont-seem-to-be-workingrecognized-by-choice-of-command-line-interface-here","text":"There are (at least) four different ways to get some sort of command line access. The traditional command line, PowerShell, via SSH and Git bash. That\u2019s not even including the linux subsystem. The document on Windows script troubles gives common solutions.","title":"1. I'm using Windows and the scripts don't seem to be working/recognized by [choice of command line interface here]"},{"location":"faq/#2-i-am-using-windows-7-10-ive-installed-via-pip-using-a-virtual-environment-but-they-dont-use-my-virtual-environments-python","text":"What seems like a simple problem is surprisingly complex, as outlined here: https://matthew-brett.github.io/pydagogue/installing_scripts.html . This is further complicated by some other factors. If you have only one Python installation on your machine, you are probably OK. Mind you, if you\u2019re looking at this and that\u2019s the case, maybe I\u2019m wrong. If you have multiple Pythons, you can try changing your environment variables to point to the correct Python. If you are using ArcGIS products, ArcGIS may write items into your Windows registry which will associate any .py file with it\u2019s grossly outdated Python 2.7, or just the wrong Python. More specifically: Computer\\HKEY_CLASSES_ROOT\\Python.File\\shell\\open\\command in the Windows registry. Unfortunately, the Windows computer that I have available for testing (as I normally use Linux or Mac) does not have an administrator account, so I can\u2019t confirm that changing this key will work (although there\u2019s no reason to believe it won\u2019t).","title":"2. I am using Windows [7-10]. I've installed via pip using a virtual environment, but they don't use my virtual environment's Python."},{"location":"faq/#this-is-a-pain-is-there-something-less-irritating-that-would-work","text":"Yes, there is. You can still run the scripts manually. There are two options for this. Download the repository via git to a convenient place and use the files in the scripts/ directory Point your %PATH% to where the scripts are installed. To find out where they are installed: TLDR: version: Point your %PATH% (and use first part of the path) to point to your [venv]\\Scripts directory, because they\u2019re probably there. Long winded instructions/explanation: Start a Python session: import sys sys.path [x for x in sys.path if x.endswith(\u2018site-packages\u2019)] The location of the scripts will be written in [whatever the output of sys.path]/dataverse_utils[somestuff]egg-info/installed-files.txt , usually three levels up in the scripts directory. Ironically, this is also the location of the activate portion of the comand required to start a virtual environment (if you are using a virtual environment). For some background on this, venv in Linux and Mac versions of Python uses bin , and reserves scripts for the scripts directory. Windows, however, uses Scripts for the venv module, and to make it worse it\u2019s not usually case sensitive, so anything in scripts gets put into Scripts .","title":"This is a pain. Is there something less irritating that would work?"},{"location":"scripts/","text":"Console utilities \u00b6 code { white-space : pre-wrap !important; } These utilities are available at the command line/command prompt and don\u2019t require any Python knowledge except how to install a Python library via pip, as outlined in the overview document. Once installed via pip, the scripts will be available via the command line and will not require calling Python explicitly. That is, they can be called from the command line directly. For example: dv_tsv_manifest is all you will need to type. Note that these programs have been primarily tested on Linux and MacOS, with Windows a tertiary priority . Windows is notable for its unusual file handling, so, as the MIT licence stipulates, there is no warranty as to the suitability for a particular purpose. In alphabetical order: dv_del \u00b6 This is bulk deletion utility for unpublished studies (or even single studies). It\u2019s useful when your automated procedures have gone wrong, or if you don\u2019t feel like navigating through many menus. Note the -i switch which can ask for manual confirmation of deletions. Usage usage: dv_del [-h] -k KEY [-d DATAVERSE | -p PID] [-i] [-u DVURL] [--version] Delete draft studies from a Dataverse collection options: -h, --help show this help message and exit -k KEY, --key KEY Dataverse user API key -d DATAVERSE, --dataverse DATAVERSE Dataverse collection short name from which to delete all draft records. eg. \"ldc\" -p PID, --persistentId PID Handle or DOI to delete in format hdl:11272.1/FK2/12345 -i, --interactive Confirm each study deletion -u DVURL, --url DVURL URL to base Dataverse installation --version Show version number and exit dv_ldc_uploader \u00b6 This is a very specialized utility which will scrape metadata from the Linguistic Data Consortium (LDC) and create a metadata record in a Dataverse. The LDC does not have an API, so the metadata is scraped from their web site. This means that the metadata may not be quite as controlled as that which comes from an API. Data from the LDC website is converted to Dryad -style JSON via dataverse_utils.ldc via the use of the dryad2dataverse library. There are two main methods of use for this utility: Multiple metadata uploads. Multiple LDC record numbers can be supplied and a study without files will be created for each one. If a TSV file with file information is upplied via the -t or --tsv switch, the utility will upload a single LDC study and upload the contents of the tsv file to the created record. Important note As of early 2023, the LDC website is not supported by certifi . You will need to manually supply a certificate chain to use the utility. To obtain the certificate chain (in Firefox) perform the following steps: Select Tools/Page Info In the Security tab, select View Certificate Scroll to \u201cPEM (chain)\u201d Right click and \u201cSave link as\u201d Use this file for the -c/\u2013certchain option below. Searching for \u201cdownload pem certificate chain [browser]\u201d in a search engine will undoubtedly bring up results for whatever browser you like. Usage usage: dv_ldc_uploader [-h] [-u URL] -k KEY [-d DVS] [-t TSV] [-r] [-n CNAME] [-c CERTCHAIN] [-e EMAIL] [-v] [--version] studies [studies ...] Linguistic Data Consortium metadata uploader for Dataverse. This utility will scrape the metadata from the LDC website (https://catalog.ldc.upenn.edu) and upload data based on a TSV manifest. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: studies LDC Catalogue numbers to process, separated by spaces. eg. \"LDC2012T19 LDC2011T07\". Case is ignored, so \"ldc2012T19\" will also work. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -d DVS, --dvs DVS Short name of target Dataverse collection (eg: ldc). Defaults to \"ldc\" -t TSV, --tsv TSV Manifest tsv file for uploading and metadata. If not supplied, only metadata will be uploaded. Using this option requires only one positional *studies* argument -r, --no-restrict Don't restrict files after upload. -n CNAME, --cname CNAME Study contact name. Default: \"Abacus support\" -c CERTCHAIN, --certchain CERTCHAIN Certificate chain PEM: use if SSL issues are present. The PEM chain must be downloaded with a browser. Default: None -e EMAIL, --email EMAIL Dataverse study contact email address. Default: abacus-support@lists.ubc.ca -v, --verbose Verbose output --version Show version number and exit dv_manifest_gen \u00b6 Not technically a Dataverse-specific script, this utility will generate a tab-separated value output. The file consists of 3 columns: file, description and tags , and optionally a mimetype column. Editing the result and using the upload utility to parse the tsv will add descriptive metadata, tags and file paths to an upload instead of laboriously using the Dataverse GUI. Tags may be separated by commas, eg: \u201cData, SAS, June 2021\u201d. Using stdout and a redirect will also save time. First dump a file as normal. Add other files to the end with different information using the exclude header switch -x and different tags along with output redirection >> . Usage usage: dv_manifest_gen [-h] [-f FILENAME] [-t TAG] [-x] [-r] [-q QUOTE] [-a] [-m] [--version] [files ...] Creates a file manifest in tab separated value format which can then be edited and used for file uploads to a Dataverse collection. Files can be edited to add file descriptions and comma-separated tags that will be automatically attached to metadata using products using the dataverse_utils library. Will dump to stdout unless -f or --filename is used. Using the command and a dash (ie, \"dv_manifest_gen.py -\" produces full paths for some reason. positional arguments: files Files to add to manifest. Leaving it blank will add all files in the current directory. If using -r will recursively show all. options: -h, --help show this help message and exit -f FILENAME, --filename FILENAME Save to file instead of outputting to stdout -t TAG, --tag TAG Default tag(s). Separate with comma and use quotes if there are spaces. eg. \"Data, June 2021\". Defaults to \"Data\" -x, --no-header Don't include header in output. Useful if creating a complex tsv using redirects (ie, \">>\"). -r, --recursive Recursive listing. -q QUOTE, --quote QUOTE Quote type. Cell value quoting parameters. Options: none (no quotes), min (minimal, ie. special characters only )nonum (non-numeric), all (all cells). Default: min -a, --show-hidden Include hidden files. -m, --mime Include autodetected mimetypes --version Show version number and exit dv_pg_facet_date \u00b6 This specialized tool is designed to be run on the server on which the Dataverse installation exists. When material is published in a Dataverse installation, the \u201cPublication Year\u201d facet in the Dataverse GUI is automatically populated with a date, which is the publication date in that Dataverse installation . This makes sense from the point of view of research data which is first deposited into a Dataverse installation, but fails as a finding aid for either; older data sets that have been migrated and reingested licensed data sets which may have been published years before they were purchased and ingested. For example, if you have a dataset that was published in 1971 but you only added it to your Dataverse installation in 2021, it is not necessarily intuitive to the end user that the \u201cpublication date\u201d in this instance would be 2021. Ideally, you might like it to be 1971. Unfortunately, there is no API-based tool to manage this date. The only way to change it, as of late 2021, is to modify the underlying PostgreSQL database directly with the desired date. Subsequently, the study must be reindexed so that the revised publication date appears as an option in the facet. This tool will perform those operations. However, the tool must be run on the server on which the Dataverse installation exists, as reindexing API calls must be from localhost and database access is necessarily restricted. There are a few other prerequisites for using this tool which differ from the rest of the scripts included in this package. The user must have shell access to the server hosting the Dataverse installation Python 3.6 or higher must be installed The user must possess a valid Dataverse API key The user must know the PostgreSQL password If the database name and user have been changed, the user must know this as well The script requires the manual installation of psycopg2-binary or have a successfully compiled psycopg2 package for Python. See https://www.psycopg.org/docs/ . This is not installed with the normal pip install of the dataverse_utils package as none of the other scripts require it and, in general, the odds of someone using this utility are low. If you forget to install it, the program will politely remind you. This cannot be stressed enough. This tool will directly change values within the PostgreSQL database which holds all of Dataverse\u2019s information . Use this at your own risk; no warranty is implied and no responsibility will be accepted for data loss, etc. If any of the options listed for the utility make no sense to you or sound like gibberish, do not use this tool. Because editing the underlying database may have a high pucker factor for some, there is both a dry-run option and an option to just dump out SQL instead of actually touching anything. These two options do not perform a study reindex and don\u2019t alter the contents of the database. Usage usage: dv_pg_facet_date [-h] [-d DBNAME] [-u USER] -p PASSWORD [-r | -o] [-s] -k KEY [-w URL] [--version] pids [pids ...] {distributionDate,productionDate,dateOfDeposit,dist,prod,dep} A utility to change the 'Production Date' web interface facet in a Dataverse installation to one of the three acceptable date types: 'distributionDate', 'productionDate', or 'dateOfDeposit'. This must be done in the PostgreSQL database directly, so this utility must be run on the *server* that hosts a Dataverse installation. Back up your database if you are unsure. positional arguments: pids persistentIdentifier {distributionDate,productionDate,dateOfDeposit,dist,prod,dep} date type which is to be shown in the facet. The short forms are aliases for the long forms. optional arguments: -h, --help show this help message and exit -d DBNAME, --dbname DBNAME Database name -u USER, --user USER PostgreSQL username -p PASSWORD, --password PASSWORD PostgreSQL password -r, --dry-run print proposed SQL to stdout -o, --sql-only dump sql to file called *pg_sql.sql* in current directory. Appends to file if it exists -s, --save-old Dump old values to tsv called *pg_changed.tsv* in current directory. Appends to file if it exists -k KEY, --key KEY API key for Dataverse installation. -w URL, --url URL URL for base Dataverse installation. Default https://abacus.library.ubc.ca --version Show version number and exit THIS WILL EDIT YOUR POSTGRESQL DATABASE DIRECTLY. USE AT YOUR OWN RISK. dv_record_copy \u00b6 Copies an existing Dataverse study metadata record to a target collection, or replaces a currently existing record. Files are not copied, only the study record. This utility is useful for mateial which is in a series, requiring only minor changes for each iteration. Usage usage: dv_record_copy [-h] [-u URL] -k KEY (-c COLLECTION | -r REPLACE) [-v] pid Record duplicator for Dataverse. This utility will download a Dataverse record And then upload the study level metadata into a new record in a user-specified collection. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: pid PID of original dataverse recordseparated by spaces. eg. \"LDC2012T19 LDC2011T07\". Case is ignored, so \"ldc2012T19\" will also work. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -c COLLECTION, --collection COLLECTION Short name of target Dataverse collection (eg: ldc). Defaults to \"ldc\" -r REPLACE, --replace REPLACE Replace metadata data in record with this PID -v, --version Show version number and exit dv_release \u00b6 A bulk release utility for Dataverse. This utility will normally be used after a migration or large data transfer, such as a dryad2dataverse transfer from the Dryad data repository. It can release studies individually by persistent ID or just release all unreleased files in a Dataverse. Usage usage: dv_release [-h] [-u URL] -k KEY [-i] [--time STIME] [-v] [-r] [-d DV | -p PID [PID ...]] [--version] Bulk file releaser for unpublished Dataverse studies. Either releases individual studies or all unreleased studies in a single Dataverse collection. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Default: https://abacus.library.ubc.ca -k KEY, --key KEY API key -i, --interactive Manually confirm each release --time STIME, -t STIME Time between release attempts in seconds. Default 10 -v Verbose mode -r, --dry-run Only output a list of studies to be released -d DV, --dv DV Short name of Dataverse collection to process (eg: statcan) -p PID [PID ...], --pid PID [PID ...] Handles or DOIs to release in format hdl:11272.1/FK2/12345 or doi:10.80240/FK2/NWRABI. Multiple values OK --version Show version number and exit dv_replace_licen[cs]e \u00b6 This will replace the text in a record with the text Markdown file. Text is converted to HTML. Optionally, the record can be republished without incrementing the version (ie, with type=updatecurrent . usage: dv_replace_licence [-h] [-u URL] -l LIC -k KEY [-r] [--version] studies [studies ...] Replaces the licence text in a Dataverse study and [optionally] republishes it as the same version. Superuser privileges are required for republishing as the version is not incremented. This software requires the Dataverse installation to be running Dataverse software version >= 5.6. positional arguments: studies Persistent IDs of studies options: -h, --help show this help message and exit -u URL, --url URL Base URL of Dataverse installation. Defaults to \"https://abacus.library.ubc.ca\" -l LIC, --licence LIC Licence file in Markdown format -k KEY, --key KEY Dataverse API key -r, --republish Republish study without incrementing version --version Show version number and exit dv_study_migrator \u00b6 If for some reason you need to copy everything from a Dataverse record to a different Dataverse installation or a different collection, this utility will do it for you. Metadata, file names, paths, restrictions etc will all be copied. There are some limitations, though, as only the most recent version will be copied and date handling is done on the target server. The utility will either copy records specifice with a persistent identifer (PID) to a target collection on the same or another server, or replace records with an existing PID. usage: dv_study_migrator [-h] -s SOURCE_URL -a SOURCE_KEY -t TARGET_URL -b TARGET_KEY [-o TIMEOUT] (-c COLLECTION | -r REPLACE [REPLACE ...]) [-v] pids [pids ...] Record migrator for Dataverse. This utility will take the most recent version of a study from one Dataverse installation and copy the metadata and records to another, completely separate dataverse installation. You could also use it to copy records from one collection to another. positional arguments: pids PID(s) of original Dataverse record(s) in source Dataverse separated by spaces. eg. \"hdl:11272.1/AB2/JEG5RH doi:11272.1/AB2/JEG5RH\". Case is ignored. options: -h, --help show this help message and exit -s SOURCE_URL, --source_url SOURCE_URL Source Dataverse installation base URL. -a SOURCE_KEY, --source_key SOURCE_KEY API key for source Dataverse installation. -t TARGET_URL, --target_url TARGET_URL Source Dataverse installation base URL. -b TARGET_KEY, --target_key TARGET_KEY API key for target Dataverse installation. -o TIMEOUT, --timeout TIMEOUT Request timeout in seconds. Default 100. -c COLLECTION, --collection COLLECTION Short name of target Dataverse collection (eg: dli). -r REPLACE [REPLACE ...], --replace REPLACE [REPLACE ...] Replace data in these target PIDs with data from the source PIDS. Number of PIDs listed here must match the number of PID arguments to follow. That is, the number of records must be equal. Records will be matched on a 1-1 basis in order. For example: [rest of command] -r doi:123.34/etc hdl:12323/AB/SOMETHI will replace the record with identifier 'doi' with the data from 'hdl'. Make sure you don't use this as the penultimate switch, because then it's not possible to disambiguate PIDS from this argument and positional arguments. ie, something like dv_study_migrator -r blah blah -s http//test.invalid etc. -v, --version Show version number and exit dv_upload_tsv \u00b6 Now that you have a tsv full of nicely described data, you can easily upload it to an existing study if you know the persistent ID and have an API key. For the best metadata, you should probably edit it manually to add correct descriptive metadata, notably the \u201cDescription\u201d and \u201cTags\u201d. Tags are split separated by commas, so it\u2019s possible to have multiple tags for each data item, like \u201cData, SPSS, June 2021\u201d. File paths are automatically generated from the \u201cfile\u201d column. Because of this, you should probably use relative paths rather than absolute paths unless you want to have a lengthy path string in Dataverse. If uploading a tsv which includes mimetypes, be aware that mimetypes for zip files will be ignored to circumvent Dataverse\u2019s automatic unzipping feature. The rationale for manually specifiying mimetypes is to enable the use of previews which require a specific mimetype to function, but Dataverse does not correctly detect the type. For example, the GeoJSON file previewer requires a mimetype of application/geo+json , but the detection of this mimetype is not supported until Dataverse v5.9. By manually setting the mimetype, the previewer can be used by earlier Dataverse versions. Usage usage: dv_upload_tsv [-h] -p PID -k KEY [-u URL] [-r] [-n] [-t TRUNCATE] [--version] tsv Uploads data sets to an *existing* Dataverse study from the contents of a TSV (tab separated value) file. Metadata, file tags, paths, etc are all read from the TSV. JSON output from the Dataverse API is printed to stdout during the process. By default, files will be unrestricted but the utility will ask for confirmation before uploading. positional arguments: tsv TSV file to upload options: -h, --help show this help message and exit -p PID, --pid PID Dataverse study persistent identifier (DOI/handle) -k KEY, --key KEY API key -u URL, --url URL Dataverse installation base url. defaults to \"https://abacus.library.ubc.ca\" -r, --restrict Restrict files after upload. -n, --no-confirm Don't confirm non-restricted status -t TRUNCATE, --truncate TRUNCATE Left truncate file path. As Dataverse studies can retain directory structure, you can set an arbitrary starting point by removing the leftmost portion. Eg: if the TSV has a file path of /home/user/Data/file.txt, setting --truncate to \"/home/user\" would have file.txt in the Data directory in the Dataverse study. The file is still loaded from the path in the spreadsheet. Defaults to no truncation. --version Show version number and exit Notes for Windows users \u00b6 Command line scripts for Python may not necessarily behave the way they do in Linux/Mac, depending on how you access them. For detailed information on Windows systems, please see the Windows testing document","title":"Console utilities"},{"location":"scripts/#console-utilities","text":"code { white-space : pre-wrap !important; } These utilities are available at the command line/command prompt and don\u2019t require any Python knowledge except how to install a Python library via pip, as outlined in the overview document. Once installed via pip, the scripts will be available via the command line and will not require calling Python explicitly. That is, they can be called from the command line directly. For example: dv_tsv_manifest is all you will need to type. Note that these programs have been primarily tested on Linux and MacOS, with Windows a tertiary priority . Windows is notable for its unusual file handling, so, as the MIT licence stipulates, there is no warranty as to the suitability for a particular purpose. In alphabetical order:","title":"Console utilities"},{"location":"scripts/#dv_del","text":"This is bulk deletion utility for unpublished studies (or even single studies). It\u2019s useful when your automated procedures have gone wrong, or if you don\u2019t feel like navigating through many menus. Note the -i switch which can ask for manual confirmation of deletions. Usage usage: dv_del [-h] -k KEY [-d DATAVERSE | -p PID] [-i] [-u DVURL] [--version] Delete draft studies from a Dataverse collection options: -h, --help show this help message and exit -k KEY, --key KEY Dataverse user API key -d DATAVERSE, --dataverse DATAVERSE Dataverse collection short name from which to delete all draft records. eg. \"ldc\" -p PID, --persistentId PID Handle or DOI to delete in format hdl:11272.1/FK2/12345 -i, --interactive Confirm each study deletion -u DVURL, --url DVURL URL to base Dataverse installation --version Show version number and exit","title":"dv_del"},{"location":"scripts/#dv_ldc_uploader","text":"This is a very specialized utility which will scrape metadata from the Linguistic Data Consortium (LDC) and create a metadata record in a Dataverse. The LDC does not have an API, so the metadata is scraped from their web site. This means that the metadata may not be quite as controlled as that which comes from an API. Data from the LDC website is converted to Dryad -style JSON via dataverse_utils.ldc via the use of the dryad2dataverse library. There are two main methods of use for this utility: Multiple metadata uploads. Multiple LDC record numbers can be supplied and a study without files will be created for each one. If a TSV file with file information is upplied via the -t or --tsv switch, the utility will upload a single LDC study and upload the contents of the tsv file to the created record. Important note As of early 2023, the LDC website is not supported by certifi . You will need to manually supply a certificate chain to use the utility. To obtain the certificate chain (in Firefox) perform the following steps: Select Tools/Page Info In the Security tab, select View Certificate Scroll to \u201cPEM (chain)\u201d Right click and \u201cSave link as\u201d Use this file for the -c/\u2013certchain option below. Searching for \u201cdownload pem certificate chain [browser]\u201d in a search engine will undoubtedly bring up results for whatever browser you like. Usage usage: dv_ldc_uploader [-h] [-u URL] -k KEY [-d DVS] [-t TSV] [-r] [-n CNAME] [-c CERTCHAIN] [-e EMAIL] [-v] [--version] studies [studies ...] Linguistic Data Consortium metadata uploader for Dataverse. This utility will scrape the metadata from the LDC website (https://catalog.ldc.upenn.edu) and upload data based on a TSV manifest. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: studies LDC Catalogue numbers to process, separated by spaces. eg. \"LDC2012T19 LDC2011T07\". Case is ignored, so \"ldc2012T19\" will also work. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -d DVS, --dvs DVS Short name of target Dataverse collection (eg: ldc). Defaults to \"ldc\" -t TSV, --tsv TSV Manifest tsv file for uploading and metadata. If not supplied, only metadata will be uploaded. Using this option requires only one positional *studies* argument -r, --no-restrict Don't restrict files after upload. -n CNAME, --cname CNAME Study contact name. Default: \"Abacus support\" -c CERTCHAIN, --certchain CERTCHAIN Certificate chain PEM: use if SSL issues are present. The PEM chain must be downloaded with a browser. Default: None -e EMAIL, --email EMAIL Dataverse study contact email address. Default: abacus-support@lists.ubc.ca -v, --verbose Verbose output --version Show version number and exit","title":"dv_ldc_uploader"},{"location":"scripts/#dv_manifest_gen","text":"Not technically a Dataverse-specific script, this utility will generate a tab-separated value output. The file consists of 3 columns: file, description and tags , and optionally a mimetype column. Editing the result and using the upload utility to parse the tsv will add descriptive metadata, tags and file paths to an upload instead of laboriously using the Dataverse GUI. Tags may be separated by commas, eg: \u201cData, SAS, June 2021\u201d. Using stdout and a redirect will also save time. First dump a file as normal. Add other files to the end with different information using the exclude header switch -x and different tags along with output redirection >> . Usage usage: dv_manifest_gen [-h] [-f FILENAME] [-t TAG] [-x] [-r] [-q QUOTE] [-a] [-m] [--version] [files ...] Creates a file manifest in tab separated value format which can then be edited and used for file uploads to a Dataverse collection. Files can be edited to add file descriptions and comma-separated tags that will be automatically attached to metadata using products using the dataverse_utils library. Will dump to stdout unless -f or --filename is used. Using the command and a dash (ie, \"dv_manifest_gen.py -\" produces full paths for some reason. positional arguments: files Files to add to manifest. Leaving it blank will add all files in the current directory. If using -r will recursively show all. options: -h, --help show this help message and exit -f FILENAME, --filename FILENAME Save to file instead of outputting to stdout -t TAG, --tag TAG Default tag(s). Separate with comma and use quotes if there are spaces. eg. \"Data, June 2021\". Defaults to \"Data\" -x, --no-header Don't include header in output. Useful if creating a complex tsv using redirects (ie, \">>\"). -r, --recursive Recursive listing. -q QUOTE, --quote QUOTE Quote type. Cell value quoting parameters. Options: none (no quotes), min (minimal, ie. special characters only )nonum (non-numeric), all (all cells). Default: min -a, --show-hidden Include hidden files. -m, --mime Include autodetected mimetypes --version Show version number and exit","title":"dv_manifest_gen"},{"location":"scripts/#dv_pg_facet_date","text":"This specialized tool is designed to be run on the server on which the Dataverse installation exists. When material is published in a Dataverse installation, the \u201cPublication Year\u201d facet in the Dataverse GUI is automatically populated with a date, which is the publication date in that Dataverse installation . This makes sense from the point of view of research data which is first deposited into a Dataverse installation, but fails as a finding aid for either; older data sets that have been migrated and reingested licensed data sets which may have been published years before they were purchased and ingested. For example, if you have a dataset that was published in 1971 but you only added it to your Dataverse installation in 2021, it is not necessarily intuitive to the end user that the \u201cpublication date\u201d in this instance would be 2021. Ideally, you might like it to be 1971. Unfortunately, there is no API-based tool to manage this date. The only way to change it, as of late 2021, is to modify the underlying PostgreSQL database directly with the desired date. Subsequently, the study must be reindexed so that the revised publication date appears as an option in the facet. This tool will perform those operations. However, the tool must be run on the server on which the Dataverse installation exists, as reindexing API calls must be from localhost and database access is necessarily restricted. There are a few other prerequisites for using this tool which differ from the rest of the scripts included in this package. The user must have shell access to the server hosting the Dataverse installation Python 3.6 or higher must be installed The user must possess a valid Dataverse API key The user must know the PostgreSQL password If the database name and user have been changed, the user must know this as well The script requires the manual installation of psycopg2-binary or have a successfully compiled psycopg2 package for Python. See https://www.psycopg.org/docs/ . This is not installed with the normal pip install of the dataverse_utils package as none of the other scripts require it and, in general, the odds of someone using this utility are low. If you forget to install it, the program will politely remind you. This cannot be stressed enough. This tool will directly change values within the PostgreSQL database which holds all of Dataverse\u2019s information . Use this at your own risk; no warranty is implied and no responsibility will be accepted for data loss, etc. If any of the options listed for the utility make no sense to you or sound like gibberish, do not use this tool. Because editing the underlying database may have a high pucker factor for some, there is both a dry-run option and an option to just dump out SQL instead of actually touching anything. These two options do not perform a study reindex and don\u2019t alter the contents of the database. Usage usage: dv_pg_facet_date [-h] [-d DBNAME] [-u USER] -p PASSWORD [-r | -o] [-s] -k KEY [-w URL] [--version] pids [pids ...] {distributionDate,productionDate,dateOfDeposit,dist,prod,dep} A utility to change the 'Production Date' web interface facet in a Dataverse installation to one of the three acceptable date types: 'distributionDate', 'productionDate', or 'dateOfDeposit'. This must be done in the PostgreSQL database directly, so this utility must be run on the *server* that hosts a Dataverse installation. Back up your database if you are unsure. positional arguments: pids persistentIdentifier {distributionDate,productionDate,dateOfDeposit,dist,prod,dep} date type which is to be shown in the facet. The short forms are aliases for the long forms. optional arguments: -h, --help show this help message and exit -d DBNAME, --dbname DBNAME Database name -u USER, --user USER PostgreSQL username -p PASSWORD, --password PASSWORD PostgreSQL password -r, --dry-run print proposed SQL to stdout -o, --sql-only dump sql to file called *pg_sql.sql* in current directory. Appends to file if it exists -s, --save-old Dump old values to tsv called *pg_changed.tsv* in current directory. Appends to file if it exists -k KEY, --key KEY API key for Dataverse installation. -w URL, --url URL URL for base Dataverse installation. Default https://abacus.library.ubc.ca --version Show version number and exit THIS WILL EDIT YOUR POSTGRESQL DATABASE DIRECTLY. USE AT YOUR OWN RISK.","title":"dv_pg_facet_date"},{"location":"scripts/#dv_record_copy","text":"Copies an existing Dataverse study metadata record to a target collection, or replaces a currently existing record. Files are not copied, only the study record. This utility is useful for mateial which is in a series, requiring only minor changes for each iteration. Usage usage: dv_record_copy [-h] [-u URL] -k KEY (-c COLLECTION | -r REPLACE) [-v] pid Record duplicator for Dataverse. This utility will download a Dataverse record And then upload the study level metadata into a new record in a user-specified collection. Please note that this utility was built with the Abacus repository (https://abacus.library.ubc.ca) in mind, so many of the defaults are specific to that Dataverse installation. positional arguments: pid PID of original dataverse recordseparated by spaces. eg. \"LDC2012T19 LDC2011T07\". Case is ignored, so \"ldc2012T19\" will also work. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Defaults to \"https://abacus.library.ubc.ca\" -k KEY, --key KEY API key -c COLLECTION, --collection COLLECTION Short name of target Dataverse collection (eg: ldc). Defaults to \"ldc\" -r REPLACE, --replace REPLACE Replace metadata data in record with this PID -v, --version Show version number and exit","title":"dv_record_copy"},{"location":"scripts/#dv_release","text":"A bulk release utility for Dataverse. This utility will normally be used after a migration or large data transfer, such as a dryad2dataverse transfer from the Dryad data repository. It can release studies individually by persistent ID or just release all unreleased files in a Dataverse. Usage usage: dv_release [-h] [-u URL] -k KEY [-i] [--time STIME] [-v] [-r] [-d DV | -p PID [PID ...]] [--version] Bulk file releaser for unpublished Dataverse studies. Either releases individual studies or all unreleased studies in a single Dataverse collection. options: -h, --help show this help message and exit -u URL, --url URL Dataverse installation base URL. Default: https://abacus.library.ubc.ca -k KEY, --key KEY API key -i, --interactive Manually confirm each release --time STIME, -t STIME Time between release attempts in seconds. Default 10 -v Verbose mode -r, --dry-run Only output a list of studies to be released -d DV, --dv DV Short name of Dataverse collection to process (eg: statcan) -p PID [PID ...], --pid PID [PID ...] Handles or DOIs to release in format hdl:11272.1/FK2/12345 or doi:10.80240/FK2/NWRABI. Multiple values OK --version Show version number and exit","title":"dv_release"},{"location":"scripts/#dv_replace_licencse","text":"This will replace the text in a record with the text Markdown file. Text is converted to HTML. Optionally, the record can be republished without incrementing the version (ie, with type=updatecurrent . usage: dv_replace_licence [-h] [-u URL] -l LIC -k KEY [-r] [--version] studies [studies ...] Replaces the licence text in a Dataverse study and [optionally] republishes it as the same version. Superuser privileges are required for republishing as the version is not incremented. This software requires the Dataverse installation to be running Dataverse software version >= 5.6. positional arguments: studies Persistent IDs of studies options: -h, --help show this help message and exit -u URL, --url URL Base URL of Dataverse installation. Defaults to \"https://abacus.library.ubc.ca\" -l LIC, --licence LIC Licence file in Markdown format -k KEY, --key KEY Dataverse API key -r, --republish Republish study without incrementing version --version Show version number and exit","title":"dv_replace_licen[cs]e"},{"location":"scripts/#dv_study_migrator","text":"If for some reason you need to copy everything from a Dataverse record to a different Dataverse installation or a different collection, this utility will do it for you. Metadata, file names, paths, restrictions etc will all be copied. There are some limitations, though, as only the most recent version will be copied and date handling is done on the target server. The utility will either copy records specifice with a persistent identifer (PID) to a target collection on the same or another server, or replace records with an existing PID. usage: dv_study_migrator [-h] -s SOURCE_URL -a SOURCE_KEY -t TARGET_URL -b TARGET_KEY [-o TIMEOUT] (-c COLLECTION | -r REPLACE [REPLACE ...]) [-v] pids [pids ...] Record migrator for Dataverse. This utility will take the most recent version of a study from one Dataverse installation and copy the metadata and records to another, completely separate dataverse installation. You could also use it to copy records from one collection to another. positional arguments: pids PID(s) of original Dataverse record(s) in source Dataverse separated by spaces. eg. \"hdl:11272.1/AB2/JEG5RH doi:11272.1/AB2/JEG5RH\". Case is ignored. options: -h, --help show this help message and exit -s SOURCE_URL, --source_url SOURCE_URL Source Dataverse installation base URL. -a SOURCE_KEY, --source_key SOURCE_KEY API key for source Dataverse installation. -t TARGET_URL, --target_url TARGET_URL Source Dataverse installation base URL. -b TARGET_KEY, --target_key TARGET_KEY API key for target Dataverse installation. -o TIMEOUT, --timeout TIMEOUT Request timeout in seconds. Default 100. -c COLLECTION, --collection COLLECTION Short name of target Dataverse collection (eg: dli). -r REPLACE [REPLACE ...], --replace REPLACE [REPLACE ...] Replace data in these target PIDs with data from the source PIDS. Number of PIDs listed here must match the number of PID arguments to follow. That is, the number of records must be equal. Records will be matched on a 1-1 basis in order. For example: [rest of command] -r doi:123.34/etc hdl:12323/AB/SOMETHI will replace the record with identifier 'doi' with the data from 'hdl'. Make sure you don't use this as the penultimate switch, because then it's not possible to disambiguate PIDS from this argument and positional arguments. ie, something like dv_study_migrator -r blah blah -s http//test.invalid etc. -v, --version Show version number and exit","title":"dv_study_migrator"},{"location":"scripts/#dv_upload_tsv","text":"Now that you have a tsv full of nicely described data, you can easily upload it to an existing study if you know the persistent ID and have an API key. For the best metadata, you should probably edit it manually to add correct descriptive metadata, notably the \u201cDescription\u201d and \u201cTags\u201d. Tags are split separated by commas, so it\u2019s possible to have multiple tags for each data item, like \u201cData, SPSS, June 2021\u201d. File paths are automatically generated from the \u201cfile\u201d column. Because of this, you should probably use relative paths rather than absolute paths unless you want to have a lengthy path string in Dataverse. If uploading a tsv which includes mimetypes, be aware that mimetypes for zip files will be ignored to circumvent Dataverse\u2019s automatic unzipping feature. The rationale for manually specifiying mimetypes is to enable the use of previews which require a specific mimetype to function, but Dataverse does not correctly detect the type. For example, the GeoJSON file previewer requires a mimetype of application/geo+json , but the detection of this mimetype is not supported until Dataverse v5.9. By manually setting the mimetype, the previewer can be used by earlier Dataverse versions. Usage usage: dv_upload_tsv [-h] -p PID -k KEY [-u URL] [-r] [-n] [-t TRUNCATE] [--version] tsv Uploads data sets to an *existing* Dataverse study from the contents of a TSV (tab separated value) file. Metadata, file tags, paths, etc are all read from the TSV. JSON output from the Dataverse API is printed to stdout during the process. By default, files will be unrestricted but the utility will ask for confirmation before uploading. positional arguments: tsv TSV file to upload options: -h, --help show this help message and exit -p PID, --pid PID Dataverse study persistent identifier (DOI/handle) -k KEY, --key KEY API key -u URL, --url URL Dataverse installation base url. defaults to \"https://abacus.library.ubc.ca\" -r, --restrict Restrict files after upload. -n, --no-confirm Don't confirm non-restricted status -t TRUNCATE, --truncate TRUNCATE Left truncate file path. As Dataverse studies can retain directory structure, you can set an arbitrary starting point by removing the leftmost portion. Eg: if the TSV has a file path of /home/user/Data/file.txt, setting --truncate to \"/home/user\" would have file.txt in the Data directory in the Dataverse study. The file is still loaded from the path in the spreadsheet. Defaults to no truncation. --version Show version number and exit","title":"dv_upload_tsv"},{"location":"scripts/#notes-for-windows-users","text":"Command line scripts for Python may not necessarily behave the way they do in Linux/Mac, depending on how you access them. For detailed information on Windows systems, please see the Windows testing document","title":"Notes for Windows users"},{"location":"windows/","text":"Running the dataverse_utils console utilities under Windows \u00b6 On Mac and Linux, running the scripts supplied with dataverse_utils is straightforward. They\u2019re available at the command line, which means that you can simply run them by (for example): $ dv_manifest_gen.py followed by switches and variables as normal. Doing this results in output to the terminal window. This is not necessarily the case in Windows . Whether or not the utility appears on the Windows command line will likely depend on the shell you installed it with: Commonly used shells on Windows include, but are not limited to: Command prompt (cmd.exe) PowerShell Git bash Cygwin If the utilities are not functioning the way you are expecting, make sure that Python and the install location is in your PATH. As of version 0.8.3 the installation has been updated to use newer packaging standards which will ideally reduce the frequency of installation problems. Also as of v0.8.3, the .py suffix has been removed from the console utilities. This means if you are forced to run a command manually, it would be run with python [utilityname] not python [utilityname.py] Troubleshooting \u00b6 These are (likely deprecated) notes from testing on a variety of Windows installations. If you\u2019re luckly, a solution can be found below. This test case uses a new installation of https://python.org Python, v.3.9.6, installed in Windows using the GUI, so it\u2019s as basic a Windows installation as you can get. In this instance, dataverse_utils were installed using pip from the PowerShell, ie pip install git+https://github.com/ubc-library-rc/dataverse_utils . However, pip should be pip regardless of which console you use to install. Here\u2019s a handy guide to show the results of how/if the command line scripts run. This example uses dv_manifest.gen.py , but appears to be generally applicable to any scripts configured with setuptools.setup() (ie, setuptools.setup(scripts=[whatever] ). If this means nothing to you because you\u2019re not distributing your own Python packages,, just skip to the table below. Windows Terminal type \u00b6 PowerShell \u00b6 Note that on these tests the user is not an administrator . Administrator results, in all likelihood, will be completely different, ideally better. Problem Attempting to run the script results in a window popping up for a nanosecond and no output. Solution This may not occur if the PowerShell is run as an administrator. What is happening here is that the script is running, but it\u2019s showing up in a separate window. Output can be created as normal, if you use the correct switches. Unfortunately, you won\u2019t be able to see what they are, because the popup window disappears, which is not helpful. There are three potential fixes. If you can run PowerShell as an administrator, that may solve the problem. Edit the PATHEXT environment variable to include the .py extension. Note that if the user PATHEXT is edited, the system will ignore the system PATHEXT, meaning that things like .exe files won\u2019t run unless the full name is typed (eg. \u2018notepad.exe\u2019). So, if editing the user PATHEXT, make sure to include the system PATHEXT and append ;.PY to the string. Edit the PATHEXT for PowerShell itself, rather than on a system wide level. Editing $PROFILE to include the .py extension should allow the Python script to run within PowerShell. For instructions, see https://docs.microsoft.com/en-ca/powershell/module/microsoft.powershell.core/about/about_profiles?view=powershell-7.1 . Create a profile as per How to create a profile Within that profile, $env:PATHEXT += \";.py\" Depending on the nature of your Windows installation, this may be disabled by the security policy, in which case you can also try the method above. Command prompt \u00b6 This is the traditional Windows command prompt (ie, cmd.exe ). The scripts should just work after being installed with pip, as this installation is the default. For example, run with: C:\\>dv_manifest_gen.py [arguments] Obviously it don\u2019t type the C:\\> part. SSH session \u00b6 If using the built-in Windows SSH server, scripts should simply run as per the command prompt above. Windows SSH sessions default to using the Windows command prompt, not bash. Bash sessions under SSH should function if Bash is configured as below. Git Bash \u00b6 Git Bash is part of the git suite available here: https://git-scm.com/downloads . There are a few notable wrinkles with for use with Python. During installation of Git After v2.32.0.2 (and possibly earlier), you will have the option during the installation to \u201cEnable experimental support for pseudo consoles\u201d. Doing this will allow you run Python directly from the bash shell like you would normally, and the scripts should function as per the command prompt above. As a bonus, enabling this feature seems to fix errors with pipes which formerly resulted in the stdout is not a tty error when piping shell output (for instance, to grep ). If you have not checked this box, you will need to add an alias to your .bashrc and/or .bash_profile : alias python='winpty python' alias pip='winpty pip' Either that, or you will need to start Python with winpty python , which is annoying. Similarly winpty pip . Even if you have not enabled pseudo-console support and didn\u2019t complete use option 2, the scripts should still function normally though. Having scripts work but Python not work is not optimal and confusing, so a solution is there even though it technically isn\u2019t required. There are many options for Git Bash installation; testing has not covered all possible permutations of installation options.","title":"Windows notes"},{"location":"windows/#running-the-dataverse_utils-console-utilities-under-windows","text":"On Mac and Linux, running the scripts supplied with dataverse_utils is straightforward. They\u2019re available at the command line, which means that you can simply run them by (for example): $ dv_manifest_gen.py followed by switches and variables as normal. Doing this results in output to the terminal window. This is not necessarily the case in Windows . Whether or not the utility appears on the Windows command line will likely depend on the shell you installed it with: Commonly used shells on Windows include, but are not limited to: Command prompt (cmd.exe) PowerShell Git bash Cygwin If the utilities are not functioning the way you are expecting, make sure that Python and the install location is in your PATH. As of version 0.8.3 the installation has been updated to use newer packaging standards which will ideally reduce the frequency of installation problems. Also as of v0.8.3, the .py suffix has been removed from the console utilities. This means if you are forced to run a command manually, it would be run with python [utilityname] not python [utilityname.py]","title":"Running the dataverse_utils console utilities under Windows"},{"location":"windows/#troubleshooting","text":"These are (likely deprecated) notes from testing on a variety of Windows installations. If you\u2019re luckly, a solution can be found below. This test case uses a new installation of https://python.org Python, v.3.9.6, installed in Windows using the GUI, so it\u2019s as basic a Windows installation as you can get. In this instance, dataverse_utils were installed using pip from the PowerShell, ie pip install git+https://github.com/ubc-library-rc/dataverse_utils . However, pip should be pip regardless of which console you use to install. Here\u2019s a handy guide to show the results of how/if the command line scripts run. This example uses dv_manifest.gen.py , but appears to be generally applicable to any scripts configured with setuptools.setup() (ie, setuptools.setup(scripts=[whatever] ). If this means nothing to you because you\u2019re not distributing your own Python packages,, just skip to the table below.","title":"Troubleshooting"},{"location":"windows/#windows-terminal-type","text":"","title":"Windows Terminal type"},{"location":"windows/#powershell","text":"Note that on these tests the user is not an administrator . Administrator results, in all likelihood, will be completely different, ideally better. Problem Attempting to run the script results in a window popping up for a nanosecond and no output. Solution This may not occur if the PowerShell is run as an administrator. What is happening here is that the script is running, but it\u2019s showing up in a separate window. Output can be created as normal, if you use the correct switches. Unfortunately, you won\u2019t be able to see what they are, because the popup window disappears, which is not helpful. There are three potential fixes. If you can run PowerShell as an administrator, that may solve the problem. Edit the PATHEXT environment variable to include the .py extension. Note that if the user PATHEXT is edited, the system will ignore the system PATHEXT, meaning that things like .exe files won\u2019t run unless the full name is typed (eg. \u2018notepad.exe\u2019). So, if editing the user PATHEXT, make sure to include the system PATHEXT and append ;.PY to the string. Edit the PATHEXT for PowerShell itself, rather than on a system wide level. Editing $PROFILE to include the .py extension should allow the Python script to run within PowerShell. For instructions, see https://docs.microsoft.com/en-ca/powershell/module/microsoft.powershell.core/about/about_profiles?view=powershell-7.1 . Create a profile as per How to create a profile Within that profile, $env:PATHEXT += \";.py\" Depending on the nature of your Windows installation, this may be disabled by the security policy, in which case you can also try the method above.","title":"PowerShell"},{"location":"windows/#command-prompt","text":"This is the traditional Windows command prompt (ie, cmd.exe ). The scripts should just work after being installed with pip, as this installation is the default. For example, run with: C:\\>dv_manifest_gen.py [arguments] Obviously it don\u2019t type the C:\\> part.","title":"Command prompt"},{"location":"windows/#ssh-session","text":"If using the built-in Windows SSH server, scripts should simply run as per the command prompt above. Windows SSH sessions default to using the Windows command prompt, not bash. Bash sessions under SSH should function if Bash is configured as below.","title":"SSH session"},{"location":"windows/#git-bash","text":"Git Bash is part of the git suite available here: https://git-scm.com/downloads . There are a few notable wrinkles with for use with Python. During installation of Git After v2.32.0.2 (and possibly earlier), you will have the option during the installation to \u201cEnable experimental support for pseudo consoles\u201d. Doing this will allow you run Python directly from the bash shell like you would normally, and the scripts should function as per the command prompt above. As a bonus, enabling this feature seems to fix errors with pipes which formerly resulted in the stdout is not a tty error when piping shell output (for instance, to grep ). If you have not checked this box, you will need to add an alias to your .bashrc and/or .bash_profile : alias python='winpty python' alias pip='winpty pip' Either that, or you will need to start Python with winpty python , which is annoying. Similarly winpty pip . Even if you have not enabled pseudo-console support and didn\u2019t complete use option 2, the scripts should still function normally though. Having scripts work but Python not work is not optimal and confusing, so a solution is there even though it technically isn\u2019t required. There are many options for Git Bash installation; testing has not covered all possible permutations of installation options.","title":"Git Bash"}]}